{"categories":[{"title":"bolg","uri":"https://gb.ytte.top/categories/bolg/"},{"title":"git","uri":"https://gb.ytte.top/categories/git/"},{"title":"java","uri":"https://gb.ytte.top/categories/java/"},{"title":"JVM","uri":"https://gb.ytte.top/categories/jvm/"},{"title":"ZooKeeper","uri":"https://gb.ytte.top/categories/zookeeper/"},{"title":"数据库","uri":"https://gb.ytte.top/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"title":"服务器","uri":"https://gb.ytte.top/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"title":"杂项","uri":"https://gb.ytte.top/categories/%E6%9D%82%E9%A1%B9/"},{"title":"消息队列","uri":"https://gb.ytte.top/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"title":"计算机网络","uri":"https://gb.ytte.top/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"title":"项目","uri":"https://gb.ytte.top/categories/%E9%A1%B9%E7%9B%AE/"}],"posts":[{"content":"总结 查询 部门薪水   部门里最高薪水\nIN临时表\n  部门里三高薪水\n关联子查询\n  ==尽量不要==使用关联子查询，复杂度O(n²)。\n题目  2022-3-31\n部门里最高薪水 这里也可以使用关联子查询，但是复杂度O(n²)。\nCreate table If Not Exists Employee (id int, name varchar(255), salary int, departmentId int); Create table If Not Exists Department (id int, name varchar(255)); Truncate table Employee; insert into Employee (id, name, salary, departmentId) values (\u0026#39;1\u0026#39;, \u0026#39;Joe\u0026#39;, \u0026#39;85000\u0026#39;, \u0026#39;1\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;2\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;80000\u0026#39;, \u0026#39;2\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;3\u0026#39;, \u0026#39;Sam\u0026#39;, \u0026#39;60000\u0026#39;, \u0026#39;2\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;4\u0026#39;, \u0026#39;Max\u0026#39;, \u0026#39;90000\u0026#39;, \u0026#39;1\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;5\u0026#39;, \u0026#39;Janet\u0026#39;, \u0026#39;69000\u0026#39;, \u0026#39;1\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;6\u0026#39;, \u0026#39;Randy\u0026#39;, \u0026#39;85000\u0026#39;, \u0026#39;1\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;7\u0026#39;, \u0026#39;Will\u0026#39;, \u0026#39;70000\u0026#39;, \u0026#39;1\u0026#39;); Truncate table Department; insert into Department (id, name) values (\u0026#39;1\u0026#39;, \u0026#39;IT\u0026#39;); insert into Department (id, name) values (\u0026#39;2\u0026#39;, \u0026#39;Sales\u0026#39;); 因为 Employee 表包含 Salary 和 DepartmentId 字段，我们可以以此在部门内查询最高工资。\n\tSELECT \tdepartmentid, \tMAX( salary ) \tFROM \temployee \tGROUP BY \tdepartmentid 注意：有可能有多个员工同时拥有最高工资，所以最好在这个查询中不包含雇员名字的信息。\n| DepartmentId | MAX(Salary) | |--------------|-------------| | 1 | 90000 | | 2 | 80000 | 然后，我们可以把表 Employee 和 Department 连接，再在这张临时表里用 IN 语句查询部门名字和工资的关系。（打破之前的思想：原本以为只能单字段IN，实际可以多字段IN）\nSELECT \td1.NAME AS Department, \te1.NAME Employee, \te1.salary Salary FROM \tEmployee e1 \tJOIN Department d1 ON e1.departmentId = d1.id WHERE \t( e1.departmentid, e1.salary ) IN (  SELECT  departmentid,  MAX( salary )  FROM  employee  GROUP BY  departmentid \t) 部门里三高薪水 Create table If Not Exists Employee (id int, name varchar(255), salary int, departmentId int); Create table If Not Exists Department (id int, name varchar(255)); Truncate table Employee; insert into Employee (id, name, salary, departmentId) values (\u0026#39;1\u0026#39;, \u0026#39;Joe\u0026#39;, \u0026#39;85000\u0026#39;, \u0026#39;1\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;2\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;80000\u0026#39;, \u0026#39;2\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;3\u0026#39;, \u0026#39;Sam\u0026#39;, \u0026#39;60000\u0026#39;, \u0026#39;2\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;4\u0026#39;, \u0026#39;Max\u0026#39;, \u0026#39;90000\u0026#39;, \u0026#39;1\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;5\u0026#39;, \u0026#39;Janet\u0026#39;, \u0026#39;69000\u0026#39;, \u0026#39;1\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;6\u0026#39;, \u0026#39;Randy\u0026#39;, \u0026#39;85000\u0026#39;, \u0026#39;1\u0026#39;); insert into Employee (id, name, salary, departmentId) values (\u0026#39;7\u0026#39;, \u0026#39;Will\u0026#39;, \u0026#39;70000\u0026#39;, \u0026#39;1\u0026#39;); Truncate table Department; insert into Department (id, name) values (\u0026#39;1\u0026#39;, \u0026#39;IT\u0026#39;); insert into Department (id, name) values (\u0026#39;2\u0026#39;, \u0026#39;Sales\u0026#39;);  方法一：（关联子查询）  公司里前 3 高的薪水意味着有不超过 3 个工资比这些值大。\nselect e1.Name as \u0026#39;Employee\u0026#39;, e1.Salary from Employee e1 where 3 \u0026gt; (  select count(distinct e2.Salary)  from Employee e2  where e2.Salary \u0026gt; e1.Salary ); 在这个代码里，我们统计了有多少人的工资比 e1.Salary 高，所以样例的输出应该如下所示。\n| Employee | Salary | |----------|--------| | Henry | 80000 | | Max | 90000 | | Randy | 85000 | 然后，我们需要把表 Employee 和表 Department 连接来获得部门信息。\nSELECT  d.Name AS \u0026#39;Department\u0026#39;, e1.Name AS \u0026#39;Employee\u0026#39;, e1.Salary FROM  Employee e1  JOIN  Department d ON e1.DepartmentId = d.Id WHERE  3 \u0026gt; (SELECT  COUNT(DISTINCT e2.Salary)  FROM  Employee e2  WHERE  e2.Salary \u0026gt; e1.Salary  AND e1.DepartmentId = e2.DepartmentId  ) ;   方法二：使用mysql8中的DENSE_RANK()函数，按DepartmentId字段隔离排序Salary。\nDENSE_RANK() OVER ( PARTITION BY DepartmentId ORDER BY Salary DESC ) 效果如下：\n+----------------+-------------+--------+------------+ | sales_employee | fiscal_year | sale | sales_rank | +----------------+-------------+--------+------------+ | John | 2016 | 200.00 | 1 | | Alice | 2016 | 150.00 | 2 | | Bob | 2016 | 100.00 | 3 | | Bob | 2017 | 150.00 | 1 | | John | 2017 | 150.00 | 1 | | Alice | 2017 | 100.00 | 2 | | John | 2018 | 250.00 | 1 | | Alice | 2018 | 200.00 | 2 | | Bob | 2018 | 200.00 | 2 | +----------------+-------------+--------+------------+ 9 rows in set (0.01 sec)   SELECT B.Name AS Department, A.Name AS Employee, A.Salary FROM (SELECT DENSE_RANK() OVER (partition by DepartmentId order by Salary desc) AS ranking,DepartmentId,Name,Salary  FROM Employee) AS A JOIN Department AS B ON A.DepartmentId=B.id WHERE A.ranking\u0026lt;=3  搞定！\n人生无常，大肠包小肠！\n继续学习吧！\n","id":0,"section":"posts","summary":"","tags":["mysql","题目"],"title":"Mysql Exercises and Summaries","uri":"https://gb.ytte.top/2022/03/31/mysql-exercises-and-summaries/","year":"2022"},{"content":" 搞定！\n人生无常，大肠包小肠！\n继续学习吧！\n","id":1,"section":"posts","summary":"","tags":["mysql"],"title":"MySql基础","uri":"https://gb.ytte.top/2022/03/24/mysql%E5%9F%BA%E7%A1%80/","year":"2022"},{"content":"顺序io随机io\n回表查询\nMySQL高级 sql数据：\n  course表\n  8\nCREATE TABLE `course` (  `id` int NOT NULL AUTO_INCREMENT,  `course_id` int NOT NULL,  `course_name` varchar(40) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,  PRIMARY KEY (`id`) USING BTREE,  KEY `idx` (`id`) USING BTREE ) ENGINE=InnoDB AUTO_INCREMENT=201 DEFAULT CHARSET=utf8mb3;   5.7\nCREATE TABLE `course` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `course_id` int(11) NOT NULL,  `course_name` varchar(40) DEFAULT NULL,  PRIMARY KEY (`id`) USING BTREE,  KEY `idx` (`id`) USING BTREE ) ENGINE=InnoDB AUTO_INCREMENT=201 DEFAULT CHARSET=utf8;     student_info表\n  8\nCREATE TABLE `student_info` (  `id` int NOT NULL AUTO_INCREMENT,  `student_id` int NOT NULL,  `name` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,  `course_id` int NOT NULL,  `class_id` int DEFAULT NULL,  `create_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,  PRIMARY KEY (`id`) USING BTREE,  KEY `idx_sid` (`student_id`),  KEY `idx_cre_time` (`create_time`) ) ENGINE=InnoDB AUTO_INCREMENT=1000001 DEFAULT CHARSET=utf8mb3;   5.7\nCREATE TABLE `student_info` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `student_id` int(11) NOT NULL,  `name` varchar(20) DEFAULT NULL,  `course_id` int(11) NOT NULL,  `class_id` int(11) DEFAULT NULL,  `create_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,  PRIMARY KEY (`id`) USING BTREE ) ENGINE=InnoDB AUTO_INCREMENT=1000001 DEFAULT CHARSET=utf8;     存储过程，为course增加100条数据；为student_info 增加100w条数据。\nset global log_bin_trust_function_creators=TRUE;  DELIMITER // CREATE FUNCTION rand_string ( n INT ) RETURNS VARCHAR ( 255 ) BEGIN \tDECLARE \tchars_str VARCHAR ( 100 ) DEFAULT \u0026#39;abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ\u0026#39;; \tDECLARE \treturn_str VARCHAR ( 255 ) DEFAULT \u0026#39;\u0026#39;; \tDECLARE \ti INT DEFAULT 0; \tWHILE \ti \u0026lt; n DO \tSET return_str = CONCAT( \treturn_str, \tSUBSTRING( chars_str, FLOOR( 1+RAND ()* 52 ), 1 )); \tSET i = i + 1;  \tEND WHILE; \tRETURN return_str;  END // DELIMITER;    DELIMITER // CREATE FUNCTION rand_num ( from_num INT, to_num INT ) RETURNS INT ( 11 ) BEGIN \tDECLARE \ti INT DEFAULT 0;  \tSET i = FLOOR( \tfrom_num + RAND()*( \tto_num - from_num + 1 \t)); \tRETURN i;  END // DELIMITER;  show variables like \u0026#39;log_bin_trust_function_creators\u0026#39;; set global log_bin_trust_function_creators=1;  DELIMITER // CREATE PROCEDURE insert_course ( max_num INT ) BEGIN \tDECLARE \ti INT DEFAULT 0;  \tSET autocommit = 0; \tREPEAT  \tSET i = i + 1; \tINSERT INTO course ( course_id, course_name ) \tVALUES \t( \trand_num ( 10000, 10100 ), \trand_string ( 6 )); \tUNTIL i = max_num \tEND REPEAT; \tCOMMIT;  END // DELIMITER;   DELIMITER // CREATE PROCEDURE insert_stu ( max_num INT ) BEGIN \tDECLARE \ti INT DEFAULT 0;  \tSET autocommit = 0; \tREPEAT  \tSET i = i + 1; \tINSERT INTO student_info ( course_id, class_id, student_id, NAME ) \tVALUES \t( \trand_num ( 10000, 10100 ), \trand_num ( 10000, 10200 ), \trand_num ( 1, 200000 ), \trand_string ( 6 )); \tUNTIL i = max_num \tEND REPEAT; \tCOMMIT;  END // DELIMITER;  CALL insert_course(100);  CALL insert_stu(1000000);   索引的创建与设计原则 索引的必要使用场景 字段的数值有唯一性的限制 业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。\n说明：唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明显的。\n频繁作为 WHERE 查询条件的字段 某个字段在SELECT语句的 WHERE 条件中经常被使用到，那么就需要给这个字段创建索引了。尤其是在数据量大的情况下，创建普通索引就可以大幅提升数据查询的效率。\n比如查询 student_id =144324的时候\nSELECT course_id ,class_id, `name`,create_time,student_id FROM student_info WHERE student_id =144324   在student_id不加索引的情况下，100w条数据，耗时为350ms左右，即在百毫秒级\nALTER TABLE student_info ADD INDEX idx_sid(student_id)  SHOW INDEX FROM student_info #查看是否添加成功   在添加索引的情况下，耗时为22ms，降了一个数量级，这是很客观的。\n   经常 GROUP BY 和 ORDER BY 的列 索引就是让数据按照某种顺序进行存储或检索，因此当我们使用GROUP BY对数据进行分组查询，或者使用ORDER BY对数据进行排序的时候，就需要对分组或者排序的字段进行索引，从而减少排序的时间。如果待排序的列有多个，那么可以在这些列上建立组合索引。\nGROUP BY eg：在100w条数据中，按照student_id对学生选修课程进行分组，显示不同的student_id和课程数量，显示100个。\nSELECT student_id ,COUNT(*) AS num FROM student_info GROUP BY student_id LIMIT 100  DROP INDEX idx_sid ON student_info SHOW INDEX FROM student_info  在不加索引的情况下，耗时850毫秒。 在加索引的情况下（这里使用的是2）当中的idx_sid索引），耗时21毫秒。  ORDER BY ORDER BY单独使用的情况与GROUP BY差不多\nORDER BY和GROUP BY同时使用的情况 ==由于GROUP BY先执行，且GROUP BY为先排序再分组，所以主要耗时就集中在GROUP BY上。==\n使用以下三种情况来观察不同索引状况下，sql执行的速度差异\n 三种情况：  ​\t①. student_id和create_time分别建立索引\n​\t②. student_id和create_time建立联合索引，且student_id在前\n​\t③. student_id和create_time建立联合索引，且create_time在前\nSELECT student_id,COUNT(*)\tAS\tnum\tFROM\tstudent_info GROUP BY\tstudent_id ORDER BY\tcreate_time desc LIMIT 100 不加除主键索引以外的其他索引的情况下，耗时约==0.8~1s==\n 两种版本：  ​\t①. MySQL 5.7.5及以上功能依赖检测功能\n​\t②. MySQL 5.7.5以下无依赖检测功能\n执行上方代码后MySQL 5.7.5及以上会报错\nsql_mode=only_full_group_by #出现问题 MySQL 5.7.5及以上由于sql_mode=only_full_group_by是默认开启依赖检测功能，所以HAVING条件或ORDER BY列表的查询引用在GROUP BY子句中。 简单来说就是：输出的结果是叫target list，就是select后面跟着的字段，还有一个地方group by column，就是group by后面跟着的字段。由于开启ONLY_FULL_GROUP_BY的设置，如果一个字段没有在target list和group by字段中同时出现，或者是聚合函数的值的话，那么这条sql查询是被mysql认为非法的，会报错误。\n但是如果同时GROUP BY student_id，create_time 这两个字段，就失去了本例的作用。所以需要MySQL 5.7.5及以上的需要关闭依赖检测功能\n具体解析链接====》文档——石墨\nselect @@global.sql_mode #查看sql_mode详细  set @@global.sql_mode =\u0026#39;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\u0026#39;; #去掉ONLY_FULL_GROUP_BY，重新设置值，对于新建的数据库有效 8.0中还要去掉NO_AUTO_CREATE_USER  set global sql_mode=\u0026#39;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\u0026#39;; #对于已存在的数据库，则需要在对应的数据下执行 继续测试\n  添加单列student_id索引和create_time索引\n​\t5.7中：\n耗时6s左右，添加索引就是将这些时间提前消耗掉，让客户查询的时候减少查询时间。\nALTER TABLE student_info ADD INDEX idx_sid(student_id);  ALTER TABLE student_info ADD INDEX idx_cre_time(create_time DESC); 这里 EXPLAIN 后会发现只使用了student_id 的索引idx_sid（单列索引的情况，只使用语句执行过程第一个索引）所以这里只使用了索引idx_sid，而idx_cre_time压根没用到。\nSHOW INDEX FROM student_info  EXPLAIN  SELECT student_id,COUNT(*)\tAS\tnum\tFROM\tstudent_info GROUP BY\tstudent_id ORDER BY\tcreate_time DESC LIMIT 100; 各自单列索引的情况下，耗时大约1.5s（上面不加索引的情况下耗时1s）\n​\t8.0中\n==todo 不明原因，未得到解决，上述方式在5.7中大约需要1.5s左右，而在8.0中需要几分钟的时间。==\n  联合索引的情况① ：\n先删除除主键外其他的单列索引\nidx_sid_cre_time(student_id,create_time)，student_id在前\n​\t5.7：\nALTER TABLE student_info ADD INDEX idx_sid_cre_time(student_id,create_time);  SHOW INDEX FROM student_info;  EXPLAIN  SELECT student_id,COUNT(*)\tAS\tnum\tFROM\tstudent_info GROUP BY\tstudent_id ORDER BY\tcreate_time DESC LIMIT 100; 耗时约==300ms==左右（未加索引1s左右，单列索引1.5s左右）\n8.0：\nDROP INDEX idx_sid ON student_info ; DROP INDEX idx_cre_time ON student_info; DROP INDEX idx_sid_cre_time ON student_info;  ALTER TABLE student_info ADD INDEX idx_sid_cre_time(student_id,create_time DESC);  SHOW INDEX FROM student_info;  EXPLAIN  SELECT student_id,COUNT(*)\tAS\tnum\tFROM\tstudent_info GROUP BY\tstudent_id ORDER BY\tcreate_time DESC LIMIT 100;   索引不加DESC时，耗时约==800ms==左右（不加索引1s左右，单列索引几分钟）\n  索引加DESC时，耗时约==300ms==左右（不加索引1s左右，单列索引几分钟）\n    联合索引的情况② ：\nidx_cre_sid_time(create_time,student_id)，create_time在前\n在语句执行中，先进行group by 所以此时如果存在idx_sid(student_id)的话，会优先使用group by student_id的student_id索引（idx_sid），这样就变成了使用单列索引，联合索引idx_sid_cre_time就不起作用了。\n如果不存在idx_sid，只有联合索引idx_cre_sid_time：\n​ 5.7：\nDROP INDEX idx_sid ON student_info ; DROP INDEX idx_cre_time ON student_info; DROP INDEX idx_sid_cre_time ON student_info;  ALTER TABLE student_info ADD INDEX idx_cre_sid_time(create_time,student_id);  SELECT student_id,COUNT(*)\tAS\tnum\tFROM\tstudent_info GROUP BY\tstudent_id ORDER BY\tcreate_time DESC LIMIT 100; 耗时约==800ms左右== （未加索引1s左右，单列索引1.5s左右，idx_sid_cre_time索引耗时约300ms）\n​ 8.0 ：\nDROP INDEX idx_sid ON student_info ; DROP INDEX idx_cre_time ON student_info; DROP INDEX idx_sid_cre_time ON student_info; DROP INDEX idx_cre_sid_time ON student_info;  ALTER TABLE student_info ADD INDEX idx_cre_sid_time(create_time DESC,student_id);  SELECT student_id,COUNT(*)\tAS\tnum\tFROM\tstudent_info GROUP BY\tstudent_id ORDER BY\tcreate_time DESC LIMIT 100;   索引不加DESC时，耗时约==1s==左右（不加索引1s左右，单列索引几分钟，idx_sid_cre_time索引800ms左右）\n  索引加DESC时，耗时约==1s==左右（不加索引1s左右，单列索引几分钟，idx_sid_cre_time索引300ms左右）\n    UPDATE、DELETE 的 WHERE 条件列 对数据按照某个条件进行查询后再进行 UPDATE 或 DELETE 的操作，如果对 WHERE 字段创建了索引，就能大幅提升效率。\n原理是因为我们需要先根据 WHERE 条件列检索出来这条记录，然后再对它进行更新或删除。如果进行更新的时候，更新的字段是非索引字段，提升的效率会更明显，这是因为非索引字段更新不需要对索引进行维护。\nDISTINCT 字段需要创建索引 有时候我们需要对某个字段进行去重，使用 DISTINCT，那么对这个字段创建索引，也会提升查询效率。\n比如，我们想要查询课程表中不同的 student_id 都有哪些，如果我们没有对 student_id 创建索引，执行\nSQL 语句：\nSELECT DISTINCT(student_id) FROM `student_info`; 运行结果（600637 条记录，运行时间 0.683s ）：\n如果我们对 student_id 创建索引，再执行 SQL 语句：\nSELECT DISTINCT(student_id) FROM `student_info`; 运行结果（600637 条记录，运行时间 0.010s ）：\nSQL 查询效率有了提升，同时显示出来的 student_id 还是按照 递增的顺序 进行展示的。这是因为索引会对数据按照某种顺序进行排序，所以在去重的时候也会快很多。\n多表 JOIN 连接操作时，创建索引注意事项 首先， ==连接表的数量尽量不要超过 3 张==，因为每增加一张表就相当于增加了一次嵌套的循环，数量级增长会非常快，严重影响查询的效率。\n其次， 对 WHERE 条件创建索引 ，因为 WHERE 才是对数据条件的过滤。如果在数据量非常大的情况下，没有 WHERE 条件过滤是非常可怕的。\n最后，==类似字符串和int比较时，会存在隐式转换，即使用了函数，一旦使用了函数就会让索引失效。 对用于连接的字段创建索引 ，并且该字段在多张表中的 类型必须一致 。==比如 course_id 在 student_info 表和 course 表中都为 int(11) 类型，而不能一个为 int 另一个为 varchar 类型。\n举个例子，如果我们只对 student_id 创建索引，执行 SQL 语句：\nSELECT course_id, name, student_info.student_id, course_name FROM student_info JOIN course ON student_info.course_id = course.course_id WHERE name = \u0026#39;462eed7ac6e791292a79\u0026#39;; 运行结果（1 条数据，运行时间 0.189s ）：\n这里我们对 name 创建索引，再执行上面的 SQL 语句，运行时间为 0.002s 。\n使用列的类型小的创建索引 使用字符串前缀创建索引 创建一张商户表，因为地址字段比较长，在地址字段上建立前缀索引\ncreate table shop(address varchar(120) not null);  alter table shop add index(address(12)); 问题是，截取多少呢？截取得多了，达不到节省索引存储空间的目的；截取得少了，重复内容太多，字\n段的散列度(选择性)会降低。怎么计算不同的长度的选择性呢？\n先看一下字段在全部数据中的选择度：\nselect count(distinct address) / count(*) from shop; 通过不同长度去计算，与全表的选择性对比：\n公式：\ncount(distinct left(列名, 索引长度))/count(*) 例如：\nselect count(distinct left(address,10)) / count(*) as sub10, -- 截取前10个字符的选择度 count(distinct left(address,15)) / count(*) as sub11, -- 截取前15个字符的选择度 count(distinct left(address,20)) / count(*) as sub12, -- 截取前20个字符的选择度 count(distinct left(address,25)) / count(*) as sub13 -- 截取前25个字符的选择度 from shop; 引申另一个问题：索引列前缀对排序的影响\n拓展：Alibaba《Java开发手册》\n==在 varchar 字段上建立索引时，必须指定索引长度，根据实际文本区分度决定索引长度。==\n说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会 高达 90% 以上 ，可以使用\n count(distinct left(列名, 索引长度))/count(*)的区分度来确定。\n区分度高(散列性高)的列适合作为索引 使用最频繁的列放到联合索引的左侧 在多个字段都要创建索引的情况下，联合索引优于单值索引 限制索引的数目 哪些情况不适合创建索引  WHERE条件（包括GROUP BY、ORDER BY)里用不到的字段不需要创建索引 数据量小的表最好不要使用索引 有大量重复数据的列上不要建立索引  比如性别。   避免对经常更新的表创建过多的索引 不建议用无序的值作为索引  例如身份证、UUID(在索引比较时需要转为ASCII，并且插入时可能造成页分裂)、MD5、HASH、无序长字符串等。   删除不再使用或者很少使用的索引 不要定义冗余或重复的索引  性能分析工具的使用 数据库服务器的优化步骤 视频 p134————尚硅谷\n一般数据库遇到问题需要调优的时候需要经历一下几个过程。\n整个流程划分成观察（show status）和行动（Action）字母 S 的部分代表观察（会使用相应的分析工具），字母 A 代表的部分是行动（对应分析可以采取的行动）。如图：\nmysql优化的效果是随着成本提高而降低的：\n查看系统性能参数 视频 p134 ———— 尚硅谷\n在MySQL中，可以使用 SHOW STATUS 语句查询一些MySQL数据库服务器的 ==性能参数== 、 ==执行频率== 。\n语法如下：\nSHOW [GLOBAL|SESSION] STATUS LIKE \u0026#39;参数\u0026#39;;    参数 意译     Connections 连接MySQL服务器的次数   Uptime MySQL服务器的上线时间   Slow_queries 慢查询的次数   Innodb_rows_read Select查询返回的行数   Innodb_rows_inserted 执行INSERT操作插入的行数   Innodb_rows_updated 执行UPDATE操作更新的行数   Innodb_rows_deleted 执行DELETE操作删除的行数   Com_select 查询操作的次数   Com_insert 插入操作的次数。对于批量插入的 INSERT 操作，只累加一次   Com_update 更新操作的次数   Com_delete 删除操作的次数    统计SQL的查询成本 视频 p134 ———— 尚硅谷\nlast_query_cost.\n需要在原始命令行中输入查询，Navicat中查询的last_query_cost不正确\n如果我们想要查询 id=900001 的记录，然后看下查询成本，我们可以直接在聚簇索引上进行查找：\n耗时在0.01s左右\nSELECT student_id, class_id, NAME, create_time FROM student_info WHERE id = 900001; 然后再看下查询优化器的成本，实际上我们只需要检索一个页即可：\n如果我们想要查询 id 在 900001 到 9000100 之间的学生记录：\n耗时在0.04s左右\nSELECT student_id, class_id, NAME, create_time FROM student_info WHERE id BETWEEN 900001 AND 901000; 然后再看下查询优化器的成本，这时我们大概需要进行 400 个页的查询。\n你能看到页的数量是刚才的 400 倍，但是查询的效率并没有明显的数量级变化，实际上这两个 SQL 查询的时间基本上一样，就是因为采用了==顺序读取==的方式将页面一次性加载到缓冲池中，然后再进行查找。虽然页数量（last_query_cost）增加了不少 ，但是通过缓冲池的机制，并 没有增加多少查询时间 。\n**使用场景：**它对于比较开销是非常有用的，特别是我们有好几种查询方式可选的时候。\n定位执行慢的SQL：慢查询日志 视频 p135————尚硅谷\nMySQL的慢查询日志，用来记录在MySQL中响==应时间超过阀值==的语句，具体指运行时间超过==long_query._time==值的SQL,则会被记录到慢查询日志中。long_query._time的默认值为==10==，意思是运行10秒以上（不含10秒）的语句，认为是超出了我们的最大忍耐时间值。\n它的主要作用是，帮助我们发现那些执行时间特别长的SQL查询，并且有针对性地进行优化，从而提高系统的整体效率。当我们的数据库服务器发生阻塞、运行变慢的时候，检查一下慢查询日志，找到那些慢查询，对解决问题很有帮助。比如一条sql执行超过5秒钟，我们就算慢SQL，希望能收集超过5秒的sql，结合explain进行全面分析。\n默认情况下，MySQL数据库没有开启慢查询日志，需要我们手动来设置这个参数。如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。\n慢查询日志支持将日志记录写入文件。\nmyisam中会自动存储一个所有数据的数量的变量，所以与innodb相比，count(*)执行的速度会快于innodb。\n补充说明： 除了上述变量，控制慢查询日志的还有一个系统变量：min_examined_row_limit。这个变量的意思是，查询 扫描过的最少记录数。这个变量和查询执行时间，共同组成了判别一个查询是否是慢查询的条件。如果查询 扫描过的记录数大于等于这个变量的值，并且查询执行时间超过long_query_.time的值，那么，这个查询就 被记录到慢查询日志中；反之，则不被记录到慢查询日志中。\n这个值默认是0。与long-query_.time=10合在一起，表示只要查询的执行时间超过10秒钟，哪怕一个记录也 没有扫描过，都要被记录到慢查询日志中。你也可以根据需要，通过修改“y.ii”文件，来修改查询时长，或 者通过SET指令，用SQL语句修改“min_examined_.row_limit\u0026quot;的值。\n日常开发需注意的结论： ①converting HEAP to MyISAM:查询结果太大，内存不够，数据往磁盘上搬了。 ②Creating tmp table：创建临时表。先拷贝数据到l临时表，用完后再删除临时表。 ③Copying to tmp table on disk：把内存中临时表复制到磁盘上，警惕！ ④1 ocked。 如果在show profilei诊断结果中出现了以上4条结果中的任何一条，则sql语句需要优化。 注意： 不过SHOW PROFILE命令将被弃用，我们可以从information_schema中的profiling数据表进行查看。\n查看SQL执行成本：show profile 分析查询语句：explain 概述 定位了查询慢的SQL之后，我们就可以使用EXPLAIN或DESCRIBE工具做针对性的分析查询语句。DESCRIBE语句 的使用方法与EXPLAIN语句是一样的，并且分析结果也是一样的。 MySQL中有专门负责优化SELECT语句的优化器模块，主要功能：通过计算分析系统中收集到的统计信息，为客户 端请求的Qury提供它认为最优的执行计划（他认为最优的数据检索方式，但不见得是DBA认为是最优的，这部分 最耗费时间)。\n这个执行计划展示了接下来具体执行查询的方式，比如多表连接的顺序是什么，对于每个表采用什么访问方法来 具体执行查询等等。MySQL为我们提供了EXPLAIN语句来帮助我们查看某个查询语句的具体执行计划，大家看懂 EXPLAIN语句的各个输出项，可以有针对性的提升我们查询语句的性能。\n官网介绍\nhttps://dev.mysql.com/doc/refman/5.7/en/explain-output.html\nhttps://dev.mysql.com/doc/refman/8.0/en/explain-output.html\n版本情况\n  MySQL 5.6.3以前只能 EXPLAIN SELECT ；MYSQL 5.6.3以后就可以 EXPLAIN SELECT，UPDATE， DELETE\n  在5.7以前的版本中，想要显示 partitions 需要使用 explain partitions 命令；想要显示filtered 需要使用 explain extended 命令。在5.7版本后，默认explain直接显示partitions和 filtered中的信息\n  基本语法 EXPLAIN 或 DESCRIBE语句的语法形式如下：\nEXPLAIN SELECT select_options #或者 DESCRIBE SELECT select_options 如果我们想看看某个查询的执行计划的话，可以在具体的查询语句前边加一个 EXPLAIN ，就像这样：\nmysql\u0026gt; EXPLAIN SELECT 1; EXPLAIN 语句输出的各个列的作用如下：\n数据准备 建表 CREATE TABLE s1 ( \tid INT AUTO_INCREMENT, \tkey1 VARCHAR ( 100 ), \tkey2 INT, \tkey3 VARCHAR ( 100 ), \tkey_part1 VARCHAR ( 100 ), \tkey_part2 VARCHAR ( 100 ), \tkey_part3 VARCHAR ( 100 ), \tcommon_field VARCHAR ( 100 ), \tPRIMARY KEY ( id ), \tINDEX idx_key1 ( key1 ), \tUNIQUE INDEX idx_key2 ( key2 ), \tINDEX idx_key3 ( key3 ), \tINDEX idx_key_part ( key_part1, key_part2, key_part3 ) ) ENGINE = INNODB CHARSET = utf8;  CREATE TABLE s2 ( \tid INT AUTO_INCREMENT, \tkey1 VARCHAR ( 100 ), \tkey2 INT, \tkey3 VARCHAR ( 100 ), \tkey_part1 VARCHAR ( 100 ), \tkey_part2 VARCHAR ( 100 ), \tkey_part3 VARCHAR ( 100 ), \tcommon_field VARCHAR ( 100 ), \tPRIMARY KEY ( id ), \tINDEX idx_key1 ( key1 ), \tUNIQUE INDEX idx_key2 ( key2 ), \tINDEX idx_key3 ( key3 ), \tINDEX idx_key_part ( key_part1, key_part2, key_part3 ) ) ENGINE = INNODB CHARSET = utf8; 设置参数 log_bin_trust_function_creators 创建函数，假如报错，需开启如下命令：允许创建函数设置：\nset global log_bin_trust_function_creators=1; # 不加global只是当前窗口有效。 创建函数 DELIMITER // CREATE FUNCTION rand_string1 ( n INT ) RETURNS VARCHAR ( 255 ) BEGIN \tDECLARE \tchars_str VARCHAR ( 100 ) DEFAULT \u0026#39;abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ\u0026#39;; \tDECLARE \treturn_str VARCHAR ( 255 ) DEFAULT \u0026#39;\u0026#39;; \tDECLARE \ti INT DEFAULT 0; \tWHILE \ti \u0026lt; n DO \t\tSET return_str = CONCAT( \treturn_str, \tSUBSTRING( chars_str, FLOOR( 1+RAND ()* 52 ), 1 )); \t\tSET i = i + 1; \t\tEND WHILE; \tRETURN return_str; \tEND // DELIMITER; 创建存储过程 创建往s1表中插入数据的存储过程：\nDELIMITER // CREATE PROCEDURE insert_s1 ( \tIN min_num INT ( 10 ), \tIN max_num INT ( 10 )) BEGIN \tDECLARE \ti INT DEFAULT 0; \t\tSET autocommit = 0; \tREPEAT \t\tSET i = i + 1; \tINSERT INTO s1 \tVALUES \t( \t( min_num + i ), \trand_string1 ( 6 ), \t( min_num + 30 * i + 5 ), \trand_string1 ( 6 ), \trand_string1 ( 10 ), \trand_string1 ( 5 ), \trand_string1 ( 10 ), \trand_string1 ( 10 )); \tUNTIL i = max_num \tEND REPEAT; \tCOMMIT; \tEND // DELIMITER; 创建往s2表中插入数据的存储过程：\nDELIMITER // CREATE PROCEDURE insert_s2 ( \tIN min_num INT ( 10 ), \tIN max_num INT ( 10 )) BEGIN \tDECLARE \ti INT DEFAULT 0; \t\tSET autocommit = 0; \tREPEAT \t\tSET i = i + 1; \tINSERT INTO s2 \tVALUES \t( \t( min_num + i ), \trand_string1 ( 6 ), \t( min_num + 30 * i + 5 ), \trand_string1 ( 6 ), \trand_string1 ( 10 ), \trand_string1 ( 5 ), \trand_string1 ( 10 ), \trand_string1 ( 10 )); \tUNTIL i = max_num \tEND REPEAT; \tCOMMIT; \tEND // DELIMITER; 调用存储过程 CALL insert_s1(10001,10000); CALL insert_s2(10001,10000); 即可完成s1与s2中1w条数据的插入\nEXPLAIN各列作用  table：表名 id：在一个大的查询语句中每个SELECT关键字都对应一个唯一的id select_type：查询的类型 partition(略)：匹配的分区信息 type：针对单表的访问方法 possible_keys和key：可能用到的索引 和 实际上使用的索引 key_len：实际使用到的索引长度(即：字节数) ref：当使用索引列等值查询时，与索引列进行等值匹配的对象信息。 rows：预估的需要读取的记录条数 filtered: 某个表经过搜索条件过滤后剩余记录条数的百分比 Extra:一些额外的信息（更准确的理解MySQL到底将如何执行给定的查询语句）  table #1. table：表名 #查询的每一行记录都对应着一个单表 EXPLAIN SELECT * FROM s1;  #s1:驱动表 s2:被驱动表 （mysql会根据优化器优化，不一定s1:驱动表 s2:被驱动表，有可能反过来） EXPLAIN SELECT * FROM s1 INNER JOIN s2; id #2. id：在一个大的查询语句中每个SELECT关键字都对应一个唯一的id  SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39;;   SELECT * FROM s1 INNER JOIN s2  ON s1.key1 = s2.key1  WHERE s1.common_field = \u0026#39;a\u0026#39;;   SELECT * FROM s1  WHERE key1 IN (SELECT key3 FROM s2);   SELECT * FROM s1 UNION SELECT * FROM s2;   EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39;;   EXPLAIN SELECT * FROM s1 INNER JOIN s2;   EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = \u0026#39;a\u0026#39;;   ######查询优化器可能对涉及子查询的查询语句进行重写,转变为多表查询的操作########  # 子查询的复杂度是n方，多表查询是n+n，所以优化器将能优化成多表查询的子查询给优化了，  # 所以这里查询到的id只有1，而不是几个select几个不同的id  EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key2 FROM s2 WHERE common_field = \u0026#39;a\u0026#39;);   #Union去重 union需要建立零时表去组合去重工作，  #所以会多出一行id为空table为\u0026lt;union1,2\u0026gt; select_type为UNION RESULT------------------------------  EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2;  #UNION ALL不会创建零时表  EXPLAIN SELECT * FROM s1 UNION ALL SELECT * FROM s2; id如果相同，可以认为是一组，从上往下顺序执行，在所有组中，id值越大，优先级越高，越先执行\n关注点：id号每个号码，表示一趟独立的查询, 一个sql的查询趟数越少越好\nselect_type #3. select_type：SELECT关键字对应的那个查询的类型,确定小查询在整个大查询中扮演了一个什么角色   # 查询语句中不包含`UNION`或者子查询的查询都算作是`SIMPLE`类型---------------------------  EXPLAIN SELECT * FROM s1;    #连接查询也算是`SIMPLE`类型--------------------------------------------------------  EXPLAIN SELECT * FROM s1 INNER JOIN s2;    #对于包含`UNION`或者`UNION ALL`或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个  #查询的`select_type`值就是`PRIMARY`-----------------------------------------------    #对于包含`UNION`或者`UNION ALL`的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询  #以外，其余的小查询的`select_type`值就是`UNION`--------------------------------------   #`MySQL`选择使用临时表来完成`UNION`查询的去重工作，针对该临时表的查询的`select_type`就是  #`UNION RESULT`-----------------------------------------------------------------  EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2;   EXPLAIN SELECT * FROM s1 UNION ALL SELECT * FROM s2;   #子查询：-------------------------------------------------------------------------  #如果包含子查询的查询语句不能够转为对应的`semi-join`的形式（子查询会默认转化为多表连接的方式，  #但也有不能转化的情况），并且该子查询是不相关子查询。  # in 在非相关子查询中会被优化器转变成多表连接查询，降低复杂度；  #如下方所说在相关子查询中`select_type`就是`DEPENDENT SUBQUERY`，这时候in会被转变为Exist。  #该子查询的第一个`SELECT`关键字代表的那个查询的`select_type`就是`SUBQUERY`  EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = \u0026#39;a\u0026#39;;    #如果包含子查询的查询语句不能够转为对应的`semi-join`的形式，并且该子查询是相关子查询，  #则该子查询的第一个`SELECT`关键字代表的那个查询的`select_type`就是`DEPENDENT SUBQUERY`  EXPLAIN SELECT * FROM s1  WHERE key1 IN (SELECT key1 FROM s2 WHERE s1.key2 = s2.key2) OR key3 = \u0026#39;a\u0026#39;;  #注意的是，select_type为`DEPENDENT SUBQUERY`的查询可能会被执行多次。--------------------    #在包含`UNION`或者`UNION ALL`的大查询中，如果各个小查询都依赖于外层查询的话，那除了  #最左边的那个小查询之外，其余的小查询的`select_type`的值就是`DEPENDENT UNION`。  EXPLAIN SELECT * FROM s1  WHERE key1 IN (SELECT key1 FROM s2 WHERE key1 = \u0026#39;a\u0026#39; UNION SELECT key1 FROM s1 WHERE key1 = \u0026#39;b\u0026#39;);    #对于包含`派生表`的查询，该派生表对应的子查询的`select_type`就是`DERIVED` （从新生成的表中获取数据）  EXPLAIN SELECT *  FROM (SELECT key1, COUNT(*) AS c FROM s1 GROUP BY key1) AS derived_s1 WHERE c \u0026gt; 1;    #当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，  #该子查询对应的`select_type`属性就是`MATERIALIZED`  EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2); #子查询被转为了物化表 partition(略) type system ， const ， eq_ref ， ref ， fulltext ， ref_or_null ， index_merge ， unique_subquery ， index_subquery ， range ， index ， ALL\n # 5. type：针对单表的访问方法   #当表中`只有一条记录`并且该表使用的存储引擎的统计数据是精确的，比如MyISAM、Memory，  #那么对该表的访问方法就是`system`。  CREATE TABLE t(i INT) ENGINE=MYISAM;  INSERT INTO t VALUES(1);   EXPLAIN SELECT * FROM t;   #换成InnoDB  CREATE TABLE tt(i INT) ENGINE=INNODB;  INSERT INTO tt VALUES(1);  EXPLAIN SELECT * FROM tt;    #当我们根据主键或者唯一二级索引列与常数进行等值匹配时，对单表的访问方法就是`const`  EXPLAIN SELECT * FROM s1 WHERE id = 10005;   #表中key是有索引的，但是这里查询到的是ALL没有索引的情况，原因是，key3是varchar类型，我们给的值是  #int类型，所以会进行隐式转换，这就会用到函数，一用到函数，索引就会失效，所以显示的为ALL  EXPLAIN SELECT * FROM s1 WHERE key3 = 10066;    #在连接查询时，如果被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的  #（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则  #对该被驱动表的访问方法就是`eq_ref`  EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.id = s2.id;    #当通过普通的二级索引列与常量进行等值匹配时来查询某个表，那么对该表的访问方法就可能是`ref`  EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39;;    #当对普通二级索引进行等值匹配查询，该索引列的值也可以是`NULL`值时，那么对该表的访问方法  #就可能是`ref_or_null`  EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39; OR key1 IS NULL;    #单表访问方法时在某些场景下可以使用`Intersection`、`Union`、  #`Sort-Union`这三种索引合并的方式来执行查询  #二者都是单列索引，默认情况下式使用一个索引，如果是取并集，就会将两个索引合并，index_merge  EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39; OR key3 = \u0026#39;a\u0026#39;;    #`unique_subquery`是针对在一些包含`IN`子查询的查询语句中，如果查询优化器决定将`IN`子查询  #转换为`EXISTS`子查询，而且子查询可以使用到主键进行等值匹配的话，那么该子查询执行计划的`type`  #列的值就是`unique_subquery`  EXPLAIN SELECT * FROM s1  WHERE key2 IN (SELECT id FROM s2 WHERE s1.key1 = s2.key1) OR key3 = \u0026#39;a\u0026#39;;    #如果使用索引获取某些`范围区间`的记录，那么就可能使用到`range`访问方法  EXPLAIN SELECT * FROM s1 WHERE key1 IN (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;);   #同上  EXPLAIN SELECT * FROM s1 WHERE key1 \u0026gt; \u0026#39;a\u0026#39; AND key1 \u0026lt; \u0026#39;b\u0026#39;;    #当我们可以使用索引覆盖，但需要扫描全部的索引记录时，该表的访问方法就是`index`  EXPLAIN SELECT key_part2 FROM s1 WHERE key_part3 = \u0026#39;a\u0026#39;;    #最熟悉的全表扫描  EXPLAIN SELECT * FROM s1; 总结：\n结果值从最好到最坏依次是：==system \u0026gt; const \u0026gt; eq_ref \u0026gt; ref== \u0026gt; fulltext \u0026gt; ref_or_null \u0026gt; index_merge \u0026gt;unique_subquery \u0026gt; index_subquery \u0026gt; ==range \u0026gt; index \u0026gt; ALL== 其中比较重要的几个提取出来。SQL 性能优化的目标：至少要达到 range 级别，要求是 ref 级别，最好是 consts级别。（阿里巴巴开发手册要求）\npossible_keys和key #6. possible_keys和key：可能用到的索引 和 实际上使用的索引   EXPLAIN SELECT * FROM s1 WHERE key1 \u0026gt; \u0026#39;z\u0026#39; AND key3 = \u0026#39;a\u0026#39;;  key_len #7. key_len：实际使用到的索引长度(即：字节数) # 帮你检查`是否充分的利用上了索引`，`值越大越好`,主要针对于联合索引，有一定的参考意义。  EXPLAIN SELECT * FROM s1 WHERE id = 10005;    EXPLAIN SELECT * FROM s1 WHERE key2 = 10126;  # varchar 100 utf8一个字符占三个字节，所以是300， 记录真正长度（用来观看查询） 2字节（记录303）， 记录是否为null 1字节 所以303  EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39;;  #303  EXPLAIN SELECT * FROM s1 WHERE key_part1 = \u0026#39;a\u0026#39;;   #606联合索引的情况下，数字越大查询到的页记录就少，就越精准，所以越大越好  EXPLAIN SELECT * FROM s1 WHERE key_part1 = \u0026#39;a\u0026#39; AND key_part2 = \u0026#39;b\u0026#39;;   EXPLAIN SELECT * FROM s1 WHERE key_part1 = \u0026#39;a\u0026#39; AND key_part2 = \u0026#39;b\u0026#39; AND key_part3 = \u0026#39;c\u0026#39;;   EXPLAIN SELECT * FROM s1 WHERE key_part3 = \u0026#39;a\u0026#39;;  #练习： #varchar(10)变长字段且允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1)+1(NULL)+2(变长字段)  #varchar(10)变长字段且不允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1)+2(变长字段)  #char(10)固定字段且允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1)+1(NULL)  #char(10)固定字段且不允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1) 众所周知，记录是以行的形式进行保存的，Mysql5.1以后，行的保存格式默认为Compact格式。行记录Compact格式为：\n变长字段NULL标志位记录头信息列1数据列2数据列3数据\u0026hellip;\n第一个变长字段是记录这行的总字段长度，如果行记录的字段总长小于255字节，变长字段就占一个字节(一个字节有8位，8位的二进制最多能表示到255)。当大于255时，变长字段的长度就是两个字节。Mysql规定变成字段不超过两个字节，就意味着行的段总长最多不能超过65535个字节(两个字节有16位，16位最多能表示65535)。\nref  # 8. ref：当使用索引列等值查询时，与索引列进行等值匹配的对象信息。  #比如只是一个常数或者是某个列。   EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39;;    EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.id = s2.id;    EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s2.key1 = UPPER(s1.key1); rows # 9. rows：预估的需要读取的记录条数  # `值越小越好` ，越小，数据就更有可能在同一个叶中，进行的io次数就会越少  EXPLAIN SELECT * FROM s1 WHERE key1 \u0026gt; \u0026#39;z\u0026#39;; filtered # 10. filtered: 某个表经过搜索条件过滤后剩余记录条数的百分比   #如果使用的是索引执行的单表扫描，那么计算时需要估计出满足除使用  #到对应索引的搜索条件外的其他搜索条件的记录有多少条。  EXPLAIN SELECT * FROM s1 WHERE key1 \u0026gt; \u0026#39;z\u0026#39; AND common_field = \u0026#39;a\u0026#39;;    #对于单表查询来说，这个filtered列的值没什么意义，我们`更关注在连接查询  #中驱动表对应的执行计划记录的filtered值`，它决定了被驱动表要执行的次数(即：rows * filtered)  EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.common_field = \u0026#39;a\u0026#39;;  Extra #11. Extra:一些额外的信息  #更准确的理解MySQL到底将如何执行给定的查询语句   #当查询语句的没有`FROM`子句时将会提示该额外信息  EXPLAIN SELECT 1;    #查询语句的`WHERE`子句永远为`FALSE`时将会提示该额外信息  EXPLAIN SELECT * FROM s1 WHERE 1 != 1;    #当我们使用全表扫描来执行对某个表的查询，并且该语句的`WHERE`  #子句中有针对该表的搜索条件时，在`Extra`列中会提示上述额外信息。没有索引的时候显示useing where  EXPLAIN SELECT * FROM s1 WHERE common_field = \u0026#39;a\u0026#39;;    #当使用索引访问来执行对某个表的查询，并且该语句的`WHERE`子句中  #有除了该索引包含的列之外的其他搜索条件时，在`Extra`列中也会提示上述额外信息。  EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39; AND common_field = \u0026#39;a\u0026#39;;    #当查询列表处有`MIN`或者`MAX`聚合函数，但是并没有符合`WHERE`子句中  #的搜索条件的记录时，将会提示该额外信息  EXPLAIN SELECT MIN(key1) FROM s1 WHERE key1 = \u0026#39;abcdefg\u0026#39;;  #Select tables optimized away  EXPLAIN SELECT MIN(key1) FROM s1 WHERE key1 = \u0026#39;NlPros\u0026#39;; #NlPros 是 s1表中key1字段真实存在的数据   #select * from s1 limit 10;   #当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以  #使用覆盖索引的情况下，在`Extra`列将会提示该额外信息。比方说下边这个查询中只  #需要用到`idx_key1`而不需要回表操作：  EXPLAIN SELECT key1,id FROM s1 WHERE key1 = \u0026#39;a\u0026#39;;    #有些搜索条件中虽然出现了索引列，但却不能使用到索引  #看课件理解索引条件下推  #有些搜索条件中虽然出现了索引列，但却不能使用到索引，比如下边这个查询： #SELECT FROM s1 WHERE key1 \u0026gt;\u0026#39;z\u0026#39;AND key1 LIKE \u0026#39;%a\u0026#39; #其中的key1\u0026gt;\u0026#39;z\u0026#39;可以使用到索引，但是key1 LIKE%a\u0026#39;却无法使用到索引，在以前版本的MySQL中 #是按照下边步骤来执行这个查询的：  #1.先根据key1\u0026gt;\u0026#39;z\u0026#39;这个条件，从二级索引1dx_key1中获取到对应的二级索引记录。 #2.根据上一步骤得到的二级索引记录中的主键值进行回表，找到完整的用户记录再检测该记录是否符合 #key1LIKE\u0026#39;%a\u0026#39;这个条件，将符合条件的记录加入到最后的结果集。  #但是虽然key1 LIKE\u0026#39;%a\u0026#39;不能组成范围区间参与range访问方法的执行，但这个条件毕竟只涉及到了 #key1列，所以MySQL把上边的步骤改进了一下：  #1.先根据key1\u0026gt;\u0026#39;z\u0026#39;这个条件，定位到二级索引idx_key1中对应的二级索引记录。 #2.对于指定的二级索引记录，先不着急回表，而是先检测一下该记录是否满足key1 LIKE\u0026#39;%à\u0026#39;这个条件， #如果这个条件不满足，则该二级索引记录压根儿就没必要回表。 #3.对于满足key1LIKE\u0026#39;%a\u0026#39;这个条件的二级索引记录执行回表操作。 #我们说回表操作其实是一个随机I0，,比较耗时，所以上述修改虽然只改进了一点点，但是可以省去好多回表 ##操作的成本。MySQL把他们的这个改进称之为索引条件下推（英文名：Index Condition Pushdown)。 #如果在查询语句的执行过程中将要使用索引条件下推这个特性，在Extra列中将会显示Using index #condition,比如这样：  EXPLAIN SELECT * FROM s1 WHERE key1 \u0026gt; \u0026#39;z\u0026#39; AND key1 LIKE \u0026#39;%a\u0026#39;;    #在连接查询执行过程中，当被驱动表不能有效的利用索引加快访问速度，MySQL一般会为  #其分配一块名叫`join buffer`的内存块来加快查询速度，也就是我们所讲的`基于块的嵌套循环算法`  #见课件说明  EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.common_field = s2.common_field;    #当我们使用左（外）连接时，如果`WHERE`子句中包含要求被驱动表的某个列等于`NULL`值的搜索条件，  #而且那个列又是不允许存储`NULL`值的，那么在该表的执行计划的Extra列就会提示`Not exists`额外信息  EXPLAIN SELECT * FROM s1 LEFT JOIN s2 ON s1.key1 = s2.key1 WHERE s2.id IS NULL;    #如果执行计划的`Extra`列出现了`Using intersect(...)`提示，说明准备使用`Intersect`索引  #合并的方式执行查询，括号中的`...`表示需要进行索引合并的索引名称；  #如果出现了`Using union(...)`提示，说明准备使用`Union`索引合并的方式执行查询；  #出现了`Using sort_union(...)`提示，说明准备使用`Sort-Union`索引合并的方式执行查询。  EXPLAIN SELECT * FROM s1 WHERE key1 = \u0026#39;a\u0026#39; OR key3 = \u0026#39;a\u0026#39;;    #当我们的`LIMIT`子句的参数为`0`时，表示压根儿不打算从表中读出任何记录，将会提示该额外信息  EXPLAIN SELECT * FROM s1 LIMIT 0;    #有一些情况下对结果集中的记录进行排序是可以使用到索引的。  #比如：  EXPLAIN SELECT * FROM s1 ORDER BY key1 LIMIT 10;    #很多情况下排序操作无法使用到索引，只能在内存中（记录较少的时候）或者磁盘中（记录较多的时候）  #进行排序，MySQL把这种在内存中或者磁盘上进行排序的方式统称为文件排序（英文名：`filesort`）。   #如果某个查询需要使用文件排序的方式执行查询，就会在执行计划的`Extra`列中显示`Using filesort`提示  EXPLAIN SELECT * FROM s1 ORDER BY common_field LIMIT 10;    #在许多查询的执行过程中，MySQL可能会借助临时表来完成一些功能，比如去重、排序之类的，比如我们  #在执行许多包含`DISTINCT`、`GROUP BY`、`UNION`等子句的查询过程中，如果不能有效利用索引来完成  #查询，MySQL很有可能寻求通过建立内部的临时表来执行查询。如果查询中使用到了内部的临时表，在执行  #计划的`Extra`列将会显示`Using temporary`提示  EXPLAIN SELECT DISTINCT common_field FROM s1;   #EXPLAIN SELECT DISTINCT key1 FROM s1;   #同上。  EXPLAIN SELECT common_field, COUNT(*) AS amount FROM s1 GROUP BY common_field;   #执行计划中出现`Using temporary`并不是一个好的征兆，因为建立与维护临时表要付出很大成本的，所以  #我们`最好能使用索引来替代掉使用临时表`。比如：扫描指定的索引idx_key1即可  EXPLAIN SELECT key1, COUNT(*) AS amount FROM s1 GROUP BY key1;  #json格式的explain EXPLAIN FORMAT=JSON SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key2 WHERE s1.common_field = \u0026#39;a\u0026#39;; 小结\n EXPLAIN不考虑各种Cache EXPLAIN不能显示MySQL在执行查询时所作的优化工作 EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况 部分统计信息是估算的，并非精确值  EXPLAIN的进一步使用 EXPLAIN****四种输出格式\n这里谈谈EXPLAIN的输出格式。EXPLAIN可以输出四种格式：\n传统格式 ， JSON格式 ， TREE格式 以及可 视化输出 。用户可以根据需要选择适用于自己的格式。\n索引优化与查询优化 todo\n 2022-3-29 19:27:294\n数据库的设计规范 为什么需要数据库设计 数据库的范式设计越高阶，冗余度就越低，同时高阶的范式一定符合低阶范式的要求，满足最低要求的范式是第一范式(1NF)。在第一范式的基础上进一步满足更多规范要求的称为第二范式(2NF),其余范式以次类推。一般来说，在关系型数据库设计中，最高也就遵循到BCNF,普遍还是3NF。但也不绝对，有时候为了提高某些查询性能，我们还需要破坏范式规侧也就是反规范化。\n范 式 范式简介 **在关系型数据库中，关于数据表设计的基本原则、规则就称为范式。**可以理解为，一张数据表的设计结构需要满足的某种设计标准的级别 。要想设计一个结构合理的关系型数据库，必须满足一定的范式。范式的定义会使用到主键和候选键，数据库中的键(Ky)）由一个或者多个属性组成。\n范式都包括哪些 目前关系型数据库有六种常见范式，按照范式级别，从低到高分别是：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF，又称完美范式）。\n键和相关属性的概念 数据表中常用的几种键和属性的定义：\n 超键：能唯一标识元组的属性集叫做超键。 候选键：如果超键不包括多余的属性，那么这个超键就是候选键。 主键：用户可以从候选键中选择一个作为主键。 外键：如果数据表R1中的某属性集不是R1的主键，而是另一个数据表R2的主键，那么这个属性集就是数据表R1的外键。 主属性：包含在任一候选键中的属性称为主属性。 非主属性：与主属性相对，指的是不包含在任何一个候选键中的属性。  通常，我们也将候选键称之为“码”，把主键也称为“主码”。因为键可能是由多个属性组成的，针对单个属性，我 们还可以用主属性和非主属性来进行区分\n举例：\n这里有两个表：\n球员表(player) ：球员编号 | 姓名 | 身份证号 | 年龄 | 球队编号\n球队表(team) ：球队编号 | 主教练 | 球队所在地\n 超键 ：对于球员表来说，超键就是包括球员编号或者身份证号的任意组合，比如（球员编号）（球员编号，姓名）（身份证号，年龄）等。 候选键 ：就是最小的超键，对于球员表来说，候选键就是（球员编号）或者（身份证号）。 主键 ：我们自己选定，也就是从候选键中选择一个，比如（球员编号）。 外键 ：球员表中的球队编号。 主属性 、 非主属性 ：在球员表中，主属性是（球员编号）（身份证号），其他的属性（姓名）（年龄）（球队编号）都是非主属性。  第一范式(1st NF) 第一范式主要是确保数据表中每个字段的值必须具有原子性，也就是说数据表中每个字段的值为不可再次拆分的最小数据单元。我们在设计某个字段的时候，对于字段X来说，不能把字段X拆分成字段X-1和字段X-2。事实上，任何的DBMS都会满足第一范式的要求，不会将字段进行拆分。\n举例1：\n假设一家公司要存储员工的姓名和联系方式。它创建一个如下表：\n该表不符合 1NF ，因为规则说“表的每个属性必须具有原子（单个）值”，lisi和zhaoliu员工的emp_mobile 值违反了该规则。为了使表符合 1NF ，我们应该有如下表数据：\n属性的原子性是 主观的 。例如，城市全称是否需要分开呢？答案取决于业务。\n第二范式(2nd NF) 第二范式要求，在满足第一范式的基础上，还要**满足数据表里的每一条数据记录，都是可唯一标识的（要有主键）。而且所有非主键字段，都必须完全依赖主键，不能只依赖主键的一部分。**如果知道主键的所有属性的值，就可以检索到任何元组（行）的任何属性的任何值。(要求中的主键，其实可以拓展替换为候选键)。\n==举例1==：\n成绩表 （学号，课程号，成绩）关系中，（学号，课程号）可以决定成绩，但是学号不能决定成绩，课程号也不能决定成绩，所以（学号，课程号）→成绩 就是 完全依赖关系 。\n==举例2==：\n比赛表 player_game ，里面包含球员编号、姓名、年龄、比赛编号、比赛时间和比赛场地等属性，这里候选键和主键都为（球员编号，比赛编号），我们可以通过候选键（或主键）来决定如下的关系：\n(球员编号, 比赛编号) → (姓名, 年龄, 比赛时间, 比赛场地，得分) 但是这个数据表不满足第二范式，因为数据表中的字段之间还存在着如下的对应关系：\n姓名年龄完全取决于球员编号，和比赛编号没有关系，同理比赛时间, 比赛场地。\n(球员编号) → (姓名，年龄) (比赛编号) → (比赛时间, 比赛场地) 对于非主属性来说，并非完全依赖候选键。这样会产生一下问题：\n 数据冗余 ：  如果一个球员可以参加 m 场比赛，那么球员的姓名和年龄就重复了 m-1 次。一个比赛也可能会有 n 个球员参加，比赛的时间和地点就重复了 n-1 次。\n  插入异常 ：\n如果我们想要添加一场新的比赛，但是这时还没有确定参加的球员都有谁，那么就没法插入。\n  删除异常 ：\n如果我要删除某个球员编号，如果没有单独保存比赛表的话，就会同时把比赛信息删除掉。\n  更新异常 ：\n如果我们调整了某个比赛的时间，那么数据表中所有这个比赛的时间都需要进行调整，否则就会出现一场比赛时间不同的情况。\n  因此我们可以把球员比赛表设计为下面的三张表：\n 1NF 告诉我们字段属性需要是原子性的，而 2NF 告诉我们一张表就是一个独立的对象，一张表只表达一个意思。\n 第三范式(3rdNF) 第三范式是在第二范式的基础上，要求数据表中不能存有非主属性A依赖于非主属性B,非主属性B依赖于主键C的情况，即存在“A→B→C”的决定关系)，通俗地讲，==所有非主键属性之间不能有依赖关系，必须相互独立。==\n这里的主键可以拓展为候选键。\n商品类别名称依赖于商品类别编号，不符合第三范式。\n符合3NF后的数据模型通俗地讲，2NF和3NF通常以这句话概括：“每个非键属性依赖于键，依赖于整个键，并且除了键别无他物”。\n小结 关于数据表的设计，有三个范式要遵循。\n  第一范式(1NF),==确保每列保持原子性== 数据库的每一列都是不可分割的原子数据项，不可再分的最小数据单元，而不能是集合、数组、记录等非原子数据项。\n  第二范式(2NF),==确保每列都和主键完全依赖==\n尤其在复合主键的情况下，非主键部分不应该依赖于部分主键。\n  第三范式(3NF)==确保每列都和主键列直接相关，而不是间接相关==\n  范式的优点：数据的标准化有助于消除数据库中的数据冗余，第三范式(3N)通常被认为在性能、扩展性和数据完整性方面达到了最好的平衡。\n  范式的缺点：范式的使用，可能降低查询的效率。因为范式等级越高，设计出来的数据表就越多、越精细，数据的冗余度就越低，进行数据查询的时候就可能需要关联多张表，这不但代价昂贵，也可能使一些索引策略无效。\n  反范式化 视频 p153——尚硅谷\n有的时候不能简单按照规范要求设计数据表，因为有的数据看似冗余，其实对业务来说十分重要。这时候，我们就要**遵循业务优先的原则，首先满足业务需求，再尽量减少冗余。**顶多违反第二三范式，第一范式还是要遵循的。\n如果数据库中的数据量比较大，系统的UV和PV访问频次比较高，三大范式在一定程度上会影响数据库的读性能。因此反范式优化也是一种优化思路。此时，可以通过在数据表中增加冗余字段来提高数据库的读性能。\n  为满足某种商业目标 , 数据库性能比规范化数据库更重要 在数据规范化的同时 , 要综合考虑数据库的性能 通过在给定的表中添加额外的字段，以大量减少需要从中搜索信息所需的时间 通过在给定的表中插入计算列，以方便查询     举例1：\n第三范式当中举例，如果需要经常查询商品的名称，大可将增加冗余的字段商品名称。\n  反范式的新问题  存储 空间变大 了 一个表中字段做了修改，另一个表中冗余的字段也需要做同步修改，否则 数据不一致 若采用存储过程来支持数据的更新、删除等额外操作，如果更新频繁，会非常 消耗系统资源 在 数据量小 的情况下，反范式不能体现性能的优势，可能还会让数据库的设计更加 复杂  反范式的适用场景 当冗余信息有价值或者能 大幅度提高查询效率 的时候，我们才会采取反范式的优化。\n  增加冗余字段的建议\n只有满足这两个条件，才可以考虑增加==冗余字段==。\n 这个冗余字段==不需要经常进行修改；== 这个冗余字段==查询的时候不可或缺。==    历史快照、历史数据的需要\n  在现实生活中，我们经常需要一些冗余信息，比如订单中的收货人信息，包括姓名、电话和地址等。每次发生的 订单收货信息 都属于历史快照 ，需要进行保存，但用户可以随时修改自己的信息，这时保存这些冗余信息是非常有必要的。\n 反范式优化也常用在 数据仓库的设计中，因为数据仓库通常 存储历史数据 ，对增删改的实时性要求不强，对历史数据的分析需求强。这时适当允许数据的冗余度，更方便进行数据分析。\n我简单总结下数据仓库和数据库在使用上的区别：\n  数据库设计的目的在于捕获数据，而数据仓库设计的目的在于分析数据；\n  数据库对数据的增删改实时性要求强，需要存储在线的用户数据，而数据仓库存储的一般是历史数据：\n  数据库设计需要尽量避免冗余，但为了提高查询效率也允许一定的冗余度，而数据仓库在设计上更偏向采用反范式设计\n   BCNF(巴斯范式) 视频 p154 ———— 尚硅谷\n巴斯范式(BCNF),也叫做巴斯-科德范式(Boyce-Codd Normal Form)。BCNF被认为没有新的设计规范加入，只是对第三范式中设计规范要求更强，使得数据库冗余度更小。所以，称为是修正的第三范式。\n若一个关系达到了第三范式，并且它只有一个候选键，或者它的每个候选键都是单属性，则该关系自然达到BC范式。\n一般来说，一个数据库设计符合3NF或BCNF就可以了。\n举例1：\n在这个表中，一个仓库只有一个管理员，同时一个管理员也只管理一个仓库。我们先来梳理下这些属性之间的依赖关系。\n仓库名决定了管理员，管理员也决定了仓库名，同时（仓库名，物品名）的属性集合可以决定数量这个属性。这样，我们就可以找到数据表的候选键。\n 候选键 ：是（管理员，物品名）和（仓库名，物品名），然后我们从候选键中选择一个作为 主键 ，比如（仓库名，物品名）。 主属性 ：包含在任一候选键中的属性，也就是仓库名，管理员和物品名。 非主属性 ：数量这个属性。  符合三范式\n我们需要根据范式的等级，从低到高来进行判断。\n 首先，数据表每个属性都是原子性的，符合 1NF 的要求； 其次，数据表中非主属性”数量“都与候选键全部依赖，（仓库名，物品名）决定数量，（管理员，物品名）决定数量。因此，数据表符合 2NF 的要求； 最后，数据表中的非主属性，不传递依赖于候选键。因此符合 3NF 的要求。  存在的问题\n 增加一个仓库，但是还没有存放任何物品。根据数据表实体完整性的要求，主键不能有空值，因此会出现 插入异常 ； 如果仓库更换了管理员，我们就可能会 修改数据表中的多条记录 ； 如果仓库里的商品都卖空了，那么此时仓库名称和相应的管理员名称也会随之被删除。  即便数据表符合 3NF 的要求，同样可能存在插入，更新和删除数据的异常情况。\n问题解决\n首先我们需要确认造成异常的原因：==主属性仓库名对于候选键（管理员，物品名）是部分依赖的关系，这样就有可能导致上面的异常情况。因此引入BCNF，它在3NF的基础上消除了主属性对候选键的部分依赖或者传递依赖关系。==\n如果在关系R中，U为主键，B属性是主键的一个属性，若存在B-\u0026gt;A，A为主属性，则该关系不属于BCNF。\n根据 BCNF 的要求，我们需要把仓库管理关系 warehouse_keeper 表拆分成下面这样：\n  仓库表 ：（仓库名，管理员）\n  库存表 ：（仓库名，物品名，数量）\n  这样就不存在主属性对于候选键的部分依赖或传递依赖，上面数据表的设计就符合 BCNF。\n第四范式 todo\n第五范式、域键范式 todo\n实战案例 视频 p155 ———— 尚硅谷\n商超进货系统中的进货单表进行剖析：\n进货单表：\n这个表中的字段很多，表里的数据量也很惊人。大量重复导致表变得庞大，效率极低。如何改造？\n在实际工作场景中，这种由于数据表结构设计不合理，而导致的数据重复的现象并不少见。往往是系统虽然能够运行，承载能力却很差，稍微有点流量，就会出现内存不足、CUP使用率飙升的情况，甚至会导致整个项目失败。\n 2022-3-30\nER模型 视频 p156 ———— 尚硅谷\n数据库设计是牵一发而动全身的。那有没有什么办法提前看到数据库的全貌呢？比如需要哪些数据表、 数据表中应该有哪些字段，数据表与数据表之间有什么关系、通过什么字段进行连接，等等。这样我们才能进行整体的梳理和设计。\n其实，ER模型就是一个这样的工具。ER模型也叫作实体关系模型，是用来描述现实生活中客观存在的事物、事物的属性，以及事物之间关系的一种数据模型。在开发基于数据库的信息系统的设计阶段，通常使用E模型来描述信息需求和信息特性，帮助我们理清业务逻辑，从而设计出优秀的数据库。\n特别说明：\n==工作中数据库最好不要用外键，如上所说非常影响性能，我们可以采取应用层面数据的一致性检查：例如选课系统，如果数据库中不做外键处理，我们可以在前端后端做校验处理，让用户只能输入相应的课程（或者采用下拉菜单选择的方式选择），从而实现外键的作用。==\n设计原则 三少一多：\n 数据==表的个数==越少越好 RDBMS的核心在于对实体和联系的定义，也就是E-R图(Entity Relationship Diagram),数据表越少，证明实体 和联系设计得越简洁，既方便理解又方便操作。 数据表中的==字段==个数越少越好 字段个数越多，数据沉余的可能性越大。设置字段个数少的前提是各个字段相互独立，而不是某个字段的取值可 以由其他字段计算出来。当然字段个数少是相对的，我们通常会在数据冗余和检索效率中进行平衡。 数据表中==联合主键的字段==个数越少越好 设置主键是为了确定唯一性，当一个字段无法确定唯一性的时候，就需要采用联合主键的方式（也就是用多个字 段来定义一个主键)。联合主键中的字段越多，占用的索引空间越大，不仅会加大理解难度，还会增加运行时间 和索引空间，因此联合主键的字段个数越少越好。 使用==主键和代码逻辑层外键关系（不是外键约束）==越多越好 数据库的设计实际上就是定义各种表，以及各种字段之间的关系。这些关系越多，证明这些实体之间的冗余度越 低，利用度越高。这样做的好处在于不仅保证了数据表之间的独立性，还能提升相互之间的关联使用率。  “三少一多”原则的核心就是简单可复用。简单指的是用更少的表、更少的字段、更少的联合主键字段来完成数据 表的设计。可复用则是通过主键、外键的使用来增强数据表之间的复用率。因为一个主键可以理解是一张表的代 表。键设计得越多，证明它们之间的利用率越高。\n数据对象的编写建议 todo\n数据库对象编写建议 todo\n事务 事务概述 存储引擎支持情况 SHOW ENGINES 命令来查看MySQL 中，只有InnoDB 是支持事务的。\n基本概念 **事务：**一组逻辑操作单元，使数据从一种状态变换到另一种状态。\n**事务处理的原则：**保证所有事务都作为 一个工作单元 来执行，即使出现了故障，都不能改变这种执行方式。当在一个事务中执行多个操作时，要么所有的事务都被提交( commit )，那么这些修改就 永久 地保存下来；要么数据库管理系统将 放弃 所作的所有 修改 ，整个事务回滚( rollback )到最初状态。\n事务的ACID特性   原子性（atomicity）：\n原子性是指==事务是一个不可分割的工作单位，要么全部提交，要么全部失败回滚。==\n  一致性（consistency）：\n（国内很多网站上对一致性的阐述有误，具体你可以参考 Wikipedia 对Consistency的阐述）根据定义，一致性是指事务执行前后，数据从一个 合法性状态 变换到另外一个 合法性状态 。这种状态是 语义上 的而不是语法上的，跟具体的业务有关。通俗的说，就是==事务前后数据状态符合事实约束，符合常理==\n那什么是合法的数据状态呢？满足 预定的约束 的状态就叫做合法的状态。通俗一点，这状态是由你自己来定义的（比如满足现实世界中的约束）。满足这个状态，数据就是一致的，不满足这个状态，数据就是不一致的！如果事务中的某个操作失败了，系统就会自动撤销当前正在执行的事务，返回到事务操作之前的状态。\n 举例1：A账户有200元，转账300元出去，此时A账户余额为-100元。你自然就发现了此时数据是不一致的，为什么呢？因为你定义了一个状态，余额这列必须\u0026gt;0。 举例2：A账户200元，转账50元给B账户，A账户的钱扣了，但是B账户因为各种意外，余额并没有增加。你也知道此时数据是不一致的，为什么呢？因为你定义了一个状态，要求A+B的总余额必须不变。 举例3：在数据表中我们将姓名字段设置为唯一性约束，这时当事务进行提交或者事务发生回滚的时候，如果数据表中的姓名不唯一，就破坏了事务的一致性要求。    隔离型（isolation）：\n事务的隔离性是指一个==事务的执行不能被其他事务干扰== ，即一个事务内部的操作及使用的数据对 并发 的\n其他事务是隔离的，并发执行的各个事务之间不能互相干扰。\n如果无法保证隔离性会怎么样？假设A账户有200元，B账户0元。A账户往B账户转账两次，每次金额为50\n元，分别在两个事务中执行。如果无法保证隔离性，会出现下面的情形：\nUPDATE accounts SET money = money - 50 WHERE NAME = \u0026#39;AA\u0026#39;; UPDATE accounts SET money = money + 50 WHERE NAME = \u0026#39;BB\u0026#39;;   持久性（durability）：\n  持久性是指一个==事务一旦被提交，它对数据库中数据的改变就是 永久性的==，接下来的其他操作和数据库故障不应该对其有任何影响。\n==持久性是通过 事务日志 来保证的==。日志包括了 重做日志 和 回滚日志 。当我们通过事务对数据进行修改的时候，首先会将数据库的变化信息记录到重做日志中，然后再对数据库中对应的行进行修改。这样做的好处是，即使数据库系统崩溃，数据库重启后也能找到没有更新到数据库系统中的重做日志，重新执行，从而使事务具有持久性\n ==原子性是基础，隔离性是手段，1一致性是约束条件，而持久性是我们的目的。==\n 事务的状态 我们现在知道 事务 是一个抽象的概念，它其实对应着一个或多个数据库操作，MySQL根据这些操作所执行的不同阶段把 事务 大致划分成几个状态：\n  活动的（active） 事务对应的数据库操作正在执行过程中时，我们就说该事务处在 活动的 状态。\n  部分提交的（partially committed） 当事务中的最后一个操作执行完成，但由于操作都在内存中执行，所造成的影响并 没有刷新到磁盘时，我们就说该事务处在 部分提交的 状态。\n  失败的（failed） 当事务处在 活动的 或者 部分提交的 状态时，可能遇到了某些错误（数据库自身的错误、操作系统错误或者直接断电等）而无法继续执行，或者人为的停止当前事务的执行，我们就说该事务处在 失 败的 状态。\n  中止的（aborted） 如果事务执行了一部分而变为 失败的 状态，那么就需要把已经修改的事务中的操作还原到事务执行前的状态。换句话说，就是要撤销失败事务对当前数据库造成的影响。我们把这个撤销的过程称之为 回滚 。当 回滚 操作执行完毕时，也就是数据库恢复到了执行事务之前的状态，我们就说该事务处在了 中止的 状态。\n  提交的（committed） 当一个处在 部分提交的 状态的事务将修改过的数据都 同步到磁盘 上之后，我们就可以说该事务处在了 提交的 状态。\n  使用事务 分别为 显式事务 和 隐式事务 。\n显示事务  步骤1： START TRANSACTION 或者 BEGIN ，作用是显式开启一个事务。  mysql\u0026gt; BEGIN; #或者 mysql\u0026gt; START TRANSACTION; START TRANSACTION 语句相较于 BEGIN 特别之处在于，后边能跟随几个 修饰符 ：\n  READ ONLY：标识当前事务是一个 只读事务 ，也就是属于该事务的数据库操作只能读取数据，而不 能修改数据。\n 补充：只读事务中只是不允许修改那些其他事务也能访问到的表中的数据，对于临时表来说（我们使用 CREATE TMEPORARY TABLE创建的表)，由于它们只能在当前会话中可见，所以只读事务其实也是可以对临时表进行增、删、改操作的。\n   READ WRITE：标识当前事务是一个 读写事务 ，也就是属于该事务的数据库操作既可以读取数据， 也可以修改数据。\n  WITH CONSISTENT SNAPSHOT：启动一致性读。\n   步骤2：一系列事务中的操作（==主要是DML，不含DDL==）\n  步骤3：提交事务 或 中止事务（即回滚事务）\n# 提交事务。当提交事务后，对数据库的修改是永久性的。 mysql\u0026gt; COMMIT; # 回滚事务。即撤销正在进行的所有没有提交的修改 mysql\u0026gt; ROLLBACK; # 将事务回滚到某个保存点。 mysql\u0026gt; ROLLBACK TO [SAVEPOINT] 其中SAVEPOINT（关键点，使得不用全部回滚）相关操作\n#在事务中创建保存点，方便后续针对保存点进行回滚。一个事务中可以存在多个保存点。 SAVEPOINT 保存点名称； #朋除某个保存点。 RELEASE SAVEPOINT 保存点名称：   隐式事务 MySQL中有一个系统变量 autocommit ：\nmysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;autocommit\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | autocommit | ON | +---------------+-------+ 1 row in set (0.01 sec) 默认情况下，如果我们不显式的使用START TRANSACTION或者BEGIN语句开启一个事务，那么每一条语句都算是一个独立的事务，这种特性称之为事务的自动提交。\n若想不自动提交，就使用显示事务（ START TRANSACTION 或者 BEGIN）并且关闭 自动提交：\nSET autocommit = OFF; #或 SET autocommit = 0; 我们显式的写出C0MMIT语句来把这个事务提交掉，或者显式的写出ROLLBACK语句来把这个事务回滚掉。\n隐式提交数据的情况   数据定义语言（Data definition language，缩写为：DDL）\n  隐式使用或修改mysql数据库中的表\n当我们使用ALTER USER、CREATE USER、DROP USER、GRANT、RENAME USER、REVOKE、SET\rPASSWORD等语句时也会隐式的提交前边语句所属于的事务。\n  事务控制或关于锁定的语句\n  当我们在一个事务还没提交或者回滚时就又使用 START TRANSACTION 或者 BEGIN 语句开启了 另一个事务时，会 隐式的提交 上一个事务。\n  当前的 autocommit 系统变量的值为 OFF ，我们手动把它调为 ON 时，也会 隐式的提交 前边语 句所属的事务。\n  使用LOCK TABLES 、 UNLOCK TABLES 等关于锁定的语句也会 隐式的提交 前边语句所属的事\n务。\n    加载数据的语句\n使用LOAD DATA语句来批量往数据库中导入数据时，也会隐式的提交前边语句所属的事务。\n  关于MySQL复制的一些语句\n使用START SLAVE、STOP SLAVE、RESET SLAVE、CHANGE MASTER TO等语句时会隐式的提交前边语句所属的事务。\n  其它的一些语句\n使用ANALYZE TABLE、CACHE INDEX、CHECK TABLE、FLUSH、LOAD INDEX INTO CACHE、OPTIMIZE TABLE、REPAIR TABLE、RESET等语句也会隐式的提交前边语句所属的事务。\n   2022-4-1\n使用举例1：提交与回滚 事务执行参数completion._type：\nMySQL中completion._type参数的作用，实际上这个参数有3种可能：\nSET @@completion_type = 1;  completion=0（NO CHAIN），这是默认情况。当我们执行COMMIT的时候会提交事务，在执行下一个事务时，还需要使 用START TRANSACTION或者BEGIN来开启. completion=1，这种情况下，当我们提交事务后，相当于执行了COMMIT AND CHAIN，也就是开启一个 链式事务，即当我们提交事务之后会开启一个相同隔离级别的事务。 completion=2，这种情况下COMMIT=COMMIT AND RELEASE，也就是当我们提交后，会自动与服务器断 开连接。   当我们设置 autocommit=0 时，不论是否采用 START TRANSACTION 或者 BEGIN 的方式来开启事务，都需要用 COMMIT 进行提交，让事务生效，使用 ROLLBACK 对事务进行回滚。\n当我们设置 autocommit=1 时，每条 SQL 语句都会自动进行提交。 不过这时，如果你采用 START TRANSACTION 或者 BEGIN 的方式来显式地开启事务，那么这个事务只有在 COMMIT 时才会生效，在 ROLLBACK 时才会回滚。\n 事务的隔离级别 视频 p164 ———— 尚硅谷\nMySQL是一个 客户端／服务器 架构的软件，对于同一个服务器来说，可以有若干个客户端与之连接，每个客户端与服务器连接上之后，就可以称为一个会话（ Session ）。每个客户端都可以在自己的会话中向服务器发出请求语句，一个请求语句可能是某个事务的一部分，也就是对于服务器来说可能同时处理多个事务。事务有 隔离性 的特性，理论上在某个事务 对某个数据进行访问 时，其他事务应该进行 排 队 ，当该事务提交之后，其他事务才可以继续访问这个数据。但是这样对 性能影太大 ，我们既想保持事务的隔离性，又想让服务器在处理访问同一数据的多个事务时 性能尽量高些 ，那就看二者如何权衡取舍了。\n数据准备 CREATE TABLE student ( \tstudentno INT,  NAME VARCHAR ( 20 ), \tclass VARCHAR ( 20 ),  PRIMARY KEY ( studentno ) ) ENGINE = INNODB CHARSET = utf8;  INSERT INTO student VALUES(1, \u0026#39;小谷\u0026#39;, \u0026#39;1班\u0026#39;); 数据并发问题 下访问相同数据的事务在 不保证串行执行 （也就是执行完一个再执行另一个）的情况下可能会出现哪些问题：\n 脏写（ Dirty Write ） 脏读（ Dirty Read ） 不可重复读（ Non-Repeatable Read ） 幻读（ Phantom ）  脏写 对于两个事务 Session A、Session B，如果事务Session A 修改了 另一个 未提交 事务Session B 修改过 的数据，那就意味着发生了 脏写\n脏读 对于两个事务 Session A、Session B，Session A 读取 了已经被 Session B 更新 但还 没有被提交 的字段。 之后若 Session B 回滚 ，Session A 读取 的内容就是 临时且无效 的。\nSession A和Session B各开启了一个事务，Session B中的事务先将studentno列为1的记录的name列更新为\u0026rsquo;张三\u0026rsquo;，然后Session A中的事务再去查询这条studentno为1的记录，如果读到列name的值为\u0026rsquo;张三\u0026rsquo;，而Session B中的事务稍后进行了回滚，那么Session A中的事务相当于读到了一个不存在的数据，这种现象就称之为 脏读 。\n不可重复读 (简单系统往往视为正常，并发系统中不可重复读是个问题)对于两个事务Session A、Session B，Session A 读取 了一个字段，然后 Session B 更新 了该字段。 之后Session A 再次读取 同一个字段， 值就不同 了。那就意味着发生了不可重复读。\n我们在Session B中提交了几个 隐式事务 （注意是隐式事务，意味着语句结束事务就提交了），这些事务都修改了studentno列为1的记录的列name的值，每次事务提交之后，如果Session A中的事务都可以查看到最新的值，这种现象也被称之为 不可重复读 。\n幻读 对于两个事务Session A、Session B, Session A 从一个表中 读取 了一个字段, 然后 Session B 在该表中 插 入 了一些新的行。 之后, 如果 Session A 再次读取 同一个表, 就会多出几行。那就意味着发生了幻读。\nSession A中的事务先根据条件 studentno \u0026gt; 0这个条件查询表student，得到了name列值为\u0026rsquo;张三\u0026rsquo;的记录；之后Session B中提交了一个 隐式事务 ，该事务向表student中插入了一条新记录；之后Session A中的事务再根据相同的条件 studentno \u0026gt; 0查询表student，得到的结果集中包含Session B中的事务新插入的那条记录，这种现象也被称之为 幻读 。我们把新插入的那些记录称之为 幻影记录 。\n注意1 有的同学会有疑问，那如果Session B中删除了一些符合studentno\u0026gt;g的记录而不是插入新记录，那Session A 之后再根据studentno\u0026gt;0的条件读取的记录变少了，这种现象算不算幻读呢？这种现象不属于幻读，幻读强 调的是多了记录。\n注意2 那对于先前已经读到的记录，之后又读取不到这种情况相当于对每一条记录都发生了 不可重复读的 现象。幻读只是重点强调了读取到了之前读取没有获取到的记录。\nSQL中的四种隔离级别 严重性来排序：脏写 \u0026gt; 脏读 \u0026gt; 不可重复读 \u0026gt; 幻读。\n隔离性和性能都要兼顾。设立一些隔离级别，隔离级别越低，并发问题发生的就越多。\nSQL标准 中设立了4个 隔离级别 ： (以下4种隔离级别都解决的脏写的问题)\n READ UNCOMMITTED ：读未提交，在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。不能避免脏读、不可重复读、幻读。 READ COMMITTED ：读已提交，它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。可以避免脏读，但不可重复读、幻读问题仍然存在。 REPEATABLE READ ：可重复读，事务A在读到一条数据之后，此时事务B对该数据进行了修改并提交，那么事务A再读该数据，读到的还是原来的内容。可以避免脏读、不可重复读，但幻读问题仍然存在。这是MySQL的默认隔离级别。 SERIALIZABLE ：可串行化，确保事务可以从一个表中读取相同的行。在这个事务持续期间，禁止其他事务对该表执行插入、更新和删除操作。所有的并发问题都可以避免，但性能十分低下。能避免脏读、不可重复读和幻读。  MySQL支持的四种隔离级别 视频 p165 ———— 尚硅谷\n  Oracle就只支持READ COMMITTED(默认隔离级别)和SERIALIZABLE隔离级别。\n  MySQL虽然支持4种隔离级别，但与SQL标准中所规定的各级隔离级别 允许发生的问题却有些出入，MySQL在 REPEATABLE READ隔离级别下，是可以禁止幻读问题的发生的，禁止幻读 的原因在后面的章节中。\n  查看隔离级别：\n  MySQL 5.7.20版本之前\nSHOW VARIABLES LIKE \u0026#39;tx_isolation\u0026#39;;   MySQL 5.7.20版本之后\nSHOW VARIABLES LIKE \u0026#39;transaction_isolation\u0026#39;;   或者不同MySQL版本中都可以使用的：\nSELECT @@transaction_isolation;   如何设置事务的隔离级别 SET [GLOBAL|SESSION] TRANSACTION_ISOLATION = \u0026#39;隔离级别\u0026#39; #其中，隔离级别格式： \u0026gt; READ-UNCOMMITTED \u0026gt; READ-COMMITTED \u0026gt; REPEATABLE-READ \u0026gt; SERIALIZABLE 关于设置时使用GLOBAL或SESSION的影响：   使用 GLOBAL 关键字（在全局范围影响）：\nSET GLOBAL TRANSACTION_ISOLATION = \u0026#39;SERIALIZABLE\u0026#39;;  当前已经存在的会话无效 只对执行完该语句之后产生的会话起作用    使用 SESSION 关键字（在会话范围影响）：\nSET SESSION TRANSACTION_ISOLATION = \u0026#39;SERIALIZABLE\u0026#39;;  对当前会话的所有后续的事务有效 如果在事务之间执行，则对后续的事务有效 该语句可以在已经开启的事务中间执行，但不会影响当前正在执行的事务    修改配置文件：\n如果在服务器启动时想改变事务的默认隔离级别，可以修改启动参数transaction_isolation的值。比如，在 启动服务器时指定了transaction_isplation=SERIALIZABLE,那么事务的默认隔离级别就从原来的 REPEATABLE-READ变成了SERIALIZABLE。\n   小结：\n数据库规定了多种事务隔离级别，不同隔离级别对应不同的干扰程度，隔离级别越高，数据一致性 就越好，但并发性越弱。\n 不同隔离级别举例 演示1. 读未提交之脏读（解决了脏写） 设置隔离级别为未提交读：\n事务1和事务2的执行流程如下：\n演示2：读已提交（解决了脏写，脏读） 视频 p167 ———— 尚硅谷\n演示3：可重复读（解决了脏写，脏读，不可重复读） 视频 p167 ———— 尚硅谷\n设置隔离级别为可重复读，事务的执行流程如下：\n演示4：幻读 视频 p168 ———— 尚硅谷\nrepeatable read 解决不了幻读，但是mysql可以避免幻读，看后续\n事务1中事务开启后，事务2 提交， 事务1中查询还是查不到的，但是如果在事务1中插入相同的数据，会报错，从而证明了发生了幻读。\n事务的常见分类  扁平事务（Flat Transactions） 带有保存点的扁平事务（Flat Transactions with Savepoints） 链事务（Chained Transactions） 嵌套事务（Nested Transactions） 分布式事务（Distributed Transactions）  扁平事务 最简单的一种，但是在实际生产环境中，这可能是使用最频繁的事务，在扁平事务 中，所有操作都处于同一层次，其由BEGIN WORK开始，由COMMIT WORK或ROLLBACK WORK结束，其间的操作是 原子的，要么都执行，要么都回滚，因此，扁平事务是应用程序成为原子操作的基本组成模块。扁平事务虽然简单，但是在实际环境中使用最为频繁，也正因为其简单，使用频繁，故每个数据库系统都实现了对扁平事务的支持。扁平事务的主要限制是不能提交或者回滚事务的某一部分，或分几个步骤提交。\n扁平事务一般有三种不同的结果：\n 事务成功完成。在平常应用中约占所有事务的96%。 应用程序要求停止事 务。比如应用程序在捕获到异常时会回滚事务，约占事务的3%。 外界因素强制终止事务。如连接超时或连接断 开，约占所有事务的1%。  带有保存点的扁平事务 todo\n链事务 todo\n嵌套事务 todo\n分布式事务 todo\n事务的日志 事务的四种特性到底是基于什么机制实现呢？\n 事务的隔离性由 锁机制 实现。 而事务的原子性、一致性和持久性由事务的 redo 日志和undo 日志来保证。  REDO LOG 称为 重做日志 ，提供再写入操作，恢复提交事务修改的页操作，用来保证事务的持 久性。 UNDO LOG 称为 回滚日志 ，回滚行记录到某个特定版本，用来保证事务的原子性、一致性。    UNDO 不是 REDO 的逆过程，UNDO 和 REDO都可以视为一种恢复操作，都是存储引擎层生成的日志。\nredo log:记录的是\u0026quot;物理级别\u0026quot;上的页修改操作，比如页号xxx、偏移量yyy 写入了\u0026rsquo;zzz\u0026rsquo;数据。主要为了保证数据的可靠性；\nundo log:记录的是逻辑操作日志，比如对某一行数据进行了INSERT语句 操作，那么undo logi就记录一条与之相反的DELETE操作。主要用于事务的回滚 ( undo log记录的是每个修改操作的逆操作)和一致性非锁定读(undo log回滚行记录到某种特定的版本-MVCC,即多版本并发控制 ) 。\nREDO LOG 概念 视频 p169 ———— 尚硅谷\nINNODB存储引擎是以页为单位来管理存储空间的。在真正访问页面之前，需要把在磁盘上的页缓存到内存中的 Buffer Poo1之后才可以访问。所有的变更都必须先更新缓冲池中的数据，然后缓冲池中的脏页会以一定的频率被刷入磁盘(checkPoint机制)，通过缓冲池来优化CPU和磁盘之间的鸿沟，这样就可以保证整体的性能不 会下降太快。\n一方面，缓冲池可以帮助我们消除CPU和磁盘之间的鸿沟，checkpoint机制可以保证数据的最终落盘，然而由于checkpoint 并不是每次变更的时候就触发 的，而是master线程隔一段时间去处理的。所以最坏的情况就是事务提交后，刚写完缓冲池，数据库宕机了，那么这段数据就是丢失的，无法恢复。\n另一方面，事务包含 持久性 的特性，就是说对于一个已经提交的事务，在事务提交后即使系统发生了崩溃，这个事务对数据库中所做的更改也不能丢失。\n那么如何保证这个持久性呢？\n  一个简单的做法 ：在事务提交完成之前把该事务所修改的所有页面都刷新到磁盘，但是这个简单粗暴的做法有些问题\n 修改量与刷新磁盘工作量严重不成比例 有时候我们仅仅修改了某个页面中的一个字节，但是我们知道在IoDB中是以页为单位来进行磁盘1o的，也 就是说我们在该事务提交时不得不将一个完整的页面从内存中刷新到磁盘，我们又知道一个页面默认是16KB 大小，只修改一个字节就要刷新16KB的数据到磁盘上显然是太小题大做了。 随机1O刷新较慢 一个事务可能包含很多语句，即使是一条语句也可能修改许多页面，假如该事务修改的这些页面可能并不相 邻，这就意味着在将某个事务修改的Buffer Pool中的页面刷新到磁盘时，需要进行很多的随机I0,随机I0比 顺序I0要慢，尤其对于传统的机械硬盘来说。    另一个解决的思路 ：我们只需要把 修改 了哪些东西 记录一下 就好。比如，某个事务将系统表空间中 第10号 页面中偏移量为 100 处的那个字节的值 1 改成 2 。我们只需要记录一下：将第0号表空间的10号页面的偏移量为100处的值更新为 2 。\n  InnoDB引擎的事务采用了WAL技术（Write-Ahead Logging),这种技术的思想就是先写日志，再写磁盘，只 有日志写入成功，才算事务提交成功，这里的日志就是redo log。当发生宕机且数据未刷到磁盘的时候，可以通 过redo log来恢复，保证AclD中的D,这就是redo log的作用。\n  REDO日志的好处特点   好处\nredo日志降低了刷盘频率redo日志占用的空间非常小\n  特点\n  redo日志是顺序写入磁盘的事务执行过程中，redo log不断记录\n 2022-4-2\nredo的组成 小记   我们进行内联查询前，最好能限制连的表大小的条件都先用上了，同时尽量让条件查询和分组执行的表尽量小====》CSDN博客_mysql联表查询优化① \n  uuid采用char(32)或char(36)存储的话，需要占用32或36个字节。为节省存储空间，改为binary(16)，占用16字节。对于500W行的表，可节省7.4G的空间。\nmybatis中没有默认的type handler来完成uuid类型\u0026lt;-\u0026gt;binary类型的相互转换，需要自定义一个type handler。下面就详细地介绍如何实现。\n  DATEDIFF(w1.recordDate,w2.recordDate)=1 日期函数，计算二者差值\n ","id":2,"section":"posts","summary":"","tags":["mysql"],"title":"MySql高级","uri":"https://gb.ytte.top/2022/03/24/mysql%E9%AB%98%E7%BA%A7/","year":"2022"},{"content":"消息队列 MQ 的相关概念 RabbitMQ 概念 RabbitMQ是一个消息中间件：它接受并转发消息。可以把它当做一个快递站点，当你要发送一个包裹时，你把你的包裹放到快递站，快递员最终会把你的快递送到收件人那里，按照这种逻辑RabbitMQ是一个快递站，一个快递员帮你传递快件。RabbitMQ与快递站的主要区别在于，它不处理快件而是接收，存储和转发消息数据。\n四大核心   生产者：\n产生数据发送消息的程序是生产者\n  交换机：\n交换机是 RabbitMQ 非常重要的一个部件，一方面它接收来自生产者的消息，另一方面它将消息推送到队列中。交换机必须确切知道如何处理它接收到的消息，是将这些消息推送到特定队列还是推送到多个队列，亦或者是把消息丢弃，这个得有交换机类型决定\n  队列：\n队列是 RabbitMQ 内部使用的一种数据结构，尽管消息流经 RabbitMQ 和应用程序，但它们只能存储在队列中。队列仅受主机的内存和磁盘限制的约束，本质上是一个大的消息缓冲区。许多生产者可以将消息发送到一个队列，许多消费者可以尝试从一个队列接收数据。这就是我们使用队列的方式\n  消费者：\n消费与接收具有相似的含义。消费者大多时候是一个等待接收消息的程序。请注意生产者，消费者和消息中间件很多时候并不在同一机器上。同一个应用程序既可以是生产者又是可以是消费者。\n  核心模式  Hello World——简单模式 [Work Queue——工作模式](#工作队列 work queue) Publish/Subscribe——发布/订阅模式 Routing——路由模式 Topics——主题模式 Publisher Confirms——发布确认模式  关键词  Broker：接收和分发消息的应用，RabbitMQ Server 就是 Message Broker Virtual host：出于多租户和安全因素设计的，把 AMQP 的基本组件划分到一个虚拟的分组中，类似于网络中的 namespace 概念。当多个不同的用户使用同一个 RabbitMQ server 提供的服务时，可以划分出多个 vhost，每个用户在自己的 vhost 创建exchange／queue 等 Connection：publisher／consumer 和 broker 之间的 TCP 连接 Channel：如果每一次访问 RabbitMQ 都建立一个 Connection，在消息量大的时候建立 TCPConnection 的开销将是巨大的，效率也较低。Channel 是在 connection 内部建立的逻辑连接，如果应用程序支持多线程，通常每个 thread 创建单独的 channel 进行通讯，AMQP method 包含了 channel id 帮助客户端和 message broker 识别 channel，所以 channel 之间是完全隔离的。Channel 作为轻量级的Connection 极大减少了操作系统建立 TCP connection 的开销 Exchange：message 到达 broker 的第一站，根据分发规则，匹配查询表中的 routing key，分发消息到 queue 中去。常用的类型有：==direct== (point-to-point), ==topic== (publish-subscribe) and==fanout==(multicast) Queue：消息最终被送到这里等待 consumer 取走 Binding：exchange 和 queue 之间的虚拟连接，binding 中可以包含 routing key，Binding 信息被保存到 exchange 中的查询表中，用于 message 的分发依据  安装 非docker方式   官方网址\n  文件上传\n上传到/usr/local/software 目录下(如果没有 software 需要自己创建)\n链接：https://pan.baidu.com/s/1UTJhAesjTjQl3rl-v9MZBw 提取码：ytte \u0026ndash;来自百度网盘超级的分享\n  安装文件(分别按照以下顺序安装)\nrpm -ivh erlang-21.3-1.el7.x86_64.rpm #rabbitmq的前置erlang的环境 yum install socat -y #官网要求安装 rpm -ivh rabbitmq-server-3.8.8-1.el7.noarch.rpm #rabbitmq-server   常用命令(按照以下顺序执行)\n         chkconfig rabbitmq-server on 添加开机启动 RabbitMQ 服务   /sbin/service rabbitmq-server start 启动服务   /sbin/service rabbitmq-server status 查看服务状态   /sbin/service rabbitmq-server stop 停止服务(选择执行)   rabbitmq-plugins enable rabbitmq_management 开启 web 管理插件    开启web管理插件的时候需要停止服务，开启web管理插件后需要手动再次启动rabbitmq服务。\n  linux防火墙开启端口\nfirewall-cmd --zone=public --add-port=5672/tcp --permanent #开启5672端口 程序代码执行所需端口 firewall-cmd --zone=public --add-port=15672/tcp --permanent\t#开启15672端口 web插件所需端口  firewall-cmd --reload\t#防火墙重启  firewall-cmd --zone=public --list-ports\t#查看开放的端口  systemctl status firewalld\t#查看防火墙状态   云服务器开启15672和5672端口\n用默认账号密码(guest)访问地址 http://ip:15672/出现权限问题\n  出现权限问题是因为guest设置了超级管理员角色，但是没有被赋予权限。\n添加一个新的用户：\n   序号       1 rabbitmqctl add_user admin 123 创建账号   2 rabbitmqctl set_user_tags admin administrator 设置用户角色   3 rabbitmqctl set_permissions -p \u0026quot;/\u0026quot; admin \u0026quot;.*\u0026quot; \u0026quot;.*\u0026quot; \u0026quot;.*\u0026quot; 设置用户权限   3.1 set_permissions [-p ]  用户 user_admin 具有/vhost1 这个 virtual host 中所有资源的配置、写、读权限   4  rabbitmqctl list_users 显示当前用户和角色      再次利用 admin 用户登录\n  重置命令\n 关闭应用的命令为：rabbitmqctl stop_app 清除的命令为：rabbitmqctl reset 重新启动命令为：rabbitmqctl start_app    docker安装   pull\ndocker pull rabbitmq   run\n15672端口\ndocker run -d --name myrabbitmq -p 15672:15672 -p 5672:5672 rabbitmq   增加超级管理员用户，开启web插件\ndocker ps docker exec -it 容器ID /bin/bash rabbitmqctl add_user admin 123\t#创建账号 rabbitmqctl set_user_tags admin administrator #设置用户角色 rabbitmqctl set_permissions -p \u0026#34;/\u0026#34; admin \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; #用户 user_admin 具有/vhost1 这个 virtual host 中所有资源的配置、写、读权限 rabbitmqctl list_users #显示当前用户和角色  rabbitmq-plugins enable rabbitmq_management #开启web插件  ctrl+p+q #退出当前容器   重启容器\ndocker restart 容器ID   linux防火墙开启端口\nfirewall-cmd --zone=public --add-port=5672/tcp --permanent #开启5672端口 firewall-cmd --zone=public --add-port=15672/tcp --permanent\t#开启15672端口  firewall-cmd --reload\t#防火墙重启  firewall-cmd --zone=public --list-ports\t#查看开放的端口  systemctl status firewalld\t#查看防火墙状态   云服务器开启15672和5672端口\n  登陆网页查看\n  helloworld 用 Java 编写两个程序。发送单个消息的生产者和接收消息并打印出来的消费者。\n在下图中，“ P”是我们的生产者，“ C”是我们的消费者。中间的框是一个队列-RabbitMQ 代表使用者保留的消息缓冲区\n  pom依赖：\n \u0026lt;!--指定 jdk 编译版本--\u0026gt;  \u0026lt;build\u0026gt;  \u0026lt;plugins\u0026gt;  \u0026lt;plugin\u0026gt;  \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt;  \u0026lt;configuration\u0026gt;  \u0026lt;source\u0026gt;8\u0026lt;/source\u0026gt;  \u0026lt;target\u0026gt;8\u0026lt;/target\u0026gt;  \u0026lt;/configuration\u0026gt;  \u0026lt;/plugin\u0026gt;  \u0026lt;/plugins\u0026gt;  \u0026lt;/build\u0026gt;   \u0026lt;dependencies\u0026gt;  \u0026lt;!--rabbitmq 依赖客户端--\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;com.rabbitmq\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;amqp-client\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;5.8.0\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;!--操作文件流的一个依赖--\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;commons-io\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;commons-io\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;2.6\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;/dependencies\u0026gt;   消息的生产者\npublic class Producer {   //队列名称常量  private final static String QUEUE_NAME = \u0026#34;hello\u0026#34;;   public static void main(String[] args) {  //创建一个连接工厂  ConnectionFactory connectionFactory = new ConnectionFactory();  connectionFactory.setHost(\u0026#34;101.35.136.150\u0026#34;);  connectionFactory.setUsername(\u0026#34;ytte\u0026#34;);  connectionFactory.setPassword(\u0026#34;ytte\u0026#34;);  //channel 实现了自动 close 接口 自动关闭 不需要显示关闭  try {  Connection connection = connectionFactory.newConnection();  Channel channel = connection.createChannel();  /** * 生成一个队列 * 1.队列名称 * 2.队列里面的消息是否持久化 默认消息存储在内存中 * 3.该队列是否只供一个消费者进行消费 是否进行共享 false 可以多个消费者消费 * （如果设置成了true）只有队列能接收到消息，消费者也接受不到消息 * 4.是否自动删除 最后一个消费者端开连接以后 该队列是否自动删除 true 自动删除 * 5.其他参数 */\t channel.queueDeclare(QUEUE_NAME, false, false, false, null);  String message = \u0026#34;hello world\u0026#34;;  /** * 发送一个消息 * 1.发送到那个交换机 * 2.路由的 key 是哪个 * 3.其他的参数信息 * 4.发送消息的消息体 */  channel.basicPublish(\u0026#34;\u0026#34;,QUEUE_NAME,null,message.getBytes());  System.out.println(\u0026#34;消息发送完毕\u0026#34;);   } catch (IOException | TimeoutException e) {  e.printStackTrace();  }   } }   消息的消费者\npublic class Consumer {   private final static String QUEUE_NAME = \u0026#34;hello\u0026#34;;    public static void main(String[] args) {  ConnectionFactory connectionFactory = new ConnectionFactory();  connectionFactory.setHost(\u0026#34;101.35.136.150\u0026#34;);  connectionFactory.setUsername(\u0026#34;ytte\u0026#34;);  connectionFactory.setPassword(\u0026#34;ytte\u0026#34;);  System.out.println(\u0026#34;等待接收消息。。。。。。。。\u0026#34;);   try {  Connection connection = connectionFactory.newConnection();  Channel channel = connection.createChannel();   //推送的消息如何进行消费的接口回调  DeliverCallback deliverCallback = (consumerTag, message) -\u0026gt; {  System.out.println(new String(message.getBody()));  };   //取消消费的一个回调接口 如在消费的时候队列被删除掉了  CancelCallback cancelCallback = consumerTag -\u0026gt; {  System.out.println(\u0026#34;消息消费被中断\u0026#34;);  };   /** * 消费者消费消息 * 1.消费哪个队列 * 2.消费成功之后是否要自动应答 true 代表自动应答 false 手动应答 * 3.消费者未成功消费的回调 */  channel.basicConsume(QUEUE_NAME,true,deliverCallback,cancelCallback);    } catch (IOException | TimeoutException e) {  e.printStackTrace();  }  } }   工作队列WorkQueue 工作队列(又称任务队列)的主要思想是避免立即执行资源密集型任务，而不得不等待它完成。我们把任务发送到队列。==当有多个工作线程时，这些工作线程将一起处理这些任务。每个消息任务只能被完整的执行一次，如果一个消费者工作线程断开连接，他正在处理的消息任务会返回队列中，并由其他消费者工作线程来执行，所以说是只能被完整的执行一次。==\n轮循分发消息 即，如果存在两个消费者工作线程，消息会轮流分发给二者，你一个我一个。\n抽取工具类 重复代码太多抽取工具类\npublic class RabbitMqUtils {  //得到一个连接的 channel  public static Channel getChannel() throws Exception {  //创建一个连接工厂  ConnectionFactory factory = new ConnectionFactory();  factory.setHost(\u0026#34;101.35.136.150\u0026#34;);  factory.setUsername(\u0026#34;ytte\u0026#34;);  factory.setPassword(\u0026#34;ytte\u0026#34;);  Connection connection = factory.newConnection();   return connection.createChannel();  } } 消费者 不创建多个进行，使用idea自带的单一类可创建多实例。\n这样就可以多次启动WorkQueue1，且可更改其中的代码在启动（将C1 消费者启动改为C2 消费者启动）\npublic class WorkQueue1 {  private static final String QUEUE_NAME=\u0026#34;hello\u0026#34;;  public static void main(String[] args) throws Exception {   Channel channel = RabbitMqUtils.getChannel();  DeliverCallback deliverCallback=(consumerTag, delivery)-\u0026gt;{  String receivedMessage = new String(delivery.getBody());  System.out.println(\u0026#34;接收到消息:\u0026#34;+receivedMessage);  };   CancelCallback cancelCallback=(consumerTag)-\u0026gt;{  System.out.println(consumerTag+\u0026#34;消费者取消消费接口回调逻辑\u0026#34;);  };   System.out.println(\u0026#34;C1 消费者启动等待消费.................. \u0026#34;);  channel.basicConsume(QUEUE_NAME,true,deliverCallback,cancelCallback);  } } 生产者 public class Task01 {  private static final String QUEUE_NAME = \u0026#34;hello\u0026#34;;   public static void main(String[] args) throws Exception {  try (Channel channel = RabbitMqUtils.getChannel();) {   channel.queueDeclare(QUEUE_NAME, false, false, false, null);   //从控制台当中接受信息  Scanner scanner = new Scanner(System.in);  while (scanner.hasNext()) {  String message = scanner.next();  channel.basicPublish(\u0026#34;\u0026#34;, QUEUE_NAME, null, message.getBytes());  System.out.println(\u0026#34;发送消息完成:\u0026#34; + message);  }  }  } } 结果 消息应答 RabbitMQ 一旦向消费者传递了一条消息，便立即将该消息标记为删除。在这种情况下，突然有个消费者挂掉了，我们将丢失正在处理的消息。以及后续发送给该消费这的消息，因为它无法接收到。\n为了保证消息在发送过程中不丢失，rabbitmq 引入消息应答机制，消息应答就是：消费者在接收到消息并且处理该消息之后，告诉 rabbitmq 它已经处理了，rabbitmq 可以把该消息删除了。\n 自动应答 手动应答  自动应答 消息发送后立即被认为已经传送成功，这种模式需要在高吞吐量和数据传输安全性方面做权衡，因为如果消息在接收到之前，消费者出现连接或者 channel 关闭，那么消息就丢失了；虽然这种模式消费者可以传递过载的消息，但是这样有可能使得消费者由于接收太多还来不及处理的消息，造成消息的积压，内存耗尽，最终这些消费者线程被操作系统杀死。所以自动应答仅适用在消费者可以高效并以某种速率能够处理这些消息的情况下使用。\n一般不采用自动应答，采用手动应答。\n手动应答 常用方法：\n  Channel.basicAck(用于肯定确认)\nRabbitMQ 已知道该消息并且成功的处理消息，可以将其丢弃了\n  Channel.basicNack(用于否定确认)\n  Channel.basicReject(用于否定确认)\n与 Channel.basicNack 相比少一个参数，不处理该消息了直接拒绝，可以将其丢弃了\n  手动应答的好处是可以批量应答并且减少网络拥堵\n批量应答即，消息在消费者的对应的信道中，还未被接受也会被消费者回复收到应答给RabbitMq。\n消息自动重新入队 如果消费者由于某些原因失去连接(其通道已关闭，连接已关闭或 TCP 连接丢失)，导致消息未发送 ACK 确认，RabbitMQ 将了解到消息未完全处理，并将对其重新排队。如果此时其他消费者可以处理，它将很快将其重新分发给另一个消费者。这样，即使某个消费者偶尔死亡，也可以确保不会丢失任何消息。\n消息手动应答代码 生产者：\npublic class Task02 {   private static final String TASK_QUEUE_NAME = \u0026#34;ack_queue\u0026#34;;   public static void main(String[] argv) throws Exception {  try (Channel channel = RabbitMqUtils.getChannel()) {  channel.queueDeclare(TASK_QUEUE_NAME, false, false, false, null);  Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请输入信息\u0026#34;);  while (sc.hasNext()) {  String message = sc.nextLine();  channel.basicPublish(\u0026#34;\u0026#34;, TASK_QUEUE_NAME, null, message.getBytes(\u0026#34;UTF-8\u0026#34;));  System.out.println(\u0026#34;生产者发出消息\u0026#34; + message);  }  }  } } 消费者1：\npublic class Work01 {  private static final String ACK_QUEUE_NAME = \u0026#34;ack_queue\u0026#34;;   public static void main(String[] args) throws Exception {  Channel channel = RabbitMqUtils.getChannel();  System.out.println(\u0026#34;C1 等待接收消息处理时间较短\u0026#34;);   //消息消费的时候如何处理消息  DeliverCallback deliverCallback = (consumerTag, delivery)-\u0026gt; {  String message = new String(delivery.getBody()); \t// 睡眠1s  SleepUtils.sleep(1);  System.out.println(\u0026#34;接收到消息:\u0026#34; + message);  /** * 1.消息标记 tag * 2.是否批量应答未应答消息 */  channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false);  };   //采用手动应答  channel.basicConsume(ACK_QUEUE_NAME, false, deliverCallback, (consumerTag) -\u0026gt; {  System.out.println(consumerTag + \u0026#34;消费者取消消费接口回调逻辑\u0026#34;);  });   } } 消费者2：\npublic class Work02 {  private static final String ACK_QUEUE_NAME = \u0026#34;ack_queue\u0026#34;;   public static void main(String[] args) throws Exception {  Channel channel = RabbitMqUtils.getChannel();  System.out.println(\u0026#34;C2 等待接收消息处理时间较短\u0026#34;);   //消息消费的时候如何处理消息  DeliverCallback deliverCallback = (consumerTag, delivery)-\u0026gt; {  String message = new String(delivery.getBody()); \t//睡眠20s表示正在处理还多的代码  SleepUtils.sleep(20);  System.out.println(\u0026#34;接收到消息:\u0026#34; + message);  /** * 1.消息标记 tag * 2.是否批量应答未应答消息 */  channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false);  };   //采用手动应答  channel.basicConsume(ACK_QUEUE_NAME, false, deliverCallback, (consumerTag) -\u0026gt; {  System.out.println(consumerTag + \u0026#34;消费者取消消费接口回调逻辑\u0026#34;);  });   } } SleepUtils：\npublic class SleepUtils {  public static void sleep(int second) {  try {  Thread.sleep(1000 * second);  } catch (InterruptedException _ignored) {  Thread.currentThread().interrupt();  }  } } 效果 生产者发送1~9，消费者工作线程轮流接受消息，1和2中平均接收到消息，但是由于1中睡眠1s，2中睡眠20s，所以可以看出2对应的信道里还屯留了几条消息（不知道是否可以这样说），而1中马上就处理完了。这时候我们关闭2进程，这就导致没有处理消息，或者没有处理完成，也就没有发送应答，Rabbit会检查到没有接收到应答消息，将未处理的消息交个能处理它们的1中，1继续处理消息。\n 2022-3-30\nRabbitMQ 持久化 保障当 RabbitMQ 服务停掉以后消息生产者发送过来的消息不丢失。默认情况下 RabbitMQ 退出或由于某种原因崩溃时，它忽视队列和消息，除非告知它不要这样做。我们需要==将队列和消息都标记为持久化==。\n队列如何实现持久化 ==队列实现持久化 需要在声明队列的时候把 durable 参数设置为持久化==\n注意：之前声明的不是持久化的队列，需要把原先队列先删除，或者重新创建一个持久化的队列，不然就会出现错误\n控制台中持久化与非持久化队列的 UI 显示区：\n现在即使重启 rabbitmq 队列也依然存在。\n消息实现持久化 ==消息持久化需要在消息生产者修改代码，MessageProperties.PERSISTENT_TEXT_PLAIN添加这个属性。==\nchannel.basicPublish(\u0026#34;\u0026#34;, TASK_QUEUE_NAME, MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes(\u0026#34;UTF-8\u0026#34;)); 将消息标记为持久化并不能完全保证不会丢失消息。尽管它告诉 RabbitMQ 将消息保存到磁盘，但是这里依然存在当消息刚准备存储在磁盘的时候 但是还没有存储完，消息还在缓存的一个间隔点。此时并没有真正写入磁盘。后面的发布确认章节，会细说如何让生产者收到确认的消息。（生产者收到确认消息，即消息已经 到达队列并且持久化了）\n不公平分发 之前的轮流分发在某种场景下不是好的方法，比方说有两个消费者在处理任务，二者处理速度不同，任务是平分的，就会造成一方消费者空闲 一方忙活。不能够最大化利用现有资源。\n为了避免这种情况，我们可以设置参数\nchannel.basicQos(1) 在web页面中可以看到Prefetch count 为1，即上面设置的参数为1\n如果我还没有处理完或者我还没有应答你，你先别分配给我，我目前只能处理一个任务，然后 rabbitmq 就会把该任务分配给没有那么忙的那个空闲消费者，当然如果所有的消费者都没有完成手上任务，队列还在不停的添加新任务，队列有可能就会遇到队列被撑满的情况，这个时候就只能添加新的 worker 或者改变其他存储任务的策略。\n预取值 ==就相当于channel中有个缓冲区，缓冲区的大小（所能暂存消息的数量）可以通过使用 basic.Qos “预取计数”值来设置。该值定义通道上允许的未确认消息的最大数量。一旦数量达到配置的数量，RabbitMQ 将停止在通道上传递更多消息。==\n例如，假设在通道上有未确认的消息 5、6、7，8，预取值设置为 4，此时RabbitMQ 将不会在该通道上再传递任何 消息。若 排在前面的消息 刚刚被确认 ACK，RabbitMQ 才会再发送消息。消息应答和 QoS 预取值对用户吞吐量有重大影响。\n通常，增加预取值，来提高向消费者传递消息的速度。虽然自动应答传输消息速率是最佳的，但是，在这种情况下已传递但尚未处理的消息的数量也会增加，不同的负载该值取值也不同 100 到 300 范围内的值通常可提供最佳的吞吐量，并且不会给消费者带来太大的风险。预取值为 1 是最保守的。\n发布确认 原理 生产者将信道设置成 confirm 模式，所有在该信道上面发布的消息都将会被指派一个唯一的 ID(从 1 开始)，消息被投递到所有匹配的队列之后，mq就会发送一个确认信息给生产者(包含消息的唯一 ID)，告诉生产者消息已经正确到达目的队列了。\n如果消息和队列是可持久化的，那么确认消息会在将消息写入磁盘之后发出，mq回传给生产者的确认消息中的delivery-tag 属性，包含了确认消息的序列号，此外 broker 也可以设置basic.ack 的multiple 属性，表示到这个序列号之前的所有消息都已经得到了处理。\nconfirm 模式最大的好处在于可以是异步的，发布一条消息，生产者应用程序就可以在等信道返回确认的同时继续发送下一条消息，当消息最终得到确认之后，生产者应用便可以通过回调方法来处理该确认消息，如果 RabbitMQ 因为自身内部错误导致消息丢失，就会发送一条 nack 消息，生产者应用程序同样可以在回调方法中处理该 nack 消息。\n 2022-3-31\n开启发布确认的方法 开启需要调用方法 confirmSelect，每当你要想使用发布确认，都需要在 channel 上调用该方法\n单个确认发布 这是一种简单的确认方式，==同步确认发布的方式==，发布一个消息之后只有它被确认发布，后续的消息才能继续发布，waitForConfirmsOrDie(long)这个方法只有在消息被确认的时候才返回(返回布尔值)，如果在指定时间范围内这个消息没有被确认那么它将抛出异常。\n缺点：==发布速度特别的慢==，因为如果==没有收到确认发布的消息就会阻塞所有后续消息==的发布，这种方式最多提供每秒不超过数百条发布消息的吞吐量。适用简单和小应用程序。\npublic class e111 {  private static final int MESSAGE_COUNT = 1000;   public static void main(String[] args) throws Exception {  publishMessageIndividually();  }   public static void publishMessageIndividually() throws Exception {  try (Channel channel = RabbitMqUtils.getChannel()) {  //产生随机的队列名  String queueName = UUID.randomUUID().toString();  //声明队列  channel.queueDeclare(queueName, false, false, false, null);  //开启发布确认  channel.confirmSelect();  long begin = System.currentTimeMillis();  //循环1000 发送消息，收到确认信息再发送消息  for (int i = 0; i \u0026lt; MESSAGE_COUNT; i++) {  String message = i + \u0026#34;\u0026#34;;  channel.basicPublish(\u0026#34;\u0026#34;, queueName, null, message.getBytes());  //等待接受确认信息 服务端返回 false 或超时时间内未返回，生产者可以消息重发  boolean flag = channel.waitForConfirms();  if (flag) {  System.out.println(\u0026#34;消息发送成功\u0026#34;);  }  }  long end = System.currentTimeMillis();  System.out.println(\u0026#34;发布\u0026#34; + MESSAGE_COUNT + \u0026#34;个单独确认消息,耗时\u0026#34; + (end - begin) + \u0026#34;ms\u0026#34;);  }  } } //结果 发布1000个单独确认消息,耗时15837ms 批量确认发布 与单个等待确认消息相比，==先发布一批消息然后一起确认==可以极大地提高吞吐量，==缺点：当发生故障导致发布出现问题时，不知道是哪个消息出现问题==，我们必须将整个批处理保存在内存中，以记录重要的信息而后重新发布消息。当然这种方案仍然是==同步的==，也一样阻塞消息的发布。\npublic static void BatchPublishMessage() throws Exception {  try (Channel channel = RabbitMqUtils.getChannel()) {  String queueName = UUID.randomUUID().toString();  channel.queueDeclare(queueName, false, false, false, null);  channel.confirmSelect();  long start = System.currentTimeMillis();   for (int i = 0; i \u0026lt; MESSAGE_COUNT; i++) {  String message = i + \u0026#34;\u0026#34;;  channel.basicPublish(\u0026#34;\u0026#34;, queueName, null, message.getBytes());   if (MESSAGE_COUNT % (i+1) == 0) {  boolean b = channel.waitForConfirms();  if (b) {  System.out.println(\u0026#34;消息发送成功\u0026#34;);  }  }  }  long end = System.currentTimeMillis();  System.out.println(\u0026#34;发布\u0026#34; + MESSAGE_COUNT + \u0026#34;个单独确认消息,耗时\u0026#34; + (end - start) + \u0026#34;ms\u0026#34;);  } } //结果 发布1000个单独确认消息,耗时344ms 异步确认发布 public static void AsynchronousPublishMessage() throws Exception {  try (Channel channel = RabbitMqUtils.getChannel()) {  String queueName = UUID.randomUUID().toString();  //开启发布确认  channel.confirmSelect();  /** * 线程安全有序的一个哈希表，适用于高并发的情况 * 1.将序号与消息进行关联 * 2.批量删除条目 只要给到序列号 * 3.支持并发访问 */  ConcurrentSkipListMap\u0026lt;Long, String\u0026gt; outstandingConfirms = new ConcurrentSkipListMap\u0026lt;\u0026gt;();  /** * 确认收到消息的一个回调 * 1.消息序列号 * 2.multiple * true 未能收到当前序号的确认消息 * false 确认收到当前序号消息 */  ConfirmCallback ackCallback = (sequenceNumber, multiple) -\u0026gt; {  //只清除当前序列号的消息  if (!multiple) {  String message = outstandingConfirms.get(sequenceNumber);  System.out.println(\u0026#34;发布的消息\u0026#34; + message + \u0026#34;被确认\u0026#34;);  outstandingConfirms.remove(sequenceNumber);  }  };  ConfirmCallback nackCallback = (sequenceNumber, multiple) -\u0026gt; {  if (multiple) {  String message = outstandingConfirms.get(sequenceNumber);  System.out.println(\u0026#34;发布的消息\u0026#34;+message+\u0026#34;未被确认，序列号\u0026#34;+sequenceNumber);  }  };   /** * 添加一个异步确认的监听器 * 1.确认收到消息的回调 * 2.未收到消息的回调 */  channel.addConfirmListener(ackCallback, nackCallback);   long begin = System.currentTimeMillis();   for (int i = 0; i \u0026lt; MESSAGE_COUNT; i++) {  String message = \u0026#34;消息\u0026#34; + i;  /** * channel.getNextPublishSeqNo()获取下一个消息的序列号 * 通过序列号与消息体进行一个关联 * 接收到的确认的，未确认的消息都收集到map中 */  outstandingConfirms.put(channel.getNextPublishSeqNo(), message);  channel.basicPublish(\u0026#34;\u0026#34;, queueName, null, message.getBytes());  System.out.println(\u0026#34;消息发送成功\u0026#34;);  }   long end = System.currentTimeMillis();  System.out.println(\u0026#34;发布\u0026#34; + MESSAGE_COUNT + \u0026#34;个异步确认消息,耗时\u0026#34; + (end - begin) + \u0026#34;ms\u0026#34;);  SleepUtils.sleep(20);  } }  //结果 发布1000个异步确认消息,耗时62ms 如何处理异步未确认消息 把未确认的消息放到一个基于内存的能被发布线程访问的队列，比如说用ConcurrentLinkedQueue 这个队列在 confirm callbacks 与发布线程之间进行消息的传递。\n对比   发布1000个单独确认消息,耗时15837ms\n  发布1000个单独确认消息,耗时344ms\n  发布1000个异步确认消息,耗时62ms\n  交换机 在之前，将做一些完全不同的信息传达给多个消费者的模式称为 ”发布/订阅”。为了说明这种模式，我们将构建一个简单的日志系统。由两个程序组成：第一个程序将发出日志消息，第二个程序是消费者。其中我们会启动两个消费者，其中一个消费者接收到消息后把日志存储在磁盘，另外一个消费者接收到消息后把消息打印在屏幕上，事实上第一个程序发出的日志消息将广播给所有消费者。todo（解释的不清晰）。\n实际上，生产者只能将消息发送到交换机(exchange)，交换机工作的内容是，一方面接收来自生产者的消息，另一方面将它们推入队列。交换机必须确切知道如何处理收到的消息。是应该把这些消息放到特定队列还是说把他们到许多队列中（这就是发布订阅模式（类似广播）），还是说应该丢弃它们。这些都由交换机的类型来决定。\nExchanges 的类型  直接(direct) 主题(topic) 标题(headers) 扇出(fanout)  默认交换机 填写“”即表示使用默认的交换机。消息能路由发送到队列中其实是由routingKey(bindingkey)绑定 key 指定的\n临时队列 之前的章节我们使用的是具有特定名称的队列(还记得 hello 和 ack_queue 吗？)。队列的名称我们来说至关重要-我们需要指定我们的消费者去消费哪个队列的消息。每当我们连接到 Rabbit 时，我们都需要一个全新的空队列，为此我们可以创建一个具有随机名称的队列，或者能让服务器为我们选择一个随机队列名称那就更好了。其次一旦我们断开了消费者的连接，队列将被自动删除。创建临时队列的方式如下:\nString queueName = channel.queueDeclare().getQueue(); 创建出来之后长成这样:\n绑定(bindings) binding 是 exchange 和 queue 之间的桥梁，表示exchange 和队列进行了绑定关系。比如说下面这张图告诉我们的就是 X 与 Q1 和 Q2 进行了绑定\nFanout Fanout 是将接收到的所有消息==广播到它知==道的所有队列中。系统中默认有些 exchange 类型。\nLogs 和临时队列的绑定关系如下图:\n总结 消息队列的使用场景\n分布式事务 重复消费、消息丢失、顺序消费 可用性  搞定！\n人生无常，大肠包小肠！\n继续学习吧！\n","id":3,"section":"posts","summary":"","tags":["Rabbit Mq"],"title":"Rabbit Mq","uri":"https://gb.ytte.top/2022/03/22/rabbit-mq/","year":"2022"},{"content":"基础篇 安装  官方网站\n 基本安装   确定你是CentOS7及以上版本\ncat /etc/redhat-release   卸载旧版本\n  yum安装gcc相关\nyum -y install gcc yum -y install gcc-c++   安装需要的软件包\nyum install -y yum-utils   设置stable镜像仓库\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo   更新yum软件包索引\nyum makecache fast   安装DOCKER CE\nyum -y install docker-ce docker-ce-cli containerd.io   启动docker\nsystemctl start docker   测试\ndocker --version   卸载\nsystemctl stop docker yum remove docker-ce docker-ce-cli containerd.io yum remove docker-ce docker-ce-cli containerd.io yum remove docker-ce docker-ce-cli containerd.io   阿里云镜像加速   https://promotion.aliyun.com/ntms/act/kubernetes.html   注册一个属于自己的阿里云账户(可复用淘宝账号)\n  获得加速器地址连接\n 登陆阿里云开发者平台 点击控制台 选择容器镜像服务 获取加速器地址    粘贴脚本直接运行\nmkdir -p /etc/docker  tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://aa25jngu.mirror.aliyuncs.com\u0026#34;] } EOF   重启服务器\nsystemctl daemon-reload systemctl restart docker    搞定！\n人生无常，大肠包小肠！\n继续学习吧！\n","id":4,"section":"posts","summary":"","tags":["Docker"],"title":"Docker","uri":"https://gb.ytte.top/2022/03/22/docker/","year":"2022"},{"content":"前言 本文讲述hexo和hugo搭建静态个人博客，配置，部署到gitee和github，给github的仓库配置自定义访问域名。首先会最简单的方式实现功能，所以文章分为基本篇和进阶篇。\n基础篇 快速实现基本功能。两种实现方式：\nhexo：js写的，页面加载速度不hugo；但是用的人多，中文好看的主题多。\nhugo：go写的，用的人少，如果不使用本文使用的主题，就需要自己去多研究其他主题怎么更改配置；但是页面加载速度快。\n基本环境：\n  安装Git\n参考2\n  详细的学习网站：\n参考3\n    安装Node.js\n参考4\n  方式一：hexo hexo中文文档 ：文档 | Hexo\n 安装Hexo GitHub创建个人仓库 生成SSH添加到GitHub hexo配置远程仓库GitHub 发布文章 配置文件 主题  安装Hexo   创建一个文件夹blog（命名随意），在这个文件夹下直接右键打开git bash，输入如下代码安装hexo\nnpm install -g hexo-cli   安装完成后 hexo -v查看一下版本\n  初始化hexo\nhexo init myblog 然后\ncd myblog //进入这个myblog文件夹 npm install   install完成后文件夹目录如下\n node_modules: hexo依赖包 public：存放生成的页面（执行相关命令后md文件会被转换成html文件自动放在这里） scaffolds：生成文章的一些模板（之后创建md文章的时候就会默认使用这里的模板） source：用来存放你的文章 （自己写的文章全部放在这里，md文件放在post文件夹里） themes：主题文件 _config.yml: 博客的配置文件    本地启动看看是否安装及配置成功\n hexo g 是用来将md文件生成html页面自动放入public的。 hexo server是用来本地启动的  hexo new hexo g hexo server  启动完成会提示浏览器访问 localhost:4000。 出现一下图片样式，即表示本地启动配置成功。   使用ctrl+c可以把服务关掉    GitHub创建个人仓库 IPHP  在GitHub.com中看到一个New repository，新建仓库 创建一个和你用户名相同的仓库，后面加.github.io，只有这样，部署到GitHub page的时候，才会被识别，才能通过xxxx.github.io访问到博客，其中xxx就是你注册GitHub的用户名。 仓库类型选择【public】，勾选【Initialize this repository with a README】 点击【create repository】  生成SSH添加到GitHub 传送——Git多个ssh - YTTE Site)中的本地同步到一个git仓库 9~11步骤\nhexo配置远程仓库GitHub   打开根目录下的配置文件 _config.yml，翻到最后，修改一下代码。这里使用ssh方式push（如果没有配置ssh密钥，传送\ndeploy:\rtype: git\rrepo: github: git@github.com/YourgithubName/YourgithubName.github.io.git\rbranch: master   根目录下右键git bash 安装deploy-git 部署的命令，将git命令简化。等待安装完成。\nnpm install hexo-deployer-git --save   新建文章：layout一般选择post及文章 ，E.g. (比如) hexo new post 好好学习，然后会自动把 .md文章发送到根目录/source/_post中\nhexo new [layout] \u0026lt;title\u0026gt;   发布文章   使用hexo的命令来clean清理上一次生成的静态页面（也可以不清理），generate生成静态文章，deploy部署\nhexo clean hexo g hexo d   如图表示成功。通过http://yourgithubname.github.io即可访问你自己的博客。\n  配置文件 根目录下的_config.yml，Ctrl + F 查询填写，即可查看必须要更改的地方\n# Hexo Configuration ## Docs: https://hexo.io/docs/configuration.html ## Source: https://github.com/hexojs/hexo/  #Ctrl + F 查询填写，即可查看必须要更改的地方  # Site title: YTTE  # 填写 subtitle: \u0026#39;Personal Blogs\u0026#39; # 填写 description: \u0026#39;\u0026#39; # 填写 keywords: java  # 填写 author: JX  # 填写 language: en timezone: \u0026#39;Asia/Shanghai\u0026#39; # 填写  # URL ## Set your site url here. For example, if you use GitHub Page, set url as \u0026#39;https://username.github.io/project\u0026#39; url: https://gb.ytte.top  # 填写 域名或者https://YourgithubName.github.io permalink: :year/:month/:day/:title/ permalink_defaults: pretty_urls:  trailing_index: true # Set to false to remove trailing \u0026#39;index.html\u0026#39; from permalinks  trailing_html: true # Set to false to remove trailing \u0026#39;.html\u0026#39; from permalinks  # Directory source_dir: source public_dir: public tag_dir: tags archive_dir: archives category_dir: categories code_dir: downloads/code i18n_dir: :lang skip_render:  # Writing new_post_name: :title.md # File name of new posts default_layout: post titlecase: false # Transform title into titlecase external_link:  enable: true # Open external links in new tab  field: site # Apply to the whole site  exclude: \u0026#39;\u0026#39; filename_case: 0 render_drafts: false post_asset_folder: false relative_link: false future: true highlight:  enable: true  line_number: true  auto_detect: false  tab_replace: \u0026#39;\u0026#39;  wrap: true  hljs: false prismjs:  enable: false  preprocess: true  line_number: true  tab_replace: \u0026#39;\u0026#39;  # Home page setting # path: Root path for your blogs index page. (default = \u0026#39;\u0026#39;) # per_page: Posts displayed per page. (0 = disable pagination) # order_by: Posts order. (Order by date descending by default) index_generator:  path: \u0026#39;\u0026#39;  per_page: 10  order_by: -date  # Category \u0026amp; Tag default_category: uncategorized category_map: tag_map:  # Metadata elements ## https://developer.mozilla.org/en-US/docs/Web/HTML/Element/meta meta_generator: true  # Date / Time format ## Hexo uses Moment.js to parse and display date ## You can customize the date format as defined in ## http://momentjs.com/docs/#/displaying/format/ date_format: YYYY-MM-DD time_format: HH:mm:ss ## updated_option supports \u0026#39;mtime\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;empty\u0026#39; updated_option: \u0026#39;mtime\u0026#39;  # Pagination ## Set per_page to 0 to disable pagination per_page: 10 pagination_dir: page  # Include / Exclude file(s) ## include:/exclude: options only apply to the \u0026#39;source/\u0026#39; folder include: exclude: ignore:  # Extensions ## Plugins: https://hexo.io/plugins/ ## Themes: https://hexo.io/themes/ theme: pure  # 填写选择跟目录下theme文件夹里的主题的文件夹 名称   # Deployment ## Docs: https://hexo.io/docs/one-command-deployment deploy:  type: \u0026#39;git\u0026#39; # 填写  repo:  github: git@github.com:YourgithubName/YourgithubName.github.io.git  # 填写  branch: master  # 填写 官方文档配置 | Hexo\n   参数 描述     title 网站标题   subtitle 网站副标题   description 网站描述   author 您的名字   language 网站使用的语言   timezone 网站时区。\u0026lsquo;Asia/Shanghai\u0026rsquo;   url 网址（域名或者https://YourgithubName.github.io）   root 网站根目录   permalink 文章的 永久链接 格式    比如新建一个文章叫test.md，那么这个时候他自动生成的地址就是http://yoursite.com/2022/01/01/test，或者http://YourgithubName.github.io/2022/01/01/test。\n打开 根目录/scaffolds/post.md 在里面可以设置 每次new 的post 的模板。\ntitle: {{ title }} date: {{ date }} tags: {{ tags }} categories: {{categories}} toc: true  #以上代码放在yaml font matter（在typora中右键-插入当中）  \u0026lt;meta name=\u0026#34;referrer\u0026#34; content=\u0026#34;no-referrer\u0026#34;/\u0026gt;  \u0026lt;!--more--\u0026gt; 更换主题 官方给的主题：Themes | Hexo\n这里本文采用pure主题Hexo theme pure(github.com)\n  在根目录下使用git bash 然后git clone 主题的github。例如pure主题的，并在最后加上theme/pure，表示在clone到theme下并重命名为pure。\ngit clone https://github.com/cofess/hexo-theme-pure.git theme/pure 主题目录：（不同主题可能不同）（本人后端，对前端不是很懂）\n _source： languages： layout：主题布局样式 ，调用函数等 screenshot： scripts： source：你的头像之类的公共资源，js，css _config.yml：主题的配置文件    每个主题都有不同的要求，有点需要你把文件夹复制到根目录，有的只需要你在根目录下_config.yml配置文件把theme 改为XXX。\n所以使用主题，需要仔细阅读它github的README.md；一般作者都会介绍如何使用。\n  pure中要求：\n 把本hexo根目录下_config.yml配置文件把theme 改为pure。（切记不是theme/pure/下的config.yml）。 根目录\\themes\\pure\\_source下（是_source 不是source）所有文件夹复制到根目录下的source文件夹。 根目录\\themes\\pure\\_config.yml中可以更改一些自己相关的信息，比如点击头像跳转自己的github。等等。  hexo clean hexo g hexo s # 本地启动 本地启动查看是否设置成功。\n然后推送到github。\nhexo clean hexo g hexo d http://YourgithubName.github.io\n  方式二：hugo 实现过程与hexo很类似\nhugo中文文档 ：Hugo中文文档 (gohugo.org)\nhugo github ：hugo: (github.com)\n 安装hugo GitHub创建个人仓库 生成SSH添加到GitHub 将hexo部署到GitHub发布文章 配置文件 主题  安装hugo 安装地址Releases · gohugoio/hugo (github.com)\n  下载解压带extend的压缩包（很多主题都采用这个扩展版hugo）。\n  解压完成会得到如下几个文件（一下目录以**F:\\hugo\\bin**为例）\n  将F:\\hugo\\bin加到path变量中，注意两点: (1) 是**path**，不是CLASSPATH，要区分下 (2) 路径后面记得加英文分号 ;以上设置好后，就可以在cmd中查看是否安装成功。\n  执行命令：hugo version HugoStatic Site Generator v0.57.2-A849CB2D windows/amd64 BuildDate: 2019-08-17T17:54:13Z　显示该条即表示成功\n  生成站点  hugo new site文件名称 (如blog) 。执行后，在F:\\hugo\\bin目录下就会生成一个 名叫blog的站点文件夹 站点文件夹目录结构：  archetypes　(存放default.md，头文件格式，每次新建文章默认显示的头部信息在此修改) content (存放博客文章，markdown格式文件) data (存放自定义或者导入的模板) layouts (存放网站的数据模板) static (存放图片、css、js等静态资源) themes (存放主题文件，每个主题都是一个独立的文件夹) config.toml (网站配置文件)    创建文章   进入站点根目录blog下，执行命令：\nhugo new post/test.md\n  执行后，会自动在content/post下生成 test.md文件，打开可编辑内容，ps.文件头部的draft要改为false，这样部署后才能看到文章。\n当前网站是没有任何内容的，需要下载个主题。\n下载主题   本文使用pure主题 hugo-theme-pure/README-ZH.md (github.com)\n  hugo主题合集Hugo Themes (gohugo.io)\n  进入站点的themes目录\ngit clone https://github.com/xiaoheiAh/hugo-theme-pure.git themes/pure 此时可回到站点目录下打开config.toml配置指定主题。如theme = \u0026ldquo;pure\u0026rdquo; 没有theme参数就自己写上\n  将themes\\pure\\exampleSite选的content文件夹复制到站点目录下。\n  在archetypes\\default.md中更改\ntitle: \u0026#34;{{ .Name | humanize | title }}\u0026#34; date: \u0026#34;{{ .Date }}\u0026#34; tags:  - \u0026#34;java\u0026#34; categories:  - \u0026#34;java\u0026#34; toc: true # bookComments: false # bookSearchExclude: false  #以上代码放在yaml font matter（在typora中右键-插入当中）  \u0026lt;!--more--\u0026gt;   本地启动 站点根目录下，执行命令：\nhugo server 执行后会显示\n \u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;此处省略上方信息\nWeb Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop\n 可访问：http://localhost:1313/ 查看效果。 Ctrl+C 结束本地运行。\n部署github pages上   登录后，点击右上角，出现下拉菜单，点击 Your repositories 进入页面\n  点击new新建仓库（如果操作过上述hexo版本，可以不新建，毕竟仓库名为yourgithubname.github.io的只能有一个）。\n  进入 Creat a new repository 页面。\n图中的 yourname 要换成自己的github的用户名，即上图中Owner显示的用户名。最后点击Creat repository即完成\n  站点目录**config.toml中baseURL**要换成自己建立的仓库，如baseURL = \u0026ldquo;https://yourname.github.io/\u0026quot;\n  进入站点根目录下，执行：\nhugo   执行后，站点根目录下会生成一个 public 文件夹，该文件下的内容即Hugo生成的整个静态网站。每次更新内容后，将 pubilc 目录里所有文件 push到GitHub即可。\n  首次使用的时候要执行以下命令：\n  注意：与hexo不同的是\n  hexo在站点目录（根目录）下执行代码。\n  hugo生成静态页面的代码hugo在站点目录下执行，git命令在public 目录下执行。\n    cd public   使用ssh连接远程仓库：\n传送——git_ssh和多个ssh - YTTE Site中的本地同步到一个git仓库 9~11步骤\n即\nssh-keygen -t rsa -C “xxx@qq.com” # 将C:\\Users\\Administrator\\ .ssh\\id_rsa.pub 里面的内容复制到github右上角头像里下方设置里ssh里面。 ssh -T git@github.com #测试设置ssh是否成功   git：(在public文件夹中)\ngit init git pull git@github.com:yourgithubname/yourgithubname.github.io.git git add . git commit -m \u0026#34;init\u0026#34; git remote add origin git@github.com:yourgithubname/yourgithubname.github.io.git git push -u origin master   以后每次站点目录下执行 hugo 命令后，再到public下执行推送命令：\ngit add . git commit -m \u0026#34;xxx\u0026#34; git push -u origin master (此处较易出错，error了就百度吧，比较经常发送的是连接失败，多试几次，还是不行，问题可能千奇百怪) 之后就可以到GitHub上看提交到分支的内容，也可访问 http://YourgithubName.github.io看页面了。\n    配置文件 #Ctrl + F 查询 更改 ，即可查看必须要更改的地方\n#Ctrl + F 查询 更改 ，即可查看必须要更改的地方  baseURL: https://YourgithubName.github.io  #更改 theme: pure #更改 title: YTTE Site\t#更改 copyright: CC BY 4.0 CN defaultContentLanguage: en  # en/zh/... footnoteReturnLinkContents: ↩ hasCJKLanguage: true paginate: 8 enableEmoji: true PygmentsCodeFences: true googleAnalytics: \u0026#34;\u0026#34; # UA-XXXXXXXX-X  permalinks:  posts: /:year/:month/:day/:title/  taxonomies:  category : categories  tag : tags  series : series outputFormats: # use for search. recommend not to modify   SearchIndex:  mediaType: \u0026#34;application/json\u0026#34;  baseName: \u0026#34;searchindex\u0026#34;  isPlainText: true  notAlternative: true  outputs:  home: [\u0026#34;HTML\u0026#34;,\u0026#34;RSS\u0026#34;,\u0026#34;SearchIndex\u0026#34;] # recommend not to modify # sitemap sitemap:  changefreq: monthly  filename: sitemap.xml  priority: 0.5  menu:  main:  - identifier: home  name: Home  title: Home  url: /  weight: 1   - identifier: archives  name: Archives  title: Archives  url: /posts/  weight: 2   - identifier: categories  name: Categories  title: Categories  url: /categories/  weight: 3   - identifier: tags  name: Tags  title: Tags  url: /tags/  weight: 4   - identifier: about  name: About  title: About  url: /about/  weight: 5   params:  since: 2022  dateFormatToUse: \u0026#34;2006-01-02\u0026#34;  enablePostCopyright: true  copyright_link: http://creativecommons.org/licenses/by/4.0/deed.zh  # the directory under content folder that you want to render  mainSections: [\u0026#34;posts\u0026#34;]  # Enable/Disable menu icons  # Icon Reference: http://blog.cofess.com/hexo-theme-pure/iconfont/demo_fontclass.html  enableMathJax: true #Enable mathjax support, to use mathematical notations  highlightjs:  langs: [\u0026#34;python\u0026#34;, \u0026#34;javascript\u0026#34;] # refer to http://staticfile.org/, search highlight.js, already have highlight.min.js   tag_cloud:  min: 8  max: 20  # Allows you to specify an override stylesheet  # put custom.css in $hugo_root_dir/static/  # customCSS: css/custom.css   menuIcons:  enable: true # 是否启用导航菜单图标  home: icon-home-fill  archives: icon-archives-fill  categories: icon-folder  tags: icon-tags  repository: icon-project  books: icon-book-fill  links: icon-friendship  about: icon-cup-fill   # profile  profile:  enabled: true # Whether to show profile bar  avatar: avatar.png  gravatar: # Gravatar email address, if you enable Gravatar, your avatar config will be overriden  author: YTTE\t#更改\t  author_title: 努力奋斗，不负韶华。\t#更改\t  author_description: Good Good Study, Day Day Up~\t#更改\t  location: Anhui, China\t#更改\t  follow: https://github.com/yougithubname\t#更改\t  # Social Links  social:  links:  github: https://github.com/yougithubname\t#更改  # weibo: http://weibo.com/{yourid}  # twitter: https://twitter.com/  # facebook: /  rss: /index.xml  link_tooltip: false # enable the social link tooltip, options: true, false  # Site  site:  logo:  enabled: true  width: 40  height: 40  url: favicon.ico  title: YTTE-blogs # 页面title\t#更改  favicon: favicon.ico  board: \u0026lt;p\u0026gt; 人生无常，大肠包小肠~\u0026lt;/p\u0026gt; # 公告牌\t更改   # Share  # weibo,qq,qzone,wechat,tencent,douban,diandian,facebook,twitter,google,linkedin  share:  enable: true # 是否启用分享  sites: weibo,qq,wechat,facebook,twitter # PC端显示的分享图标  mobile_sites: weibo,qq,qzone # 移动端显示的分享图标   # Comment  comment:  type: # type disqus/gitalk/valine 启用哪种评论系统  disqus: your_disqus_name # enter disqus shortname here  gitalk: # gitalk. https://gitalk.github.io/  owner: #必须. GitHub repository 所有者，可以是个人或者组织。  admin: #必须. GitHub repository 的所有者和合作者 (对这个 repository 有写权限的用户)。  repo: #必须. GitHub repository.  ClientID: #必须. GitHub Application Client ID.  ClientSecret: #必须. GitHub Application Client Secret.  valine: # Valine. https://valine.js.org  appid: # your leancloud application appid  appkey: # your leancloud application appkey  notify: # mail notifier , https://github.com/xCss/Valine/wiki  verify: # Verification code  placeholder: enjoy~ # comment box placeholder  avatar: mm # gravatar style  meta: nick,mail # custom comment header  pageSize: 10 # pagination size  visitor: false # Article reading statistic https://valine.js.org/visitor.html   # Donate  donate:  enable: false  # 微信打赏  wechatpay:  qrcode: donate/wechatpayimg.png  title: 微信支付  # 支付宝打赏  alipay:  qrcode: donate/alipayimg.png  title: 支付宝   # PV  pv:  busuanzi:  enable: false # 不蒜子统计  leancloud:  enable: false # leancloud统计  app_id: # leancloud \u0026lt;AppID\u0026gt;  app_key: # leancloud \u0026lt;AppKey\u0026gt;   # wordcount  postCount:  enable: true  wordcount: true # 文章字数统计  min2read: true # read time 阅读时长预计   # config  config:  skin: theme-black # theme color default is white. other type [theme-black,theme-blue,theme-green,theme-purple]  layout: main-center # main-left main-center main-right  excerpt_link: Read More  toc: true   # Sidebar  sidebar: right   # Search  search:  enable: true # enable search. thanks for https://raw.githubusercontent.com/ppoffice/hexo-theme-icarus/master/source/js/insight.js   # Sidebar only the following widgets. you can remove any you don\u0026#39;t like it.  widgets:  - board  - tag_cloud  - category  - recent_posts 扩展篇 鉴于扩展篇中需要其他知识的支持，所以提前把链接放出来：\n 一级域名 - YTTE Site\n二级域名 - YTTE Site\nHttps - YTTE Site\nGit多个ssh - YTTE Site\nGitee自动更新部署的脚本 - YTTE Site\n 继续更新中。。。。。。。。。。。。。。。\n 参考 ：\n hexo史上最全搭建教程_zjufangzh-CSDN_hexo Git下载安装及设置详细教程-CSDN博客_git下载 Git教程 - 廖雪峰的官方网站 (liaoxuefeng.com) NodeJ 安装及环境配置_DistanceZK的博客-CSDN博客 windows+hugo+github搭建个人博客 - nira - 博客园 (cnblogs.com)  搞定！\n人生无常，大肠包小肠！\n继续学习吧！\n","id":5,"section":"posts","summary":"","tags":["bolg","hugo","hexo"],"title":"静态个人博客创建配置部署","uri":"https://gb.ytte.top/2022/03/21/%E9%9D%99%E6%80%81%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%88%9B%E5%BB%BA%E9%85%8D%E7%BD%AE%E9%83%A8%E7%BD%B2/","year":"2022"},{"content":"Gitee自动更新部署的脚本使用 gitee 容量比github大，单存在不能绑定自定义域名，以及不能自动更新部署的问题。\n自动更新脚本  参考：自动更新gitee pages，_永远不会太晚的博客-CSDN博客\n 前置：\n  node\n  puppeteer\n    需要node环境\nNodeJS 安装及环境配置\n  新建建一个文件夹 用来安装 puppeteer\n  在新建的文件夹内 打开cmd输入下方代码后，不要关闭cmd窗口。\nnpm init -y npm i -s puppeteer@1.8.0 `\n  安装好后会出现package.json，打开全部替换为如下代码\n{  \u0026#34;name\u0026#34;: \u0026#34;puppeteer\u0026#34;,  \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;,  \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;,  \u0026#34;scripts\u0026#34;: {  \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34;  },  \u0026#34;keywords\u0026#34;: [],  \u0026#34;author\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;,  \u0026#34;dependencies\u0026#34;: {  \u0026#34;puppeteer\u0026#34;: \u0026#34;^1.8.0\u0026#34;  },  \u0026#34;name\u0026#34;: \u0026#34;git-update-pages\u0026#34;,  \u0026#34;bin\u0026#34;: {  \u0026#34;git-update-pages\u0026#34;: \u0026#34;index.js\u0026#34;  } }   新建index.js，填入一下代码.其中 17、22、31 行需要更改为自己的信息。更改完成后保存。\n#! /usr/bin/env node // 此处安装版本为 1.8.0 const puppeteer = require(\u0026#34;puppeteer\u0026#34;)  // 主要原理在于使用xpath获取html页面dom元素，脚本代替小手自动触发点击事件 async function giteeUpdate() {  const browser = await puppeteer.launch({  // 此处可以使用 false 有头模式进行调试, 调试完注释即可  headless: false  })  const page = await browser.newPage()  await page.goto(\u0026#34;https://gitee.com/login\u0026#34;)   // 1. 获取账号input，自动输入  let accountElements = await page.$x(\u0026#39;//*[@id=\u0026#34;user_login\u0026#34;]\u0026#39;)  // 🚨需要设置为自己的gitee账户🚨  await accountElements[0].type(\u0026#34;自己的gitee账户\u0026#34;)   // 2. 获取密码input，自动输入  let pwdElements = await page.$x(\u0026#39;//*[@id=\u0026#34;user_password\u0026#34;]\u0026#39;)  // 🚨需要设置自己的gitee密码🚨  await pwdElements[0].type(\u0026#34;自己的gitee密码\u0026#34;)   // 3. 获取登录按钮，触发点击事件  let loginButtons = await page.$x(\u0026#39;//*[@class=\u0026#34;git-login-form-fields\u0026#34;]/div[4]/input\u0026#39;)  await loginButtons[0].click()   // 4. 等待登录成功  await page.waitFor(1000)  🚨需要设置自己的gitee pages页面🚨  await page.goto(\u0026#34;https://gitee.com/你的用户名/你的仓库名/pages\u0026#34;)   // 5. 监听触发的确认弹框，并点击确认  await page.on(\u0026#34;dialog\u0026#34;, async dialog =\u0026gt; {  console.log(\u0026#34;确认更新\u0026#34;)  dialog.accept()  })   // 6. 点击更新按钮，并弹出确认弹窗  let updateButtons = await page.$x(\u0026#39;//*[@id=\u0026#34;pages-branch\u0026#34;]/div[6]\u0026#39;)  console.log(1111, updateButtons)  await updateButtons[0].click()   //7. 轮询并确认是否更新完毕  while (true) {  await page.waitFor(2000)  try {  // 获取更新状态标签  deploying = await page.$x(\u0026#39;//*[@id=\u0026#34;pages_deploying\u0026#34;]\u0026#39;)  if (deploying.length \u0026gt; 0) {  console.log(\u0026#34;更新中...\u0026#34;)  } else {  console.log(\u0026#34;更新完毕\u0026#34;)  break  }  } catch (error) {  break  }  }  await page.waitFor(500)   // 8.更新完毕，关闭浏览器  browser.close() }  giteeUpdate()   打开刚刚的cmd窗口，输入如下代码。（将npm 模块链接到对应的运行项目中去）即可全局使用package.json中bin的命令。\nnpm link   现在就可以本地使用git-update-pages命令来自动化操作去更新gitee page 更新部署了。\n ","id":6,"section":"posts","summary":"","tags":["gitee"],"title":"Gitee自动更新部署的脚本","uri":"https://gb.ytte.top/2022/03/20/gitee%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%E9%83%A8%E7%BD%B2%E7%9A%84%E8%84%9A%E6%9C%AC/","year":"2022"},{"content":"本地同步到一个git仓库    序号 代码 意译     1 git init 初始化仓库   2 在仓库端点击初始化仓库添加readme文件    3 git pull 远程仓库地址 拉取同步远程仓库代码   4- git config \u0026ndash;global user.name \u0026ldquo;xxx\u0026rdquo; 设置commit后显示的提交用户的name==这里要注意\u0026ndash;global，如果需要让不同仓库使用不同的name，email，可以将\u0026ndash;global去掉==   5 git config \u0026ndash;global user.email \u0026ldquo;xxx@xxx.com\u0026rdquo; 设置commit后显示的提交用户的email   6 git add . 将本地文件代码等交给暂存区，（HEAD指针不变化，依然指向init的状态）   7 git commit -m “提交辨识信息” 将本地文件代码等交给本地仓库（HEAD指针指向“提交辨识信息”）   8 git remote -v 查看有无远程仓库   9 ssh-keygen -t rsa -C \u0026ldquo;xxx@qq.com\u0026rdquo; 为了使用ssh连接，创建ssh公钥，回车后接着回车知道结束（问的是1.是否更改路径。2.是否设置密码。3.是否设置其他什么的 不需要！）   10 将C:\\Users\\Administrator\\ .ssh\\id_rsa.pub 里面的内容复制到github右上角头像里下方设置里ssh里面。    11 ssh -T git@github.com 测试是否连接   12 git remote add origin git@github.com:xxx/xxx.github.io.git 使用ssh地址添加远程仓库   13 git branch -v 查看本地分支   14 git push -u origin master 将本地仓库本地分支的内容同步到远程仓库origin的master分支   15 git branch \u0026ndash;set-upstream-to=origin/远程分支的名字 本地分支的名字 关联本地分支与远程分支（这样以后同步只需要git push）    还是不会那就传送。\n同一个本地仓库同步到多个不同厂家的git 仓库    以下都是基于本地同步过一个git仓库      一般ssh密钥都是保存C:\\Users\\Administrator\\ .ssh里 这里的Administrator是你的电脑本次的登录的账户，如果电脑账户只有一个一般都是这个名称   进入C:\\Users\\Administrator\\ .ssh 创建config文件（无后缀名）    使用notepad++或者vscode打开config文件，将代码输入保存     #Default github user（xxxx@xxx.com）-这是注释  Host ytte #把默认的常用的github Host设为github.com较好  HostName github.com # 远程仓库的域名地址，gitee是gitee.com \tPort 22 # 一般服务端端口都是22，所以可以省略不写，如果使用云服务器做为git公共仓库，且服务端22端口被更改则需要写上更改后的端口  PreferredAuthentications publickey  IdentityFile ~/.ssh/id_rsa #证书文件路径  #second user(xxx@xxx.com)  Host gitee  HostName gitee.com  PreferredAuthentications publickey  IdentityFile ~/.ssh/id_rsa_gitee #-----这里的id_rsa_gitee是后面创建密钥中需要更改的名称    序号 代码 意译     1 ssh-keygen -t rsa -C \u0026ldquo;xxx@qq.com\u0026rdquo; 在回车后，会提示是否更改路径和名称，我们需要把名称更改，不改路径C:\\Users\\Administrator\\.ssh\\id_rsa_gitee。这里的名称就是上方代码最后填写的   2 由于一个git账号（不论gitee还是github）都只能绑定一个ssh密钥，且该密钥不能被同时绑定两个账号（不论gitee还是github）。所以再次创建一个密匙。 ==当然可以从中权衡，github是国外的，通过ssh访问会快于https访问，而gitee是国内的，https和ssh两种访问速度差不多，所以github使用ssh，gitee使用https也是可以的，这样就不需要配置多个ssh密匙了==   3 git remote -v    4 git remote rm 删除之前的远程仓库，因为如上代码使用config配置的方式，远程仓库地址需要重新配置   5 git remote add gitee1 git@gitee:xxx/xxx.gitee.io.git 注意这里git@gitee:xxx/xxx.gitee.io.git的gitee是config中的Host gitee   6 git remote add github1 git@ytte:xxx/xxx.github.io.git 注意这里git@ytte:xxx/xxx.github.io.git的ytte是config中的Host ytte   7 git remote -v 查看两个远程仓库 gitee1 和 github1 是否正确   8 这里没有设置user.name和user.email 因为在最上面4,5中已经设置过了\u0026ndash;global全局   9 git pull git@gitee:xxx/xxx.gitee.io.git 这里@后的都需要更改为自己的信息   10 git pull git@ytte:xxx/xxx.github.io.git 这里@后的都需要更改为自己的信息    git pull 远程仓库名称 远程仓库分支名称 \u0026ndash;allow-unrelated-histories 如果不能pull，可以使用此命令强制pull合并   11 git add .    12 git commit -m “用于辨别提交信息”    13 git push -u gitee1 master gitee1 为本表格5中设置的远程仓库名称 ，master为远程仓库分支   14 git push -u github1 master github1 为本表格6中设置的远程仓库名称 ，master为远程仓库分支   15 fetch未指定地址时是默认从第一个仓库中取      搞定！\n人生无常，大肠包小肠！\n继续学习吧！\n","id":7,"section":"posts","summary":"","tags":["git","github","gitee"],"title":"git_ssh和多个ssh","uri":"https://gb.ytte.top/2022/03/20/git_ssh%E5%92%8C%E5%A4%9A%E4%B8%AAssh/","year":"2022"},{"content":"Https  视频====》哔哩哔哩_bilibili\n 前置要求   获得域名\n一级域名 - YTTE Site\n  设置二级域名 IPHP 二级域名 - YTTE Site\n  docker基础知识\n  HTTPS流程 个人理解（未学过计算机网络 need todo）\n设置SSL证书 上篇   进入阿里云控制台，点击SSL证书\n  进入后，点击左侧的SSL证书选项\n  再点击免费证书\n  点击立即购买\n  免费的证书每人每年只能拥有20张。所以选项默认不做变动。立即购买\n  点击创建证书\n  点击左侧栏的信息管理，新建联系人。按要求填写信息。\n  点击左侧栏SSL证书，点击免费证书，点击证书申请，填写域名，选择自动DNS验证，系统自动生成，下一步。\n  记录下这里的记录类型，主机记录，记录值\n  新网页标签打开控制台里的域名。\n  点击左侧栏【域名列表】，找到一个域名，点击右侧的【解析】。\n  点击添加记录。将9中的《记录类型》，《主机记录》，《记录值》选择或者填。《解析路线》和《TTL》默认。点击【确认】。\n  回到8中的页面点击【验证】，成功即可。点击【下载】将证书下载保存（选择Nginx版本）。\n   下篇   使用xhsell连接阿里云服务器\n  使用docker安装Nginx\ndocker pull nginx   运行镜像\ndocker run -p 80:80 -p 443:443 --name nginx -v /mnt/nginx/html:/usr/share/nginx/html -v /mnt/nginx/logs:/var/log/nginx -v /mnt/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v /mnt/nginx/cert/:/etc/nginx/cert/ --restart=always --privileged=true -d nginx 前缀 /mnt/nginx 可以自定义。\n  运行完成后 docker ps查看下是否运行成功（是否存在，STATUS是不是up）\n然后进入cert文件夹\ncd /mnt/nginx/cert/ 将上篇第13步下载保存的压缩包拖动移动放入xshell窗口里（就是传输到服务当前文件夹，如果使用xftp更加方便）。\n  传输完成后。输入一下命令解压，可以得到后缀为pem和pub的文件。\ntar -zxvf 压缩包全名   使用xftp进入/mnt/nginx/conf/文件夹，右键nginx.conf文件，没有的话，自己创建一个。如下代码全部替换。然后保存。\n# ctrl + F 查找 填写 来寻找需要更改的地方。 worker_processes 1;  events {  worker_connections 1024; }  http {  include /etc/nginx/mime.types; #不需要更改  default_type application/octet-stream;\t#不需要更改   log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39;  \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39;  \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;;\t#不需要更改   #access_log logs/access.log main;   sendfile on;\t#不需要更改  #tcp_nopush on;   #keepalive_timeout 0;  keepalive_timeout 65;\t#不需要更改   #gzip on; \tserver {  listen 443 ssl; # 这里是新版Nginx的写法  server_name blogimg.ytte.top; # 填写你的域名  # ssl on; # 这种已经不使用了，这么写会报警告，可以直接去掉采用第一行的写法  root /home/git/hugo; # 前台文件存放文件夹，一般使用 Nginx 初始化的文件夹，当然也可以自己修改  index index.html; # 上面配置的文件夹里面的index.html  ssl_certificate cert/7448435_blogimg.ytte.top.pem; #填写改成你的证书的名字  ssl_certificate_key cert/7448435_blogimg.ytte.top.key;#填写你的证书的名字  ssl_session_timeout 5m;  ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;  ssl_protocols TLSv1 TLSv1.1 TLSv1.2;  ssl_prefer_server_ciphers on;  location / {  index index.html index.htm;  } \t} \tserver {  listen 80;  server_name blogimg.ytte.top; #填写你的域名  rewrite ^(.*)$ https://$host:443$1 permanent; # 把http的域名请求转成https且转发到443端口 \t} }   回到Xshell使用如下命令查看nginx的容器id，重启nginx是的配置文件生效。\ndocker ps  docker restart nginx容器的id  docker ps #查看是否重启成功（STATUS为up即成功，如果显示restarting，等一会再docker ps ，如果还显示restarting，可能是6中 nginx.conf 配置错误，仔细检查一下；如果还是不行，进入/mnt/nginx/logs下查看下error错误日志，在寻找解决办法）   至此https配置完成，我们只需要访问6中配置的域名（端口为80 默认不写），nginx就能帮我们把访问这个域名80端口自动转到这个域名443端口，实现https访问。\n  基于上述扩展 因为本例是基于上述的配置七牛云文件访问的二级域名。七牛云中使用的还是http方式，所以我们可以将其更改为https方式。\n只需要重复二级域名操作 ——\u0026gt;2.配置七牛云中的步骤，在第7步与第8步之间，执行一下内容。\n 注意：加速域名要填写你申请ssl证书填写的域名\n   加速域名填写不同于之前的二级玉米；通信协议选择【https】，点击【SSL 证书服务】\n  点击【上传自有证书】。\n  将本文上篇的第13步骤保存的压缩包解压，并把两个文件的内容复制这里（证书内容=\u0026gt;pem；证书私钥=\u0026gt;key）点击【确认上传】\n  等待上传成功即可，返回添加域名界面，填写你申请ssl证书填写的域名，通信协议再次点击选择【https】，就可以选择已经添加的证书，之后执行二级域名中步骤（即第8步骤开始）。\n   搞定！\n人生无常，大肠包小肠！\n继续学习吧！\n","id":8,"section":"posts","summary":"\u003c!-- raw HTML omitted --\u003e","tags":["Https"],"title":"Https","uri":"https://gb.ytte.top/2022/03/19/https/","year":"2022"},{"content":"一级域名 1. 购买域名  地址：域名_域名查询_域名注册_.com_.cn-阿里云 (aliyun.com)  任何服务，都可以不买，只购买域名使用权\n  选择个人\n  选择个人信息\n    未实名认证的，进行阿里云个人实名认证 (aliyun.com)\n  选择已阅读，购买\n  2. 等待审核 进入控制台，点击域名列表可以看到，域名正在审核\n","id":9,"section":"posts","summary":"","tags":["域名"],"title":"一级域名","uri":"https://gb.ytte.top/2022/03/16/%E4%B8%80%E7%BA%A7%E5%9F%9F%E5%90%8D/","year":"2022"},{"content":"二级域名 给七牛云配置阿里云所属顶级域名的二级域名，本文以simg.ytte.top为例\n如果只想配置二级域名，不配置二级域名转向七牛云的话，直接参看步骤9~12。\n1. 首先要有一级域名 一级域名 - github、一级域名 - gitee\n2.配置七牛云 由于七牛云每月有初始免费的存储容量10g、CDN 10g 等等。所以很nice，就选它。\n 创建账号 选择对象存储 新建空间=\u0026gt; 访问控制选择公开。（尽量选择第一华东地区（便宜）） 进入空间，点击【域名管理】 点击自定义 CDN 加速域名【域名绑定】 (使用cdn可以加速我们访问空间内文件速度，加载更快)。其他的选项默认 填写二级域名（即一级域名价格前缀，比如我当时腾讯云申请的ytte.top，在此前面加上自己想要的前缀gb. （有个英文点哦） 其他的默认不做更改（关闭忽略 URL 参数），点击【创建】  如图将这里的域名前缀和CNAME值填入阿里云新建域名解析下。（七牛云页面不要关闭）  进入阿里云控制台 点击【域名】，点击【域名列表】，找到配置的一级域名点击【解析】 点击【添加记录】，将上述的域名前缀和CNAME值填入，记录类型选择CNAME。点击【确认】 如果阿里云域名解析状态显示正常，即可  再回到七牛云中，等待 观察是否设置成功   完成，这样就可以在七牛云 ——\u0026gt;对象存储——\u0026gt;空间——\u0026gt;【文件管理】——\u0026gt;将我们设置好的二级域名设置为外链域名点击【保存默认域名】——\u0026gt;【上传文件】，然后通过上传后的文件【更多】——\u0026gt;【复制外链】来访问或者下载文件了。从地址可以看出我们是使用刚配置的二级域名来访问这个文件的。\n   搞定！\n人生无常，大肠包小肠！\n继续学习吧！\n","id":10,"section":"posts","summary":"","tags":["域名"],"title":"二级域名","uri":"https://gb.ytte.top/2022/03/16/%E4%BA%8C%E7%BA%A7%E5%9F%9F%E5%90%8D/","year":"2022"},{"content":"Java8之Consumer、Supplier、Predicate和Function攻略 - 公众号好好学java - 博客园 (cnblogs.com)\n","id":11,"section":"posts","summary":"","tags":["基础"],"title":"Consumer、Supplier、Predicate和Function","uri":"https://gb.ytte.top/2022/03/14/consumersupplierpredicate%E5%92%8Cfunction/","year":"2022"},{"content":"ZooKeeper 单机（本地）安装配置启动 非docker 视频====》_哔哩哔哩_bilibili p7\n  需要jdk环境，并配置环境变量\nlinux安装jdk8\n  下载ZooKeeper压缩包（Apache Downloads)放在linux下的/mnt/zookeeper下。\n  tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz   将文件夹改名\nmv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7/   更改conf下 的zoo-sample.cfg为zoo.cfg\n  更改zoo.cfg中\ndataDir=/mnt/zookeeper/zookeeper-3.5.7/data\rdataLogDir=/mnt/zookeeper/zookeeper-3.5.7/dataLog   启动服务端：\n**注意：**需要在/mnt/zookeeper/zookeeper-3.5.7/目录下使用启动命令bin/zkServer.sh start 不能再/mnt/zookeeper/zookeeper-3.5.7/bin/目录下使用zkServer.sh start\n  使用jps -l 查看是否启动了zookeeper 的java进程。\n  启动客户端： bin/zkCli.sh\n    ls /查看节点\n  quit退出客户端\n  bin/zkServer.sh status查看zookeeper的状态（standalone单机模式）\n  bin/zkServer.sh stop 停止服务器端\n    docker 配置参数解读 视频====》p8\n  tickTime=2000——通信的心跳时间\nZooKeeper服务器与客户端之间通信频率，每2000ms发送一次。\n  initLimit=10——LF初始通信时限\nLeader与Follower初始连接时能容忍的最多心跳数(tickTime的数量)\n  syncLimit = 5：LF同步通信时限\nLeader和Follower之间通信时间如果超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。\n  dataDir：保存Zookeeper中的数据\n注意：默认的tmp目录，容易被Linux系统定期删除，所以一般不用默认的tmp目录。\n  clientPort = 2181：客户端连接端口，通常不做修改。\n  集群操作","id":12,"section":"posts","summary":"","tags":["ZooKeeper"],"title":"ZooKeeper","uri":"https://gb.ytte.top/2022/03/14/zookeeper/","year":"2022"},{"content":"Filter Filter-CSDN博客_java过滤器\nLstener Filter与Listener - 简书 (jianshu.com)\nListener(监听器)的简单介绍_LrvingTc的博客-CSDN博客_listener\nServlet JavaWeb——Servlet（全网最详细教程包括Servlet源码分析）_扬俊的小屋-CSDN博客_servlet\n四大域对象 JavaWeb四大域对象 - 简书 (jianshu.com)\nJAVA中的四大域对象总结_kun blog-CSDN博客_域对象\n","id":13,"section":"posts","summary":"","tags":["基础"],"title":"Filter/ Lstener/ Servlet/四大域对象","uri":"https://gb.ytte.top/1/01/01/filter/-lstener/-servlet/%E5%9B%9B%E5%A4%A7%E5%9F%9F%E5%AF%B9%E8%B1%A1/","year":"0001"},{"content":"JVM上篇 JVM与Java体系结构 1. Java虚拟机整体架构祥图 2.Java代码执行过程详图 3. JVM的架构模型","id":14,"section":"posts","summary":"","tags":["JVM"],"title":"JVM上篇","uri":"https://gb.ytte.top/1/01/01/jvm%E4%B8%8A%E7%AF%87/","year":"0001"},{"content":"彻底弄懂session，cookie，token - SegmentFault 思否\n彻底理解cookie，session，token - 墨颜丶 - 博客园 (cnblogs.com)\n","id":15,"section":"posts","summary":"","tags":["基础"],"title":"session，cookie，token","uri":"https://gb.ytte.top/1/01/01/sessioncookietoken/","year":"0001"},{"content":"SpringBoot 5. web开发 3. 请求参数处理 1. 普通参数与基本注解 1.1注解 视频====》尚硅谷雷神SpringBoot2_bilibili\n@PathVariable、\n@RequestHeader、\n@ModelAttribute、\n@RequestParam、\n@MatrixVariable（矩阵变量）、\n@CookieValue、\n@RequestBody\n@RestController public class ParameterTestController {    // car/2/owner/zhangsan  @GetMapping(\u0026#34;/car/{id}/owner/{username}\u0026#34;)  public Map\u0026lt;String,Object\u0026gt; getCar(@PathVariable(\u0026#34;id\u0026#34;) Integer id,  @PathVariable(\u0026#34;username\u0026#34;) String name,  @PathVariable Map\u0026lt;String,String\u0026gt; pv,  @RequestHeader(\u0026#34;User-Agent\u0026#34;) String userAgent,  @RequestHeader Map\u0026lt;String,String\u0026gt; header,  @RequestParam(\u0026#34;age\u0026#34;) Integer age,  @RequestParam(\u0026#34;inters\u0026#34;) List\u0026lt;String\u0026gt; inters,  @RequestParam Map\u0026lt;String,String\u0026gt; params,  @CookieValue(\u0026#34;_ga\u0026#34;) String _ga,  @CookieValue(\u0026#34;_ga\u0026#34;) Cookie cookie){    Map\u0026lt;String,Object\u0026gt; map = new HashMap\u0026lt;\u0026gt;();  // map.put(\u0026#34;id\u0026#34;,id); // map.put(\u0026#34;name\u0026#34;,name); // map.put(\u0026#34;pv\u0026#34;,pv); // map.put(\u0026#34;userAgent\u0026#34;,userAgent); // map.put(\u0026#34;headers\u0026#34;,header);  map.put(\u0026#34;age\u0026#34;,age);  map.put(\u0026#34;inters\u0026#34;,inters);  map.put(\u0026#34;params\u0026#34;,params);  map.put(\u0026#34;_ga\u0026#34;,_ga);  System.out.println(cookie.getName()+\u0026#34;===\u0026gt;\u0026#34;+cookie.getValue());  return map;  }    @PostMapping(\u0026#34;/save\u0026#34;)  public Map postMethod(@RequestBody String content){  Map\u0026lt;String,Object\u0026gt; map = new HashMap\u0026lt;\u0026gt;();  map.put(\u0026#34;content\u0026#34;,content);  return map;  }    //1、语法： 请求路径：/cars/sell;low=34;brand=byd,audi,yd  //2、SpringBoot默认是禁用了矩阵变量的功能  // 手动开启：原理。对于路径的处理。UrlPathHelper进行解析。  // removeSemicolonContent（移除分号内容）支持矩阵变量的  //3、矩阵变量必须有url路径变量才能被解析  @GetMapping(\u0026#34;/cars/{path}\u0026#34;)  public Map carsSell(@MatrixVariable(\u0026#34;low\u0026#34;) Integer low,  @MatrixVariable(\u0026#34;brand\u0026#34;) List\u0026lt;String\u0026gt; brand,  @PathVariable(\u0026#34;path\u0026#34;) String path){  Map\u0026lt;String,Object\u0026gt; map = new HashMap\u0026lt;\u0026gt;();   map.put(\u0026#34;low\u0026#34;,low);  map.put(\u0026#34;brand\u0026#34;,brand);  map.put(\u0026#34;path\u0026#34;,path);  return map;  }   // /boss/1;age=20/2;age=10   @GetMapping(\u0026#34;/boss/{bossId}/{empId}\u0026#34;)  public Map boss(@MatrixVariable(value = \u0026#34;age\u0026#34;,pathVar = \u0026#34;bossId\u0026#34;) Integer bossAge,  @MatrixVariable(value = \u0026#34;age\u0026#34;,pathVar = \u0026#34;empId\u0026#34;) Integer empAge){  Map\u0026lt;String,Object\u0026gt; map = new HashMap\u0026lt;\u0026gt;();   map.put(\u0026#34;bossAge\u0026#34;,bossAge);  map.put(\u0026#34;empAge\u0026#34;,empAge);  return map;   }  } ","id":16,"section":"posts","summary":"","tags":["SpringBoot"],"title":"SpringBoot","uri":"https://gb.ytte.top/1/01/01/springboot/","year":"0001"},{"content":"SpringCloud alibaba","id":17,"section":"posts","summary":"","tags":["SpringCloud"],"title":"SpringCloud alibaba","uri":"https://gb.ytte.top/1/01/01/springcloud-alibaba/","year":"0001"},{"content":"线程的实现方式及其优缺点 继承Thread类 public class TestCode1 extends Thread { \tprivate int i; \tpublic void run() \t{ \tfor(;i\u0026lt;100;i++) \t{ \tSystem.out.println(getName()+\u0026#34; \u0026#34;+i); \t} \t} \tpublic static void main(String[] args) \t{ \tfor(int i=0;i\u0026lt;100;i++) \t{ \tSystem.out.println(Thread.currentThread().getName()+\u0026#34; \u0026#34;+i); \tif(i==20){ \tnew TestCode1().start(); \tnew TestCode1().start(); \t} \t} \t} } 实现Runnable接口  实现Runnable接口，重写run方法，实例要作为Thread的target来创建对象，thread.start()（推荐这种，清洗可见）  class MyRunnable implements Runnable{  @Override  public void run() {  for (int i = 0; i \u0026lt; 20; i++) {  System.out.println(Thread.currentThread().getName()+\u0026#34; \u0026#34;+i);  }  } } public class MyThread {  public static void main(String[] args) {  //创建自定义类对象 线程任务对象  MyRunnable mr = new MyRunnable();  //创建线程对象  Thread t = new Thread(mr, \u0026#34;小强\u0026#34;);  t.start();  for (int i = 0; i \u0026lt; 20; i++) {  System.out.println(\u0026#34;旺财 \u0026#34; + i);  }  } }  或者在方法内直接重写  public class MyThread {  public static void main(String[] args) {   Runnable r = new Runnable(){  public void run(){  for (int i = 0; i \u0026lt; 20; i++) {  System.out.println(\u0026#34;张宇:\u0026#34;+i);  }  }  };  new Thread(r).start();  } }  直接lambda创建Runnable实现类  public class MyThread {  public static void main(String[] args) {  Runnable r = () -\u0026gt; {  for (int i = 0; i \u0026lt; 20; i++) {  System.out.println(\u0026#34;张宇:\u0026#34;+i);  }  };  new Thread(r).start();  } } 实现Callable接口   创建Callable接口的实现类，并实现Call（）方法，该方法将作为线程执行体，且该方法有返回值，再创建Callable实现类的实例。从Java8开始，可以直接使用Lambda表达式创建Callable对象。\n  使用FutureTask来包装Callable对象，该FutureTask对象封装了该Callable对象的call方法的返回值。\n  使用FutureTask对象作为Thread对象的target创建并启动新线程。\n  调用FutureTask对象的get（）方法来获取子线程执行结束后的返回值。\n  class MyCallable implements Callable {  @Override  public Object call() throws Exception {  int i = 0;  for (; i \u0026lt; 100; i++) {  Thread.sleep(100);  System.out.println(Thread.currentThread().getName() + \u0026#34;循环遍历i的值\u0026#34; + i);  }  return null;  } } public class ThreadTest {  public static void main(String[] args) {  Callable myCallable = new MyCallable();  FutureTask task = new FutureTask(myCallable);  new Thread(task).start();  for (int i = 0; i \u0026lt; 100; i++) {  try {  Thread.sleep(100);  System.out.println(Thread.currentThread().getName()+\u0026#34;循环遍历i的值\u0026#34; + i);  }  catch (Exception e) {  e.printStackTrace();  }  }  } }  //lambda方式 public class ThreadTest {  public static void main(String[] args) {  FutureTask\u0026lt;Integer\u0026gt; task = new FutureTask\u0026lt;\u0026gt;(()-\u0026gt;{  int i=0;  for(;i\u0026lt;100;i++){  System.out.println(Thread.currentThread().getName()+\u0026#34;循环变量i的值：\u0026#34;+i);  }  return i;  });  for(int i=0;i\u0026lt;100;i++){  System.out.println(Thread.currentThread().getName()+\u0026#34;循环变量i的值：\u0026#34;+i);  if(i==20){  new Thread(task,\u0026#34;有返回值的线程\u0026#34;).start();  }  try{  System.out.println(\u0026#34;子线程的返回值：\u0026#34;+task.get());  }  catch (Exception e) {  e.printStackTrace();  }  }  } } 异同点和优缺点 异同点   相同点\n 都可以实现多线程 Runnable与Callable实现方式类似    不同点\n Thread方式是通过继承来实现。 Runnable与Callable是通过实现接口方式创建线程。 Runnable与Callable  因为通过FutureTask包装Callable接口，FutureTask有get方法可以获取返回值，所以只有这种方式创建的子线程有返回值。 run方法无法抛出异常，call方法可以抛出checked exception。 Callable和Runnable都可以应用于executors。而Thread类只支持Runnable.      优缺点   优点\n  Thread\n 编写简单，如果需要访问当前线程，则无须使用Thread.currentThread（）方法，直接使用this即可获得当前线程。    Runnable与Callable\n 可以继承其他类 多个线程可以共享一个target对象，所以非常适合多个相同线程来处理同一份资源的情况，从而可以将CPU、代码和数据分开，形成清晰的模型，较好地体现了面向对象的思想。      缺点\n Thread  继承的局限性，不能继承其他类   Runnable与Callable  编程稍稍复杂，如果需要访问当前线程，则必须使用Thread.currentThread（）方法。      补充 扩充：在java中，每次程序运行至少启动2个线程。一个是main线程，一个是垃圾收集线程。因为每当使用 java命令执行一个类的时候，实际上都会启动一个JVM，每一个JVM其实在就是在操作系统中启动了一个进 程。\n子线程执行完，主线程执行 正常如1中的各种实现方式 Local跳转方式，Shimo跳转方式，主线程与子线程是争夺线程资源的，所以一般情况是会出现类似交替执行的状况。\nFutureTask来包装Callable对象 上方（ Local跳转方式，Shimo跳转方式）的这种是子线程无返回值的情况，这种情况下，子线程与多线程可以争夺线程资源，\n如果为子线程添加返回值，使用 FutureTask来包装Callable对象，通过task.get()来获取返回值，主线程进行到task.get()时，如果子线程还在执行，没有返回，主线程就会子线程返回之前等待挂起。从而达到要求。\npublic class TestCode3 { \tpublic static void main(String[] args) { \tFutureTask\u0026lt;Integer\u0026gt; task = new FutureTask\u0026lt;Integer\u0026gt;((Callable\u0026lt;Integer\u0026gt;)()-\u0026gt;{ \tint i=0; \tfor(;i\u0026lt;100;i++){ \tSystem.out.println(Thread.currentThread().getName()+\u0026#34;循环变量i的值：\u0026#34;+i); \t} \treturn i; \t}); \tfor(int i=0;i\u0026lt;100;i++){ \tSystem.out.println(Thread.currentThread().getName()+\u0026#34;循环变量i的值：\u0026#34;+i); \tif(i==20){ \tnew Thread(task,\u0026#34;有返回值的线程\u0026#34;).start(); \t} \ttry{ \tSystem.out.println(\u0026#34;子线程的返回值：\u0026#34;+task.get()); \t} \tcatch (Exception e) { \te.printStackTrace(); \t} \t} \t} } join方法 参考资料链接\n结论：t.join()方法只会使主线程(或者说调用t.join()的线程)进入等待池并等待t线程执行完毕后才会被唤醒。并不影响同一时刻处在运行状态的其他线程。\nt.join方法底层中使用wait(0)方法，即使用了该方法的线程会进入无限等待中，直到 t 执行完成才返回。进入等待的只是调用了join方法的线程，其他线程没开启的不会开启，开启了的不会等待继续执行。\npublic class TestJoin { \tpublic static void main(String[] args) throws InterruptedException { \t// TODO Auto-generated method stub \tSystem.out.println(Thread.currentThread().getName()+\u0026#34; start\u0026#34;); \tThreadTest t1=new ThreadTest(\u0026#34;A\u0026#34;); \tThreadTest t2=new ThreadTest(\u0026#34;B\u0026#34;); \tThreadTest t3=new ThreadTest(\u0026#34;C\u0026#34;); \tSystem.out.println(\u0026#34;t1start\u0026#34;); \tt1.start(); \tSystem.out.println(\u0026#34;t1end\u0026#34;); \tSystem.out.println(\u0026#34;t2start\u0026#34;); \tt2.start(); \tSystem.out.println(\u0026#34;t2end\u0026#34;); \tt1.join(); \tSystem.out.println(\u0026#34;t3start\u0026#34;); \tt3.start(); \tSystem.out.println(\u0026#34;t3end\u0026#34;); \tSystem.out.println(Thread.currentThread().getName()+\u0026#34; end\u0026#34;); \t} } 结果：\nmain start t1start t1end t2start t2end A-1 B-1 A-2 A-3 A-4 A-5 B-2 t3start t3end B-3 main end B-4 B-5 C-1 C-2 C-3 C-4 C-5 主线程在t1.join()方法处停止，并需要等待A线程执行完毕后才会执行t3.start()，然而，并不影响B线程的执行。因此，可以得出结论，t.join()方法只会使主线程进入等待池并等待t线程执行完毕后才会被唤醒。并不影响同一时刻处在运行状态的其他线程。\n join源码中，只会调用wait方法，并没有在结束时调用notify，这是因为==线程在die的时候会自动调用自身的notifyAll方法，来释放所有的资源和锁。==\n 线程与线程池的状态 线程的状态（NRBWTT）（牛肉包我舔舔）  NEW\t尚未启动 RUNNABLE 正在执⾏中 BLOCKED 阻塞的（被同步锁或者IO锁阻塞） WAITING 永久等待状态 TIMED_WAITING 等待指定的时间重新被唤醒的状态 TERMINATED 执⾏完成  线程池的状态（RSSTT）（人事是傻甜甜）   RUNNING： 这是最正常的状态 ，接受新的任务，处理等待队列中的任务 。\n  SHUTDOWN：不接受新的任务提交，但是会继续处理等待队列中的任务。\n  STOP：不接受新的任务提交，不再处理等待队列中的任务，中断正在执⾏任务的线程。\n  TIDYING：所有的任务都销毁了，workCount 为 0，线程池的状态在转换为 TIDYING 状态时，会执⾏钩⼦⽅法 terminated()。\n  TERMINATED：terminated()⽅法结束后，线程池的状态就会变成这个。\n  线程池 大佬链接====》线程池1_CSDN博客\n大佬链接====》线程池2-CSDN博客\n线程池的优点   降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。\n  提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。\n  **提高线程的可管理性。**线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。\n  线程池的继承树 线程池地使用 线程池的真正实现类是ThreadPoolExecutor ，其构造器方法有以下4种：\npublic ThreadPoolExecutor(int corePoolSize,  int maximumPoolSize,  long keepAliveTime,  TimeUnit unit,  BlockingQueue\u0026lt;Runnable\u0026gt; workQueue) {  this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,  Executors.defaultThreadFactory(), defaultHandler); }  public ThreadPoolExecutor(int corePoolSize,  int maximumPoolSize,  long keepAliveTime,  TimeUnit unit,  BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,  ThreadFactory threadFactory) {  this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,  threadFactory, defaultHandler); }  public ThreadPoolExecutor(int corePoolSize,  int maximumPoolSize,  long keepAliveTime,  TimeUnit unit,  BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,  RejectedExecutionHandler handler) {  this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,  Executors.defaultThreadFactory(), handler); }  public ThreadPoolExecutor(int corePoolSize,  int maximumPoolSize,  long keepAliveTime,  TimeUnit unit,  BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,  ThreadFactory threadFactory,  RejectedExecutionHandler handler) {  if (corePoolSize \u0026lt; 0 ||  maximumPoolSize \u0026lt;= 0 ||  maximumPoolSize \u0026lt; corePoolSize ||  keepAliveTime \u0026lt; 0)  throw new IllegalArgumentException();  if (workQueue == null || threadFactory == null || handler == null)  throw new NullPointerException();  this.corePoolSize = corePoolSize;  this.maximumPoolSize = maximumPoolSize;  this.workQueue = workQueue;  this.keepAliveTime = unit.toNanos(keepAliveTime);  this.threadFactory = threadFactory;  this.handler = handler; } 参数说明：IPHP  corePoolSize（必需）：核心线程数。默认情况下，核心线程会一直存活，但是当将 allowCoreThreadTimeout 设置为 true 时，核心线程也会超时回收。 maximumPoolSize（必需）：线程池所能容纳的最大线程数。当活跃线程数达到该数值后，后续的新任务将会阻塞。 keepAliveTime（必需）：线程闲置超时时长。如果超过该时长，非核心线程就会被回收。如果将 allowCoreThreadTimeout 设置为 true 时，核心线程也会超时回收。 unit（必需）：指定 keepAliveTime 参数的时间单位。常用的有：TimeUnit.MILLISECONDS（毫秒）、TimeUnit.SECONDS（秒）、TimeUnit.MINUTES（分）。 workQueue（必需）：任务队列。通过线程池的 execute() 方法提交的 Runnable 对象将存储在该参数中。其采用阻塞队列实现。 threadFactory（可选）：线程工厂。用于指定为线程池创建新线程的方式。 handler（可选）：拒绝策略。当达到最大线程数时需要执行的饱和策略。  线程池的使用流程：\n// 创建线程池 ThreadPoolExecutor threadPool = new ThreadPoolExecutor(CORE_POOL_SIZE,  MAXIMUM_POOL_SIZE,  KEEP_ALIVE,  TimeUnit.SECONDS,  sPoolWorkQueue,  sThreadFactory); // 向线程池提交任务 threadPool.execute(new Runnable() {  @Override  public void run() {  ... // 线程执行的任务  } }); // 关闭线程池 threadPool.shutdown(); // 设置线程池的状态为SHUTDOWN，然后中断所有没有正在执行任务的线程 threadPool.shutdownNow(); // 设置线程池的状态为 STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表 线程池的工作原理 简概：IPHP  1.如果线程数小于corePoolSize，创建一个新核心线程来运行新任务。 2.如果线程数大于等于corePoolSize但小于maximumPoolSize，则将任务放入队列。 3.如果队列已满，并且线程数小于maxPoolSize，则创建一个新非核心线程来运行任务。 4.如果队列已满，并且线程数大于或等于maxPoolSize，则拒绝该任务。  是否需要增加线程的判断顺序是：1、corePoolSize 2、workQueue 3、maxPoolSize\n举个例子：\n线程池的核心线程数corePoolSize大小为5，最大池maxPoolSize大小为10，队列workQueue为100。\n因为线程中的请求最多会创建5个，然后任务将被添加到队列中，直到达到100。当队列已满时，将创建新的线程，最多到10个线程，如果再来任务，就拒绝。\n特点：\n 通过设置corePoolSize和maximumPoolSize 相同，就可以创建固定大小的线程池。 我们使用线程池一般希望保持较少的线程数，并且只有在负载变得很大时才增加它。 通过设置maximumPoolSize为很高的值，例如 Integer.MAX_VALUE，可以允许线程池容纳任意数量的并发任务。 只有在队列填满时才创建多于corePoolSize的非核心线程，如果使用的是无界队列（例如LinkedBlockingQueue），那么线程数就不会超过corePoolSize。  线程参数解析 任务队列workQueue 任务队列是基于阻塞队列实现的，即采用生产者消费者模式，在 Java 中需要实现 BlockingQueue 接口。但 Java 已经为我们提供了 7 种阻塞队列的实现：\n  ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列（数组结构可配合指针实现一个环形队列）。\n  LinkedBlockingQueue： 一个由链表结构组成的有界阻塞队列，在未指明容量时，容量默认为Integer.MAX_VALUE。\n 这种队列容量无限大，可以防止流量突增。设置maximumPoolSize也用不到，因为队列装不满，永远不需要创建新的非核心线程。但是也有风险，处理任务的速度跟不上提交的速度，可能造成内存浪费或者OOM。\n   PriorityBlockingQueue： 一个支持优先级排序的无界阻塞队列，对元素没有要求，可以实现 Comparable 接口也可以提供 Comparator 来对队列中的元素进行比较。跟时间没有任何关系，仅仅是按照优先级取任务。\n  DelayQueue：类似于PriorityBlockingQueue，是二叉堆实现的无界优先级阻塞队列。要求元素都实现 Delayed 接口，通过执行时延从队列中提取任务，时间没到任务取不出来。\n  SynchronousQueue： 一个不存储元素的阻塞队列，消费者线程调用 take() 方法的时候就会发生阻塞，直到有一个生产者线程生产了一个元素，消费者线程就可以拿到这个元素并返回；生产者线程调用 put() 方法的时候也会发生阻塞，直到有一个消费者线程消费了一个元素，生产者才会返回。\n 工作任务不多的情况下，只是将任务中转，就可以用SynchronousQueue，这个队列本身内部没有容量，使用这种队列，maximumPoolSize就可能需要设置的大一些。因为没有队列容量作为缓冲了，很容易创建新线程。\n   LinkedBlockingDeque： 使用双向队列实现的有界双端阻塞队列。双端意味着可以像普通队列一样 FIFO（先进先出），也可以像栈一样 FILO（先进后出）。\n  LinkedTransferQueue： 它是ConcurrentLinkedQueue、LinkedBlockingQueue 和 SynchronousQueue 的结合体，但是把它用在 ThreadPoolExecutor 中，和 LinkedBlockingQueue 行为一致，但是是无界的阻塞队列。\n  注意有界队列和无界队列的区别：如果使用有界队列，当队列饱和时并超过最大线程数时就会执行拒绝策略；而如果使用无界队列，因为任务队列永远都可以添加任务，所以设置 maximumPoolSize 没有任何意义。\n线程工厂（threadFactory） 线程工厂指定创建线程的方式，需要实现 ThreadFactory 接口，并实现 newThread(Runnable r) 方法。\n 新的线程是由ThreadFactory创建的，默认使用Executors.defaultThreadFactory() 创建出来的线程都在同一个线程组，拥有同样的NORM_PRIORITY优先级并且都不是守护线程。 如果自己指定ThreadFactory，那么就可以改变线程名、线程组、优先级、是否是守护线程等。 通常使用默认的ThreadFactory就可以了  // 验证第一点，新的线程是由ThreadFactory创建的，默认使用Executors.defaultThreadFactory() public static ThreadFactory defaultThreadFactory() {  return new DefaultThreadFactory(); }  ......  static class DefaultThreadFactory implements ThreadFactory {  private static final AtomicInteger poolNumber = new AtomicInteger(1);  private final ThreadGroup group;  private final AtomicInteger threadNumber = new AtomicInteger(1);  private final String namePrefix;   DefaultThreadFactory() {  SecurityManager s = System.getSecurityManager();  group = (s != null) ? s.getThreadGroup() :  Thread.currentThread().getThreadGroup();  namePrefix = \u0026#34;pool-\u0026#34; +  poolNumber.getAndIncrement() +  \u0026#34;-thread-\u0026#34;;  }   public Thread newThread(Runnable r) {  // 验证第二点，创建出来的线程都在同一个线程组，拥有同样的NORM_PRIORITY优先级并且都不是守护线程。  Thread t = new Thread(group, r,  namePrefix + threadNumber.getAndIncrement(),  0);  if (t.isDaemon())  t.setDaemon(false);  if (t.getPriority() != Thread.NORM_PRIORITY)  t.setPriority(Thread.NORM_PRIORITY);  return t;  }  } 拒绝策略（handler） 当线程池的线程数达到最大线程数时，需要执行拒绝策略。拒绝策略需要实现 RejectedExecutionHandler 接口，并实现 rejectedExecution(Runnable r, ThreadPoolExecutor executor) 方法。不过 Executors 框架已经为我们实现了 4 种拒绝策略：\n AbortPolicy（默认）：丢弃任务并抛出 RejectedExecutionException 异常。 CallerRunsPolicy：由调用线程处理该任务。 DiscardPolicy：丢弃任务，但是不抛出异常。可以配合这种模式进行自定义的处理方式。 DiscardOldestPolicy：丢弃队列最早的未处理任务，然后重新尝试执行任务。  功能线程池 嫌上面使用线程池的方法太麻烦？其实Executors已经为我们封装好了 4 种常见的功能线程池，如下：\n 定长线程池（FixedThreadPool） 定时线程池（ScheduledThreadPool ） 可缓存线程池（CachedThreadPool） 单线程化线程池（SingleThreadExecutor）  定长线程池（FixedThreadPool） public static ExecutorService newFixedThreadPool(int nThreads) {  return new ThreadPoolExecutor(nThreads, nThreads,  0L, TimeUnit.MILLISECONDS,  new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;()); } public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) {  return new ThreadPoolExecutor(nThreads, nThreads,  0L, TimeUnit.MILLISECONDS,  new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(),  threadFactory); }  特点：只有核心线程，线程数量固定，执行完立即回收，任务队列为链表结构的有界队列。 应用场景：控制线程最大并发数。  使用：\n// 1. 创建定长线程池对象 \u0026amp; 设置线程池线程数量固定为3 ExecutorService fixedThreadPool = Executors.newFixedThreadPool(3); // 2. 创建好Runnable类线程对象 \u0026amp; 需执行的任务 Runnable task =new Runnable(){  public void run() {  System.out.println(\u0026#34;执行任务啦\u0026#34;);  } }; // 3. 向线程池提交任务 fixedThreadPool.execute(task); 定时线程池（ScheduledThreadPool ）  private static final long DEFAULT_KEEPALIVE_MILLIS = 10L;  public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) {  return new ScheduledThreadPoolExecutor(corePoolSize); } public ScheduledThreadPoolExecutor(int corePoolSize) {  super(corePoolSize, Integer.MAX_VALUE,  DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS,  new DelayedWorkQueue()); }  public static ScheduledExecutorService newScheduledThreadPool(  int corePoolSize, ThreadFactory threadFactory) {  return new ScheduledThreadPoolExecutor(corePoolSize, threadFactory); } public ScheduledThreadPoolExecutor(int corePoolSize,  ThreadFactory threadFactory) {  super(corePoolSize, Integer.MAX_VALUE,  DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS,  new DelayedWorkQueue(), threadFactory); }  特点：核心线程数量固定，非核心线程数量无限，执行完闲置 10ms 后回收，任务队列为延时阻塞队列。 应用场景：执行定时或周期性的任务。  // 1. 创建 定时线程池对象 \u0026amp; 设置线程池线程数量固定为5 ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); // 2. 创建好Runnable类线程对象 \u0026amp; 需执行的任务 Runnable task =new Runnable(){  public void run() {  System.out.println(\u0026#34;执行任务啦\u0026#34;);  } }; // 3. 向线程池提交任务 scheduledThreadPool.schedule(task, 1, TimeUnit.SECONDS); // 延迟1s后执行任务 scheduledThreadPool.scheduleAtFixedRate(task,10,1000,TimeUnit.MILLISECONDS);// 延迟10ms后、每隔1000ms执行任务 可缓存线程池（CachedThreadPool）  public static ExecutorService newCachedThreadPool() {  return new ThreadPoolExecutor(0, Integer.MAX_VALUE,  60L, TimeUnit.SECONDS,  new SynchronousQueue\u0026lt;Runnable\u0026gt;()); } public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) {  return new ThreadPoolExecutor(0, Integer.MAX_VALUE,  60L, TimeUnit.SECONDS,  new SynchronousQueue\u0026lt;Runnable\u0026gt;(),  threadFactory); }  特点：无核心线程，非核心线程数量无限，执行完闲置 60s 后回收，任务队列为不存储元素的阻塞队列。 应用场景：执行大量、耗时少的任务。  // 1. 创建可缓存线程池对象 ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); // 2. 创建好Runnable类线程对象 \u0026amp; 需执行的任务 Runnable task =new Runnable(){  public void run() {  System.out.println(\u0026#34;执行任务啦\u0026#34;);  } }; // 3. 向线程池提交任务 cachedThreadPool.execute(task) 单线程化线程池（SingleThreadExecutor） public static ExecutorService newSingleThreadExecutor() {  return new FinalizableDelegatedExecutorService  (new ThreadPoolExecutor(1, 1,  0L, TimeUnit.MILLISECONDS,  new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;())); } public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) {  return new FinalizableDelegatedExecutorService  (new ThreadPoolExecutor(1, 1,  0L, TimeUnit.MILLISECONDS,  new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(),  threadFactory)); }  特点：只有 1 个核心线程，无非核心线程，执行完立即回收，任务队列为链表结构的有界队列。 应用场景：不适合并发但可能引起 IO 阻塞性及影响 UI 线程响应的操作，如数据库操作、文件操作等。  // 1. 创建单线程化线程池 ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor(); // 2. 创建好Runnable类线程对象 \u0026amp; 需执行的任务 Runnable task =new Runnable(){  public void run() {  System.out.println(\u0026#34;执行任务啦\u0026#34;);  } }; // 3. 向线程池提交任务 singleThreadExecutor.execute(task); 对比 线程池模板 大佬链接====》Spring线程池配置模板- 博客园 \n@EnableAsync @Configuration public class LogThreadPoolConfig {   @Bean(name = \u0026#34;logThreadPool\u0026#34;)  public ThreadPoolTaskExecutor LogThreadPoolTask() {  ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();  LogThreadPoolProperties properties = this.logThreadPoolProperties();   executor.setCorePoolSize(properties.getCorePoolSize());  executor.setMaxPoolSize(properties.getMaxPoolSize());  executor.setQueueCapacity(properties.getQueueCapacity());  executor.setKeepAliveSeconds(properties.getKeepAliveSeconds());  executor.setThreadNamePrefix(properties.getThreadName());  switch (properties.getRejectedExecutionHandler()) {  case \u0026#34;abortPolicy\u0026#34;:  executor.setRejectedExecutionHandler(new AbortPolicy());  break;  case \u0026#34;callerRunsPolicy\u0026#34;:  executor.setRejectedExecutionHandler(new CallerRunsPolicy());  break;  case \u0026#34;discardOldestPolicy\u0026#34;:  executor.setRejectedExecutionHandler(new DiscardOldestPolicy());  break;  case \u0026#34;discardPolicy\u0026#34;:  executor.setRejectedExecutionHandler(new DiscardOldestPolicy());  break;  default:  executor.setRejectedExecutionHandler(new CallerRunsPolicy());  break;  }  executor.initialize();  return executor;  }    @Bean  @ConfigurationProperties(prefix = \u0026#34;threadpool.log\u0026#34;)  public LogThreadPoolProperties logThreadPoolProperties() {  return new LogThreadPoolProperties();  }    //@Getter lombok提供的getset方法生成注解  //@Setter  @Configuration  public static class LogThreadPoolProperties {   /** * 线程前缀名 */  private String threadName;  /** * 核心线程池大小 */  private int corePoolSize;  /** * 最大线程数 */  private int maxPoolSize;  /** * 队列大小 */  private int queueCapacity;  /** * 线程池维护空闲线程存在时间 */  private int keepAliveSeconds;  /** * 拒绝策略 */  private String rejectedExecutionHandler;   } } 这样就可以在yml文件中配置参数了：\nthreadpool:  log:  threadName: ThreadPool-log- # 线程池前缀名  corePoolSize: 8 # 核心线程池数：IO型推荐设置为cpu核心数*2；cpu型推荐设置为cpu数+1  maxPoolSize: 16 # 最大线程池数  queueCapacity: 1000 # 线程池阻塞队列容量  keepAliveSeconds: 60 # 允许线程空闲时间  # 拒绝策略 abortPolicy callerRunsPolicy discardOldestPolicy discardPolicy  rejectedExecutionHandler: callerRunsPolicy 使用：\nSpring提供了注解方式来方便我们使用线程池，只需要在要异步处理的方法上加 @Async(\u0026ldquo;你配置的线程池名字\u0026rdquo;)就可以了,注意这个类需要被spring扫描并纳入管理，所以要加@Service、@Component等注解。\n@Service public class ServiceImpl implements Service {   @Override  @Async(\u0026#34;logThreadPool\u0026#34;)  public void addOperationLog(BaseLog baseLog) {  //你要异步执行的逻辑  } } 小总结  ==Executors 的 4 个功能线程池虽然方便，但现在已经不建议使用了，而是建议直接通过使用 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。==  其实 Executors 的 4 个功能线程有如下弊端：\n FixedThreadPool 和 SingleThreadExecutor：主要问题是堆积的请求处理队列均采用 LinkedBlockingQueue，可能会耗费非常大的内存，甚至 OOM。 CachedThreadPool 和 ScheduledThreadPool：主要问题是线程数最大数是 Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至 OOM。  线程池里的线程数量设定为多少比较合适？  这个得看任务类型\n CPU密集型（加密、计算hash等）：最佳线程数为CPU核心数的1-2倍左右。 耗时IO型（读写数据库、文件、网络读写等）：最佳线程数一般会大于CPU核心数很多倍 ，参考Brain Goetz专家推荐的计算方法：线程数=CPU核心数*（1+平均等待时间/平均工作时间）  如果需要更精确的线程数量，那就需要根据不同的程序去做压测，这样就能得到比较合适的线程数量。\nThreadLocal 概念 ThreadLocal叫做线程变量，ThreadLocal中填充的变量 属于当前线程，该变量对其他线程而言是隔离的。ThreadLocal为变量在每个线程中都创建了一个副本，那么每个线程可以访问自己内部的副本变量。\nThreadLocal 适用于每个线程变量在线程间隔离而在方法或类间共享的场景。\nThreadLocal与Synchronized的区别   ThreadLocal\nThreadLocal则用于线程间的数据隔离。ThreadLocal为每一个线程都提供了变量的副本，使得每个线程在某一时间访问到的并不是同一个对象\n  Synchronized\nSynchronized用于线程间的数据共享。利用锁的机制，使变量或代码块在某一时该只能被一个线程访问。\n  使用 public class ThreadLocaDemo {   private static ThreadLocal\u0026lt;String\u0026gt; localVar = new ThreadLocal\u0026lt;String\u0026gt;();   static void print(String str) {  //打印当前线程中本地内存中本地变量的值  System.out.println(str + \u0026#34; :\u0026#34; + localVar.get());  //清除本地内存中的本地变量  localVar.remove();  }  public static void main(String[] args) throws InterruptedException {   new Thread(new Runnable() {  public void run() {  ThreadLocaDemo.localVar.set(\u0026#34;local_A\u0026#34;);  print(\u0026#34;A\u0026#34;);  //打印本地变量  System.out.println(\u0026#34;after remove : \u0026#34; + localVar.get());   }  },\u0026#34;A\u0026#34;).start();   Thread.sleep(1000);   new Thread(new Runnable() {  public void run() {  ThreadLocaDemo.localVar.set(\u0026#34;local_B\u0026#34;);  print(\u0026#34;B\u0026#34;);  System.out.println(\u0026#34;after remove : \u0026#34; + localVar.get());   }  },\u0026#34;B\u0026#34;).start();  } } A :local_A after remove : null B :local_B after remove : null set() ==向ThreadLocal里面存东西就是向它里面的Map存东西的，然后ThreadLocal把这个Map挂到当前的线程底下==\n public void set(T value) {  //1、获取当前线程  Thread t = Thread.currentThread();  //2、获取线程中的属性 threadLocalMap ,如果threadLocalMap 不为空，  //则直接更新要保存的变量值，否则创建threadLocalMap，并赋值  ThreadLocalMap map = getMap(t);  if (map != null)  map.set(this, value);  else  // 初始化thradLocalMap 并赋值  createMap(t, value);  } 从上面的代码可以看出，ThreadLocal set赋值的时候首先会获取当前线程thread,并获取thread线程中的ThreadLocalMap属性。如果map属性不为空，则直接更新value值，如果map为空，则实例化threadLocalMap,并将value值初始化。\n==ThreadLocalMap呢是当前线程Thread一个叫threadLocals的变量中获取的。每个线程Thread都维护了自己的threadLocals变量，所以在每个线程创建ThreadLocal的时候，实际上数据是存在自己线程Thread的threadLocals变量里面的，别人没办法拿到，从而实现了隔离。==\nThreadLocalMap getMap(Thread t) {  return t.threadLocals;  }  static class ThreadLocalMap {  /** * The entries in this hash map extend WeakReference, using * its main ref field as the key (which is always a * ThreadLocal object). Note that null keys (i.e. entry.get() * == null) mean that the key is no longer referenced, so the * entry can be expunged from table. Such entries are referred to * as \u0026#34;stale entries\u0026#34; in the code that follows. */  static class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; {  /** The value associated with this ThreadLocal. */  Object value;   Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) {  super(k);  value = v;  }  }  } 可看出ThreadLocalMap是ThreadLocal的内部静态类，而它的构成主要是用Entry来保存数据 ，而且还是继承的弱引用。在Entry内部使用ThreadLocal作为key，使用我们设置的value作为value。IPHP姬祥todo createMap\nget()  public T get() {  //1、获取当前线程  Thread t = Thread.currentThread();  //2、获取当前线程的ThreadLocalMap  ThreadLocalMap map = getMap(t);  //3、如果map数据为空，  if (map != null) {  //3.1、获取threalLocalMap中存储的值  ThreadLocalMap.Entry e = map.getEntry(this);  if (e != null) {  @SuppressWarnings(\u0026#34;unchecked\u0026#34;)  T result = (T)e.value;  return result;  }  }  //如果是数据为null，则初始化，初始化的结果，TheralLocalMap中存放key值为threadLocal，值为null  return setInitialValue();  }  private T setInitialValue() {  T value = initialValue();  Thread t = Thread.currentThread();  ThreadLocalMap map = getMap(t);  if (map != null)  map.set(this, value);  else  createMap(t, value);  return value;  } remove()  public void remove() {  ThreadLocalMap m = getMap(Thread.currentThread());  if (m != null)  m.remove(this); } remove方法，直接将ThrealLocal 对应的值从当前相差Thread中的ThreadLocalMap中删除。为什么要删除，这涉及到内存泄露的问题。\n如上所说（Local跳转方式，Shimo跳转方式）， ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用，弱引用的特点是，如果这个对象只存在弱引用，那么在下一次垃圾回收的时候必然会被清理掉。\n所以如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候会被清理掉的，这样一来 ThreadLocalMap中使用这个 ThreadLocal 的 key 也会被清理掉。但是，value 是强引用，不会被清理，这样一来就会出现 key 为 null 的 value。这样就造成了内存泄露。\nThreadLocal与Thread，ThreadLocalMap之间的关系 ThreadLocal 常见使用场景 参考====》ThreadLocal的应用场景 - sw_kong - 博客园 (cnblogs.com)\n场景的需求：\n 每个线程需要有自己单独的实例 实例需要在多个方法中共享，但不希望变量被多线程共享 ThreadLocal 用作每个线程内需要独立保存信息，以便供其他方法更方便地获取该信息的场景。每个线程获取到的信息可能都是不一样的，前面执行的方法保存了信息后，后续方法可以通过ThreadLocal 直接获取到，避免了传参，类似于全局变量的概念。  SimpleDateFormat （Java8里的 java.time.format.DateTimeFormatter 是线程安全的 ，由于它调用的parseResolved0 方法入参都是final 修饰的，不可变变量是线程安全的。）\n当时我们使用SimpleDataFormat的parse()方法，内部有一个Calendar对象，调用SimpleDataFormat的parse()方法会先调用Calendar.clear（），然后调用Calendar.add()。ThreadLocal_敖丙-CSDN博客。如果一个线程先调用了add()然后另一个线程又调用了clear()，这时候parse()方法解析的时间就不对了。\n在这种情况下，每个Thread内都有自己的实例副本，且该副本只能由当前Thread访问到并使用，相当于每个线程内部的本地变量，这也是ThreadLocal命名的含义。因为每个线程独享副本，而不是公用的，所以不存在多线程间共享的问题。\n比如有1000个线程都要用到SimpleDateFormat\npublic class ThreadLocalDemo011 {  public static ExecutorService threadPool = Executors.newFixedThreadPool(16);   public static void main(String[] args) throws InterruptedException {   for (int i = 0; i \u0026lt; 1000; i++) {  int finalI = i;  threadPool.submit(() -\u0026gt; {  String data = new ThreadLocalDemo011().date(finalI);  System.out.println(data);  });  }  threadPool.shutdown();  }  public static Integer id= 0;  private String date(int seconds){  Date date = new Date(1000 * seconds);  id++;  System.out.println(id+\u0026#34;=============\u0026#34;);  SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\u0026#34;mm:ss\u0026#34;);  return simpleDateFormat.format(date);  } } 可以看出，我们用了一个16线程的线程池，并且给这个线程池提交了1000次任务。每个任务中它做的事情和之前是一样的，还是去执行date方法，并且在这个方法中创建一个simpleDateFormat 对象。近1000个simpleDateFormat 对象会被创建。\n这么多对象的创建是有开销的，并且在使用完之后的销毁同样是有开销的，同时存在在内存中也是一种内存的浪费。\n我们可能会想到，要不所有的线程共用一个 simpleDateFormat 对象？但是simpleDateFormat 又不是线程安全的，我们必须做同步，比如使用synchronized加锁。到这里也许就是我们最终的一个解决方法。但是使用synchronized加锁会陷入一种排队的状态，多个线程不能同时工作，这样一来，整体的效率就被大大降低了。\n 使用ThreadLocal  对这种场景，ThreadLocal再合适不过了，ThreadLocal给每个线程维护一个自己的simpleDateFormat对象，这个对象在线程之间是独立的，互相没有关系的。这也就避免了线程安全问题。与此同时，simpleDateFormat对象还不会创造过多，线程池一共只有 16 个线程，所以需要16个对象即可。\npublic class ThreadLocalDemo04 {   public static ExecutorService threadPool = Executors.newFixedThreadPool(16);   public static void main(String[] args) throws InterruptedException {   for (int i = 0; i \u0026lt; 1000; i++) {  int finalI = i;  threadPool.submit(() -\u0026gt; {  String data = new ThreadLocalDemo04().date(finalI);  System.out.println(data);  });  }  threadPool.shutdown();  }   private String date(int seconds){  Date date = new Date(1000 * seconds);  SimpleDateFormat dateFormat = ThreadSafeFormater.dateFormatThreadLocal.get();  return dateFormat.format(date);  } }  class ThreadSafeFormater{  public static Integer id= 0;  public static ThreadLocal\u0026lt;SimpleDateFormat\u0026gt; dateFormatThreadLocal = ThreadLocal.withInitial(() -\u0026gt; {  id++;  System.out.println(id+\u0026#34;=============\u0026#34;);  return new SimpleDateFormat(\u0026#34;mm:ss\u0026#34;);  }); }   结果：\n可以看出只创建了16个SimpleDateFormat对象。\n  1=============\r4=============\r5=============\r6=============\r3=============\r7=============\r2=============\r8=============\r9=============\r10=============\r11=============\r13=============\r12=============\r14=============\r15=============\r16=============\r00:09\r00:16\r00:17\r........\r........\r16:05\r16:04\r16:03\r16:02  问题：  这种情况每个线程的SimpleDataFormat的parse()方法，在一个任务执行后其他任务继续使用这个线程执行任务时，会不会造成数据混乱。   答案  不会，SimpleDataFormat的parse()方法，内部有一个Calendar对象，调用SimpleDataFormat的parse()方法会先调用Calendar.clear()清除数据，然后调用Calendar.add()添加数据。    用户信息（类似线程内的全局变量） 每个线程内需要保存类似于全局变量的信息（例如在拦截器中获取的用户信息），可以让不同方法直接使用，避免参数传递的麻烦却不想被多线程共享（因为不同线程获取到的用户信息不一样）。\n例如，用 ThreadLocal 保存一些业务内容（用户权限信息、从用户系统获取到的用户名、用户ID 等），这些信息在同一个线程内相同，但是不同的线程使用的业务内容是不相同的。\n在线程生命周期内，都通过这个静态 ThreadLocal 实例的 get() 方法取得自己 set 过的那个对象，避免了将这个对象（如 user 对象）作为参数传递的麻烦。\n比如说我们是一个用户系统，那么当一个请求进来的时候，一个线程会负责执行这个请求，然后这个请求就会依次调用service-1()、service-2()、service-3()、service-4()，这4个方法可能是分布在不同的类中的。\npackage com.kong.threadlocal;   public class ThreadLocalDemo05 {  public static void main(String[] args) {  User user = new User(\u0026#34;jack\u0026#34;);  new Service1().service1(user);  } }  class Service1 {  public void service1(User user){  //给ThreadLocal赋值，后续的服务直接通过ThreadLocal获取就行了。  UserContextHolder.holder.set(user);  new Service2().service2();  } }  class Service2 {  public void service2(){  User user = UserContextHolder.holder.get();  System.out.println(\u0026#34;service2拿到的用户:\u0026#34;+user.name);  new Service3().service3();  } }  class Service3 {  public void service3(){  User user = UserContextHolder.holder.get();  System.out.println(\u0026#34;service3拿到的用户:\u0026#34;+user.name);  //在整个流程执行完毕后，一定要执行remove  UserContextHolder.holder.remove();  } }  class UserContextHolder {  //创建ThreadLocal保存User对象  public static ThreadLocal\u0026lt;User\u0026gt; holder = new ThreadLocal\u0026lt;\u0026gt;(); }  class User {  String name;  public User(String name){  this.name = name;  } } 守护线程 面试题 线程池创建的4中方法 Local跳转方式，Shimo跳转方式\n任务加入的线程池的流程 Local跳转方式，Shimo跳转方式\n线程池的7个参数。拒绝策略。 参数：Local跳转方式，Shimo跳转方式\n拒绝策略：Local跳转方式，Shimo跳转方式\n线程池中如何拿到线程的执行结果 线程池中 submit()和 execute()方法有什么区别？   execute()：只能执⾏ Runnable 类型的任务。\n  submit()：可以执⾏ Runnable 和 Callable 类型的任务。\n  死锁 两线程都有各自的锁，并都尝试去获取独占对方的锁，就会发生阻塞的现象。\n解释下生产消费模型 生产者消费者能够解决的问题如下：\n 生产与消费的速度不匹配 软件开发过程中解耦  生产者生产，消费者消费，这就必然存在一个中间容器，我们可以把这个容器想象成是一个货架，当货架空的时候，生产者要生产产品，此时消费者在等待生产者往货架上生产产品，而当货架满的时候，消费者可以从货架上拿走商品，生产者此时等待货架的空位，这样不断的循环。那么在这个过程中，生产者和消费者是不直接接触的，所谓的‘货架’其实就是一个==阻塞队列==，生产者生产的产品不直接给消费者消费，而是仍给阻塞队列，这个阻塞队列就是来解决生产者消费者的强耦合的。就是生产者消费者模型。TP①，TP②\n参考  ThreadLocal原理分析与使用场景 - 阿凡卢 - 博客园 (cnblogs.com) ThreadLocal的应用场景 - sw_kong - 博客园 (cnblogs.com) ThreadLocal_敖丙-CSDN博客 CSDN博客_生产者消费者模型 Java8之Consumer、Supplier、Predicate和Function攻略 博客园 ","id":18,"section":"posts","summary":"","tags":["Thread"],"title":"Thread","uri":"https://gb.ytte.top/1/01/01/thread/","year":"0001"},{"content":"使用typora写文档传图片，再上传石墨等网站的好方法 1. typora下载 typora开始收费，我们可以使用不收费的老版本 下载链接\n2. typora图床 picgotypora不像word，word可以直接将上传的图片保存在文件当中，而typora不行。但是typora支持picgo插件的使用，所以，我们下在picgo 下载链接\n3. 七牛云和picgo的使用，二级域名（如果不想弄直接第4步骤） 注册七牛云 ，并开通对象存储 picgo使用七牛云图床 - 知乎 。为七牛云配置二级域名（前提是有了一级域名）：二级域名-石墨\n4. picgo 配置： 进入picgo，点击左侧的图床设置，选择七牛图床。按下图所示填写配置，SecretKey向 j j 索取。\n SbOIcbA56xQGcpZyyLDtuARvi3COHhlBxa-IZxDi\nytte-picgo\nhttps://blogimg.ytte.top/\nz2\nimg-sunsifan/或者是img-wuhao\n 伍昊：\n孙思凡：\n将最后 一项，指定存储路径更改为：img-sunsifan。\n5. typora设置   打开typora后，\n  点击左上角文件，\n  点击偏好设置，\n  点击图像，\n  如图进行配置，\n   成功就会出现如图信息，  ==大功告成==\n6. picgo的使用 使用截图软件截图，打开picgo点记左侧的上传区，点击剪切板图片上传，上传成功后自动将图片地址返回到剪切板当中。\n 注意：如果直接截图后直接粘贴到typora文中，使用的是标签，在有的编辑器不能正常识别（比如石墨），所以最好先按上面方法。\n","id":19,"section":"posts","summary":"","tags":["typora"],"title":"typora","uri":"https://gb.ytte.top/1/01/01/typora/","year":"0001"},{"content":"谷粒商城步骤笔记 使用人人代码生成器 视频====》谷粒商城——p17\n  clone人人代码生成器\n  放进项目中加入模块\n  在application.yml修改数据库相关信息\n  在generator.properties中修改参数\nmainPath=com.atguigu #包名 package=com.atguigu.gulimall moduleName=product #作者 author=yttejx #Email email=1026190684@qq.com #表前缀(类名不会包含表前缀) tablePrefix=pms_   更改renren-generator的resources的template的Controller模板中@RequiresPermissions注解和导包注释掉，以后再导入使用。\n  启动renren-generator的Application。网页访问localhost:80，点击显示全部表格，全选表格，点击生成代码。\n  将生成的代码中main包复制粘贴到product项目下。\n  新建模块（maven）取名，gulimall_common（项目的公共资源全部放在这里）\n  查看product项目中报错信息。将相应的pom 和类（renren自己创建的工具类）加入到common项目中\n从renren-faster中导入\n  包：\n exception utils  Constant PageUtils Query R validator xss  去掉XssFilter和XssHttpServletRequestWrapper，是防跨站脚本攻击的，以后用springsecurity来实现。 这里需要导包Servlet，同时设置scope 为provided，因为tomcat内有Servlet        pom：\n  \u0026lt;properties\u0026gt; \t\u0026lt;maven.compiler.source\u0026gt;8\u0026lt;/maven.compiler.source\u0026gt;  \u0026lt;maven.compiler.target\u0026gt;8\u0026lt;/maven.compiler.target\u0026gt;  \u0026lt;mybatis.plus.boot.starter.version\u0026gt;3.5.1\u0026lt;/mybatis.plus.boot.starter.version\u0026gt;  \u0026lt;lombok.version\u0026gt;1.18.22\u0026lt;/lombok.version\u0026gt;  \u0026lt;commons.lang.version\u0026gt;2.6\u0026lt;/commons.lang.version\u0026gt;  \u0026lt;httpcore.version\u0026gt;4.4.15\u0026lt;/httpcore.version\u0026gt;  \u0026lt;shiro.version\u0026gt;1.4.0\u0026lt;/shiro.version\u0026gt;  \u0026lt;commons.io.version\u0026gt;2.5\u0026lt;/commons.io.version\u0026gt;  \u0026lt;mysql.connector.version\u0026gt;8.0.28\u0026lt;/mysql.connector.version\u0026gt;  \u0026lt;/properties\u0026gt;  \u0026lt;dependencies\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;mybatis-plus-boot-starter\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${mybatis.plus.boot.starter.version}\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${lombok.version}\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;commons-lang\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;commons-lang\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${commons.lang.version}\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;!-- https://mvnrepository.com/artifact/org.apache.httpcomponents/httpcore --\u0026gt;  \u0026lt;!--java 发送http请求--\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;httpcore\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${httpcore.version}\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;!-- https://mvnrepository.com/artifact/commons-io/commons-io --\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;commons-io\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;commons-io\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${commons.io.version}\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.apache.shiro\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;shiro-core\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${shiro.version}\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.apache.shiro\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;shiro-spring\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${shiro.version}\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${mysql.connector.version}\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;servlet-api\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;2.5\u0026lt;/version\u0026gt;  \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt;  \u0026lt;!-- tomcat内有Servlet--\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;jakarta.validation\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;jakarta.validation-api\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;2.0.2\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;/dependencies\u0026gt;   product的UndoLogEntity的rollbackInfo属性，数据库中使用的是Longblob，实体类中使用byte[]。\n  配置\u0026amp;测试服务的crud功能 视频====》谷粒商城——p18\n  每个项目都需要crud，所以在common中导入mysql驱动\n  配置数据源\nspring:  datasource:  username: root  password: as123  url: jdbc:mysql://localhost:3380/gulimall_pms?useUnicode=true\u0026amp;characterEncoding=UTF-8\u0026amp;useSSL=false\u0026amp;serverTimezone=Asia/Shanghai  driver-class-name: com.mysql.cj.jdbc.Driver   配置mybatis-plus\n 使用MapperScan  @MapperScan(\u0026#34;com.atguigu.gulimall.product.dao\u0026#34;)   告诉mybatis-plus，sql映射文件位置\n使用自增主键（以后数据量大了在使用其他主键策略）\nmybatis-plus:  mapper-locations: classpath*:/mapper/**/*.xml  global-config:  db-config:  id-type: auto   crud测试\n@Test  void contextLoads() {  BrandEntity brandEntity = new BrandEntity();  brandEntity.setName(\u0026#34;华为\u0026#34;);   brandService.save(brandEntity);  System.out.println(\u0026#34;保存成功----------\u0026#34;);   brandEntity.setBrandId(1L);  brandEntity.setDescript(\u0026#34;华为手机\u0026#34;);  brandService.updateById(brandEntity);  System.out.println(\u0026#34;更新成功---------\u0026#34;);   List\u0026lt;BrandEntity\u0026gt; list = brandService.list(new LambdaQueryWrapper\u0026lt;BrandEntity\u0026gt;().eq(BrandEntity::getBrandId, \u0026#34;1\u0026#34;));  list.forEach(System.out::println);  System.out.println(\u0026#34;查询成功---------\u0026#34;);  }     逆向工程生成所有代码 视频====》p19\n分","id":20,"section":"posts","summary":"","tags":["谷粒商城"],"title":"谷粒商城步骤笔记","uri":"https://gb.ytte.top/1/01/01/%E8%B0%B7%E7%B2%92%E5%95%86%E5%9F%8E%E6%AD%A5%E9%AA%A4%E7%AC%94%E8%AE%B0/","year":"0001"}],"tags":[{"title":"bolg","uri":"https://gb.ytte.top/tags/bolg/"},{"title":"Docker","uri":"https://gb.ytte.top/tags/docker/"},{"title":"git","uri":"https://gb.ytte.top/tags/git/"},{"title":"gitee","uri":"https://gb.ytte.top/tags/gitee/"},{"title":"github","uri":"https://gb.ytte.top/tags/github/"},{"title":"hexo","uri":"https://gb.ytte.top/tags/hexo/"},{"title":"Https","uri":"https://gb.ytte.top/tags/https/"},{"title":"hugo","uri":"https://gb.ytte.top/tags/hugo/"},{"title":"index","uri":"https://gb.ytte.top/tags/index/"},{"title":"JVM","uri":"https://gb.ytte.top/tags/jvm/"},{"title":"mysql","uri":"https://gb.ytte.top/tags/mysql/"},{"title":"Rabbit Mq","uri":"https://gb.ytte.top/tags/rabbit-mq/"},{"title":"SpringBoot","uri":"https://gb.ytte.top/tags/springboot/"},{"title":"SpringCloud","uri":"https://gb.ytte.top/tags/springcloud/"},{"title":"Thread","uri":"https://gb.ytte.top/tags/thread/"},{"title":"typora","uri":"https://gb.ytte.top/tags/typora/"},{"title":"ZooKeeper","uri":"https://gb.ytte.top/tags/zookeeper/"},{"title":"域名","uri":"https://gb.ytte.top/tags/%E5%9F%9F%E5%90%8D/"},{"title":"基础","uri":"https://gb.ytte.top/tags/%E5%9F%BA%E7%A1%80/"},{"title":"谷粒商城","uri":"https://gb.ytte.top/tags/%E8%B0%B7%E7%B2%92%E5%95%86%E5%9F%8E/"},{"title":"题目","uri":"https://gb.ytte.top/tags/%E9%A2%98%E7%9B%AE/"}]}